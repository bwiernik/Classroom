[["index.html", "Programming with Data Spring 2022 About This Guide Brenton’s Notes", " Programming with Data Spring 2022 Brenton M. Wiernik 2022-02-22 About This Guide Welcome to the class guide for Programming with Data for Spring 2022! This guide organizes what we will be doing in each class meeting. So you can expect it to be updated regularly – in fact, the date listed above is the last time this guide was updated. This course was developed in part based on the resources provided by Jenny Bryan found at https://stat545.com and by Mason Garrison found at https://datascience4psych.github.io/DataScience4Psych/. A playlist of videos from Mason Garrison covering many of the topics we will explore in the course is available here. Brenton’s Notes This website is constantly changing. The source code for this website in the class [repo][course_repo]. I encourage you to contribute to the course code. If you catch typos, errors, please issue a pull request with the fixes. If you find cool/useful resources, please add them. How to use these notes This document is broken down into multiple chapters. Use the table of contents on the left side of the screen to navigate, and use the hamburger icon (horizontal bars) at the top of the document to open or close the table of contents. At the top of the document, you’ll see additional icons which you can click to search the document, change the size, font or color scheme of the page. The document will be updated (unpredictably) throughout the semester. Every module corresponds to a week-ish’s worth of material. Most modules are dedicated to improving a specific skill or at the very least dedicated to a specific theme. Within each module, there are embedded videos, slides, activities, labs, and tutorials. The skills developed in each module build upon skills you’ve developed in previous modules. Although these notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give you all a set of common materials on which to draw during the course. In class, we will sometimes do things outside the notes. The idea here is that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. "],["attribution.html", "Attribution Major Attributions Additional Attributions", " Attribution This class leans heavily on other peoples’ materials and ideas. Major Attributions Mason Garrison’s Data Scientists for Psychologists course; Jenny Bryan’s (jennybryan.org) STAT 545 and Happy Git with R; Joe Rodgers’s PSY 8751 Exploratory and Graphical Data Analysis Course Mine Çetinkaya-Rundel’s Data Science in a Box. Additional Attributions Academic.io’s AWESOME DATA SCIENCE Julia Fukuyama’s EXPLORATORY DATA ANALYSIS Benjamin Soltoff’s Computing for the Social Sciences Grant McDermott’s course materials on environmental economics and data science Thomas E. Love Karl Broman EMILY SUZANNE CLARK’s Rubric for Unessays Ariel Muldoon’s tutorial on simulations "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This information is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["colophon.html", "Colophon", " Colophon These notes was written in bookdown inside RStudio. The website is hosted with github, The complete source is available from github. The book style was designed by Desirée De Leon. This version of the notes was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.1.2 (2021-11-01) #&gt; os macOS Big Sur 10.16 #&gt; system x86_64, darwin17.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz UTC #&gt; date 2022-02-22 #&gt; pandoc 2.17.1.1 @ /usr/local/bin/ (via rmarkdown) Along with these packages: "],["getting-started.html", "Getting Started 0.1 Big Ideas 0.2 Course Modality", " Getting Started This overview is designed to orient you to the class. Programming with Data (Progdata) introduces on the principles of data science, including: data wrangling, modeling, visualization, and communication. In this class, we link those principles to psychological methods and open science practices by emphasizing exploratory analyses and description, rather than confirmatory analyses and hypotheses. Through the semester, we will work our way through many topics covered in Wickham and Grolemund’s R for Data Science text and develop proficiency with tidyverse. This class emphasizes replication and reproducibility. Progdata is a practical skilled-based class and should be useful to students aiming for academia as well as those interested in industry. Applications of these methods can be applied to a full range of psychological areas, including perception (e.g, eye-tracking data), neuroscience (e.g., visualizing neural networks), and individual differences (e.g., personality assessment). 0.1 Big Ideas This class covers the following broad five areas: Reproducibility; Replication; Robust Methods; Resplendent Visualizations; and R Programming. 0.2 Course Modality The course is designed to be flexible to fit each of your unique schedules and situations. The course is scheduled to meet in person 2 days a week. As Covid-related precautions evolve, this schedule is subject to change. The class can function fully asynchronously and remotely if needed. If you are sick, do not come to in person class meetings! All assignments and projects for the class can be completed at your own pace and are due as part of your portfolio at the end of the semester. 0.2.1 Successful Asynchronous Learning This video can help you to be a successful asynchronous learner. Much of this information comes from Northeastern University’s Tips for Taking Online Classes. 0.2.1.1 Productivity During Lockdown "],["syllabus.html", "Syllabus 0.3 Materials 0.4 Assignment Instructions 0.5 Due dates", " Syllabus You can find the class syllabus here. 0.3 Materials 0.3.1 Hardware If you have a laptop that can run R, please bring it to class. This class requires that you have access to a computer outside of class. This could be a laptop or a computer available in a campus computer lab. 0.3.2 Books and Videos In addition to class materials and lab activities, there will also be videos and sections of the book R for Data Science that will be shared to support your learning. R for Data Science text 0.3.3 Software 0.3.3.1 R and RStudio R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows, and MacOS. RStudio is a free integrated development environment (IDE), a powerful user interface or dashboard for R. 0.3.3.2 Git and Github Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files – called a repository – in a structured way. Think of it like the “Track Changes” features from Microsoft Word. Github is a free IDE and hosting service for Git. As a USF student, you should be able to access the GitHub Student Developer Pack for free. It includes a free PRO upgrade for your github account. 0.4 Assignment Instructions 0.4.1 Lab Activities Each week, there will be opportunities to practice your programming skills in guided activities and problems. You should complete these activities each week and submit them via your GitHub repository for the class. Of the 15 lab activities, you must complete 12 to earn an A grade. You may choose which activities (if any) to skip. For each lab activity, your code should (1) be able to run on any computer without user input, (2) produce the desired outcome, and (3) use clear coding style and logic so that readers can understand it 0.4.2 Peer Code Review Each week, you will be assigned to review 2 classmates’ code and provide constructive feedback. You should both (1) test the code to ensure that it produces correct output and (2) complete a code review report to provide feedback on program design, coding style, and documentation. Code reviews should be completed in a timely manner so that your classmates’ can benefit from the feedback. You may review either a lab activity or portfolio piece. You must complete 10 code reviews to earn an A grade. You may choose which assigned reviews (if any) to skip. As a class, please ensure that each person gets at least one review for each of their activities. 0.4.3 Portfolio Pieces By the end of the semester, you should also have completed 3 larger “portfolio pieces”. These pieces can be one of the following: Data analysis report: Explore and analyze a dataset of interest to you to derive useful or interesting insights. You must demonstrate the skills you are learning in the course (data wrangling, data visualization, modeling, web scraping, text mining, etc.) and present one or more key insights that can be learned from the data. Your project should demonstrate good habits with respect to reproducibility, clear coding style and logic, and effective visualization and communication. You may choose (or construct) any dataset of interest to you. If you are involved in research, please feel free to use data from these projects. Otherwise, there are many datasets available online you can work with, or you can build a dataset from the web yourself. We will explore portfolio piece ideas and potential data sources throughout the semester. Professional website: Create and deploy a website highlighting your skills, experience, and professional portfolio, as appropriate to your professional goals. Cheat sheet: Create a cheat sheet in the style of https://www.rstudio.com/resources/cheatsheets/ for a package, procedure, or workflow. For example, you could make a cheat sheet for a package we discuss in class or another package, or a cheat sheet for a common workflow or procedure for your lab group. Tutorial or other resource: Write a blog post, tutorial, or other resource teaching readers an analytic, programming, visualization, experimental, or workflow method. Shiny app: Build a Shiny app implementing a method of interest (e.g., power analysis, visualizing a certain type of data, fitting a certain type of model). At least 1 of your portfolio pieces must be a Data Analysis Report. Graduate students must complete an additional portfolio piece; this one must be a website, tutorial, or shiny app. 0.5 Due dates All course products are due by the end of the semester, but you should not wait until the last week to start on them! Spaced practice is much more effective for learning than cramming, and I can’t give you useful feedback if I only see things at the end of the semester. Aim to complete lab activities as you go and to complete a portfolio piece every few weeks. "],["welcome-to-data-science.html", "Part 1 Welcome to Data Science 1.1 Module Materials", " Part 1 Welcome to Data Science This module is designed to introduce you to some of the tools and workflow we will use in the course (and in data science and programming more broadly). Each section of this model has links to some videos related to the topic. You can find the module playlist here. Most of the slides used to make the videos in this module can be found here. 1.1 Module Materials Videos Located in the subchapters of this module Slidedecks Welcome Slides Meet the toolkit Suggested Readings All subchapters of this module, including R basics and workflow R4DS Book Introduction Data exploration Introduction Optional: Short Happy Git If Short Happy Git is too much, start with Oh My Git For more depth, Happy Git with R Activities Bechdal Test Oh My Git Lab Hello R "],["what-is-data-science.html", "Part 2 What is Data Science? 2.1 See for yourselves!", " Part 2 What is Data Science? You can follow along with the slides here if they do not appear below. 2.1 See for yourselves! I’ve embedded a few examples below. 2.1.1 Shiny App 2.1.2 Hans Rosling The video below is the shorter version. Hans Rosling’s 200 Countries, 200 Years, 4 Minutes - The Joy of Stats You can find a longer talk-length version below. 2.1.3 Social Media Social media contains a ton of great (and terrible examples of data science in action. These examples range from entire subreddits, such as /r/DataisBeautiful (be sure to check out the highest voted posts) to celebrity tweets about data scientists. YASSSSSSSSSS MY LOVE STEVE IS BACK!!! #KornackiThirstcontinues pic.twitter.com/ynK4D87Bhr&mdash; Leslie Jones 🦋 (@Lesdoggg) January 5, 2021 Good reasons to not be a Data Scientist:- It is a lot of work- Literally nobody will know what you&#39;re talking about- In the end, your computer will be your only real friend&mdash; 🔥 Kareem Carr 🔥 (@kareem_carr) January 22, 2021 2.1.4 Read for yourselves! Link Preview What is Data Science @ O’reilly Data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution. They are inherently interdiscplinary. They can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions. They can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: “here’s a lot of data, what can you make from it?” What is Data Science @ Quora Data Science is a combination of a number of aspects of Data such as Technology, Algorithm development, and data interference to study the data, analyze it, and find innovative solutions to difficult problems. Basically Data Science is all about Analyzing data and driving for business growth by finding creative ways. The sexiest job of 21st century Data scientists today are akin to Wall Street “quants” of the 1980s and 1990s. In those days people with backgrounds in physics and math streamed to investment banks and hedge funds, where they could devise entirely new algorithms and data strategies. Then a variety of universities developed master’s programs in financial engineering, which churned out a second generation of talent that was more accessible to mainstream firms. The pattern was repeated later in the 1990s with search engineers, whose rarefied skills soon came to be taught in computer science programs. Wikipedia Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data. How to Become a Data Scientist Data scientists are big data wranglers, gathering and analyzing large sets of structured and unstructured data. A data scientist’s role combines computer science, statistics, and mathematics. They analyze, process, and model data then interpret the results to create actionable plans for companies and other organizations. a very short history of #datascience The story of how data scientists became sexy is mostly the story of the coupling of the mature discipline of statistics with a very young one–computer science. The term “Data Science” has emerged only recently to specifically designate a new profession that is expected to make sense of the vast stores of big data. But making sense of data has a long history and has been discussed by scientists, statisticians, librarians, computer scientists and others for years. The following timeline traces the evolution of the term “Data Science” and its use, attempts to define it, and related terms. "],["meet-our-toolbox.html", "Part 3 Meet our toolbox! 3.1 R and RStudio", " Part 3 Meet our toolbox! You can follow along with the slides here if they do not appear below. I recommend installing R, Rstudio, git, and github before starting the Bechdal activity 3.1 R and RStudio 3.1.1 Install R and RStudio &quot;https://www.youtube.com/watch?v=kVIZGCT5p9U&quot; %&gt;% embed_url() %&gt;% use_align(&quot;center&quot;) Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system – use the links up at the top of the CRAN page linked above! Install RStudio’s IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. RStudio can interface with Git(Hub). However, you must do all the Git(Hub) set up described elsewhere before you can take advantage of this. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. When you upgrade R, you generally also need to update any packages you have installed. 3.1.2 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you haven’t written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, you’ve succeeded in installing R and RStudio. 3.1.3 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: palmerpenguins, package webpage 3.1.4 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudio’s leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual "],["bechdal.html", "Part 4 Bechdel Activity", " Part 4 Bechdel Activity You can find the materials for the Bechdel activity here. The compiled version should look something like the following… "],["thoughtful-workflow.html", "Part 5 Thoughtful Workflow 5.1 R Markdown 5.2 Git and Github 5.3 Getting Help with R", " Part 5 Thoughtful Workflow At this point, I recommend you pause and think about your workflow. I give you permission to spend some time and energy sorting this out! It can be as or more important than learning a new R function or package. The experts don’t talk about this much, because they’ve already got a workflow; it’s something they do almost without thinking. Working through subsequent material in R Markdown documents, possibly using Git and GitHub to track and share your progress, is a great idea and will leave you more prepared for your future data analysis projects. Typing individual lines of R code is but a small part of data analysis and it pays off to think holistically about your workflow. If you want a lot more detail on workflows, you can wander over to the optional bit on r basics and workflow. 5.1 R Markdown If you are in the mood to be entertained, start the video from the beginning. But if you’d rather just get on with it, start watching at 6:52. You can follow along with the slides here if they do not appear below. R Markdown is an accessible way to create computational documents that combine prose and tables and figures produced by R code. An introductory R Markdown workflow, including how it intersects with Git, GitHub, and RStudio, is now maintained within the Happy Git site: Test drive R Markdown 5.2 Git and Github XKCD on Git First, it’s important to realize that Git and GitHub are distinct things. GitHub is an online hosting platform that provides an array of services built on top of the Git system. (Similar platforms include Bitbucket and GitLab.) Just like we don’t need Rstudio to run R code, we don’t need GitHub to use Git… But, it will make our lives so much easier. Git can be very powerful and useful, but it can also take some getting used to. In this class, we are going to work with some of its most basic functions. We will do all of our interfacing with Git using the GitHub app and website. You can follow along with the slides here if they do not appear below. 5.2.1 What is Github? 5.2.2 Git Git is a distributed Version Control System (VCS). It is a useful tool for easily tracking changes to your code, collaborating, and sharing. (Wait, what?) Okay, try this: Imagine if Dropbox and the “Track changes” feature in MS Word had a baby. Git would be that baby. In fact, it’s even better than that because Git is optimized for the things that social scientists and data scientists spend a lot of time working on (e.g. code). The learning curve is worth it – I promise you! With Git, you can track the changes you make to your project so you always have a record of what you’ve worked on and can easily revert back to an older version if need be. It also makes working with others easier -— groups of people can work together on the same project and merge their changes into one final source! GitHub is a way to use the same power of Git all online with an easy-to-use interface. It’s used across the software world and beyond to collaborate and maintain the history of projects. There’s a high probability that your favorite app, program or package is built using Git-based tools. (RStudio is a case in point.) Scientists and academic researchers are starting to use it as well. Benefits of version control and collaboration tools aside, Git(Hub) helps to operationalize the ideals of open science and reproducibility. Journals have increasingly strict requirements regarding reproducibility and data access. GH makes this easy (DOI integration, off-the-shelf licenses, etc.). I run my entire lab on GH; this entire course is running on github; these lecture notes are hosted on github… 5.3 Getting Help with R You can follow along with the slides here if they do not appear below. "],["r_basics.html", "Part 6 R basics and workflows 6.1 Basics of working with R at the command line and RStudio goodies 6.2 Workspace and working directory 6.3 Saving an R script 6.4 Script housekeeping 6.5 To do before next class", " Part 6 R basics and workflows Who is R? Why is R troubling PhD students?@AcademicChatter #AcademicTwitter&mdash; Dr. Marie Curie (@CurieDr) January 31, 2021 There is an implicit contract with computer and scripting languages. The computer will do tedious tasks for you. In return, you must be explicit in your instructions. The computer does not have the ability to extrapolate. So we have to work within the range of what it does understand. And that is where this entire course comes into play. We are learning how to communicate in a way that it does understand. So let’s begin! 6.1 Basics of working with R at the command line and RStudio goodies Launch RStudio/R. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. Go into the Console, where we interact with the live R process. Make an assignment and then inspect the object you just created: x &lt;- 3 * 4 x #&gt; [1] 12 All R statements where you create objects – “assignments” – have this form: objectName &lt;- value and in my head I hear, e.g., “x gets 12”. You will make lots of assignments. The &lt;- is more commonly used by R programmers. You can also use = instead. There are some small differences between the two, but they rarely come up. Feel free to use whichever you prefer. For typing &lt;-, try RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio auto-magically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. Also, Alt+Shift+K brings up a keyboard shortcut reference card. 6.1.1 Object names Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. i_use_snake_case evenOthersUseCamelCase dont.use.periods Don’t use periods. I recommend snake_case or camelCase. Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this assignment, try out RStudio’s completion facility: type the first few characters, press TAB, add characters until you disambiguate, then press return. Make another assignment: brenton_rocks &lt;- 2 ^ 3 Let’s try to inspect: brentonrocks #&gt; Error in eval(expr, envir, enclos): object &#39;brentonrocks&#39; not found brent_rocks #&gt; Error in eval(expr, envir, enclos): object &#39;brent_rocks&#39; not found Here’s where that implicit contract comes in. The computer (and R) will do amazing things, if we can ask it to do those things in a way it understands. Typos matter. Case matters. Precision matters. We have to work within it’s narrow range of ability. 6.1.2 Functions R has a mind-blowing collection of built-in functions that are accessed like so: functionName(arg1 = val1, arg2 = val2, and so on) Let’s try using seq() which makes regular sequences of numbers and, while we’re at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a function’s arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. IDEs are great. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The above also demonstrates something about how R resolves function arguments. You can always specify in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want a sequence from = 1 that goes to = 10. Because we didn’t specify step size, the default value of by in the function definition is used. In this case, the default is 1. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Make this assignment and notice similar help with quotation marks. yo &lt;- &quot;hello world&quot; If you just make an assignment, you don’t get to see the value, so then you’re tempted to immediately inspect. y &lt;- seq(1, 10) y #&gt; [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen. (y &lt;- seq(1, 10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() #&gt; [1] &quot;Tue Feb 22 17:20:04 2022&quot; Now look at your workspace – in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() #&gt; [1] &quot;all_pkgs&quot; &quot;brenton_rocks&quot; #&gt; [3] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [5] &quot;install_quietly&quot; &quot;pretty_install&quot; #&gt; [7] &quot;sample_no_surprises&quot; &quot;session&quot; #&gt; [9] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [11] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [13] &quot;y&quot; &quot;yo&quot; ls() #&gt; [1] &quot;all_pkgs&quot; &quot;brenton_rocks&quot; #&gt; [3] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [5] &quot;install_quietly&quot; &quot;pretty_install&quot; #&gt; [7] &quot;sample_no_surprises&quot; &quot;session&quot; #&gt; [9] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [11] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [13] &quot;y&quot; &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom in RStudio’s Environment pane. 6.2 Workspace and working directory One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What part your analysis is “real”, i.e. will you save it as your lasting record of what happened? Where does your analysis “live”? 6.2.1 Workspace, .RData Start to quit RStudio, but don’t finish yet! Quit R/RStudio, either from the menu, using a keyboard shortcut, or by typing q() in the Console. You’ll get a prompt like this: Save workspace image to ~/.Rdata? This is offering to save your R workspace (all of the objects you made) so that you can reload them later. This seems okay, but how are you going to remember where each of those objects came from? It’s a recipe for irreproducible disaster! Let’s change some settings in RStudio to encourage a more reliable workflow. In RStudio, click Tools -&gt; Global options… In the window that pops up, on the General pane, Uncheck “Restore .RData into workspace at startup Set “Save workspace to .RData on exit” to “Never” Uncheck “Always save history (even when not saving .RData)” 6.2.2 Working directory Any process running on your computer has a notion of its “working directory”. In R, this is where R will look, by default, for files you ask it to load. It also where, by default, any files you write to disk will go. You can explicitly check your working directory with: getwd() It is also displayed at the top of the RStudio console. For right now, we will let R set its own working directory at startup. We will adopt a more reliable workflow for organizing our projects in a few weeks. You might sometimes see something like this in someone’s script. setwd(&quot;C:\\\\Users\\\\brenton\\\\Documents\\\\myCoolProject&quot;) This sets the working directory in the R session to that file path. Do not do this! It ensures that the script only works on your computer! We will explore more reliable ways to control the working directory in a few weeks. 6.3 Saving an R script Usually, we want to save the analyses we run so we can re-run them later or refer back to them. We do that with scripts. Let’s make an R script. Click File -&gt; New File -&gt; R Script. A new part of the RStudio window appears. The Script pane. Copy and paste the following code into your new script. # what do these lines do? a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 # what about these? x &lt;- runif(40) y &lt;- a + b * x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) #&gt; [1] 0.457 # these lines save some output... write(avg_x, &quot;avg_x.txt&quot;) plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) #&gt; quartz_off_screen #&gt; 2 Run the lines of your script by selecting them and clicking the Run button in RStudio or by typing Ctrl/Cmd + Shift + Enter/Return. Now let’s save the file. Click on the floppy disk to save. Give it a name ending in .R, I used toy-line.R. Now which folder the file will be saved in. By default, it will go in the current working directory. Quit RStudio. Go to the folder where you saved the file and it there. Restart RStudio. Notice that the the files you had open are restored by default. That’s helpful. Let’s change our script to make the sample size easily editable. At the top of your script, assign a new sample size to n, e.g. n &lt;- 80. Then, replace all the hard-coded 40s with n. Change some other minor-but-detectable stuff, e.g. alter the slope of the line b, the color of the line, … whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command + Enter) Walk line by line with the mouse (click “Run” in the upper right corner of editor pane). Select multiple lines and run using the keyboard shortcut or Run button. Visit your figure in your computer’s file system and view it to verify that the PDF is changing as you expect. Note that you have edited this figure using only code. You never clicked your mouse, typed a file name, or used the keyboard. This means that you can reproduce the figure (or change it) easily again in the future with with no head-scratching! 6.4 Script housekeeping Always save your R scripts with a .R so that your computer knows what to do with them. In R, comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). To be sure your code is doing what you expect, a good habit is to restart your R session and run your script from the top. To do that, click Session -&gt; Restart R. Try it! Avoid using the mouse for pieces of your analytical workflow, such as loading a dataset or saving a figure. Terribly important for reproducibility and for making it possible to retrospectively determine how a numerical table or PDF was actually produced (searching on local disk on filename, among .R files, will lead to the relevant script). 6.5 To do before next class Make an account on GitHub. Make your username recognizable! Please put up a profile photo or image on GitHub—it makes the class community more personable and easier to work with. Finish any in-class activities listed in today’s section of the guidebook that you didn’t get done. Install the software stack for this course, as indicated below. Optionally, register for the Student Developer Pack with GitHub for a bunch of free perks for students! 6.5.1 Software Stack Installation Install R and RStudio. R here: https://cran.r-project.org RStudio here: https://www.rstudio.com/products/rstudio/download/ Commentary on installing this stuff can be found at stat545.com: r-rstudio-install Install GitHub: https://desktop.github.com/ "],["r_scipts.html", "Part 7 R Scripts 7.1 Assign an Object 7.2 Types of Objects and Operations", " Part 7 R Scripts Usually, we want to save the analyses we run so we can re-run them later or refer back to them. We do that with scripts. Let’s make an R script. Click File -&gt; New File -&gt; R Script. A new part of the RStudio window appears. The Script pane. This is where you can write a series of lines of code than you can then execute (send to the Console to run) later. 7.1 Assign an Object Pick a number, assign it to an object called favorite_number, and print it. favorite_number &lt;- 42 favorite_number #&gt; [1] 42 Run these lines of code by placing your cursor on the line and either clicking the Run button or typing Ctrl/Cmd + Shift + Enter/Return. 7.2 Types of Objects and Operations Now let’s explore some basic types of objects and operations in R. 7.2.1 Vectors Vectors store multiple entries of one data type, like numbers or characters. You’ll discover that they show up just about everywhere in R. Let’s collect some data and store this in a vector called times. How many hours did you sleep last night? Drop your answer in the chat. Here’s starter code: times &lt;- c() The c() function is how we make a vector in R. The “c” stands for “concatenate”. Operations happen component-wise. Change those times to minutes. How can we “save” the results? All parts of a vector have the same type. There are many types of variables in R. The most common types are: numeric (numbers) 1. double (numbers with decimal values; 2, 3.4, 1000) 1. integer (1L, 2L, 100L) character (words or strings; \"a\", \"foo\", \"lastname\") logical (TRUE or FALSE) factor (categorical variable; factor(c(\"control\", \"experiment\"))) 7.2.2 Functions R comes with many many functions. Functions take one or more inputs and return one or more outputs. You can think of functions as prewritten R programs. Functions all take the general form: functionName(arg1 = val1, arg2 = val2, and so on) To call a function, type its name, then parentheses (). Inside the (), type the arguments and values to use as input. What’s the average sleep time? Let’s compute that using the mean() function. mean(times) #&gt; [1] 7.2 To learn how a function works, we can look at its help file. Open the help file for mean() by typing ?mean or help(mean) in the console. The help file will includes the following: A brief description of the function A list of the arguments and how to call the function A detailed description of the arguments (Optionally) Other usage details Coded examples We can see that mean() has 4 arguments: x: A vector to compute the mean of trim: the fraction (0 to 0.5) of observations to be trim from each end na.rm: TRUE or FALSE–should missing values be removed before computing? ...: Other arguments. More on that later. Default values for arugments are given in the Usage section by =. If an argument has no default (like x in mean()), it usually means it’s required. Let’s compute the trimmed mean of times, dropping 10% of values from each end. You can either enter arguments in order: mean(times, .1) #&gt; [1] 7.25 Or by name: mean(times, trim = .1) #&gt; [1] 7.25 It’s good practice to name all arguments after the first (or maybe second if its clear). Try out some other functions, such as sd(), range(), and length(). Much of R is about becoming familiar with R’s “vocabulary”. A nice list can be found in Advanced R - Vocabulary. 7.2.3 Comparisons How many people slept less than 6 hours? Let’s answer that using comparisons. We can compare the values of times to another value using &lt;. times &lt; 6 #&gt; [1] FALSE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE #&gt; [13] FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE Comparisons return a vector of logical values. We can do other logical comparisons: times &gt; 6 #&gt; [1] TRUE FALSE FALSE TRUE FALSE TRUE FALSE TRUE TRUE TRUE FALSE TRUE #&gt; [13] TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE times == 5 #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [13] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE times &lt;= 7 #&gt; [1] FALSE TRUE TRUE TRUE TRUE FALSE TRUE FALSE TRUE FALSE TRUE TRUE #&gt; [13] FALSE FALSE FALSE TRUE TRUE TRUE FALSE FALSE times != 2 #&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #&gt; [16] TRUE TRUE TRUE TRUE TRUE We can combine multiple comparisons using &amp; (AND), | (OR), and ! (NOT). (times &lt; 4) | (times &gt; 9) #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [13] FALSE FALSE FALSE FALSE FALSE FALSE TRUE TRUE Try out these functions that work with logical values. any(times &lt; 6) #&gt; [1] TRUE all(times &lt; 6) #&gt; [1] FALSE which(times &lt; 6) #&gt; [1] 2 7 16 17 7.2.4 Subsetting Use [] to subset values from a vector. You can subset with an integer (by position) or with logicals. times[4] #&gt; [1] 7 times[c(2, 5)] #&gt; [1] 4 6 times[-6] #&gt; [1] 9 4 6 7 6 4 8 7 9 6 7 9 9 8 4 5 7 10 10 times[-c(2, 3)] #&gt; [1] 9 7 6 9 4 8 7 9 6 7 9 9 8 4 5 7 10 10 times[4:8] #&gt; [1] 7 6 9 4 8 times[times &gt;= 7] #&gt; [1] 9 7 9 8 7 9 7 9 9 8 7 10 10 Subset times: Extract the third entry. Extract everything except the third entry. Extract the second and fourth entry. The fourth and second entry. Extract the second through fifth entry. Extract all entries that are less than 4 hours Why does this work? Logical subsetting! 7.2.5 Modifying a vector You can change the vector by combining [] with &lt;-. Let’s say that we learned that the second time was incorrect and we wanted to replace it with missing data. In R, missing data is noted by NA. Replace the second entry in times with NA. Now, let’s “cap” all entries that are bigger than 8 at 8 hours. If this is more than one value, why don’t we need to match the number of values? Recycling! Be careful of recycling! Let’s compute the mean of these new times: mean(times) #&gt; [1] 7.2 What happened? How do we compute the mean of the non-missing values? 7.2.6 Data frames We usually work with more than one variable at a time. When we do that, we will work with data frames. A data frame is a list of vectors, all of the same length. R has some data frames “built in”. For example, some car data is attached to the variable name mtcars. Print mtcars to screen. Notice the tabular format. Your turn: Finish the exercises of this section: Use some of these built-in R functions to explore mtcars head() tail() str() nrow() ncol() summary() row.names() names() What’s the first column name in the mtcars dataset? Which column number is named \"wt\"? With data frames,each column is its own vector. You can extract a vector by name using $. For example, we can extract the cyl column with mtcars$cyl. You can also extract columns using [[]]. For example, try mtcars[[\"cyl\"]] or mtcars[[2]]. Extract the vector of mpg values. What’s the mean mpg of all cars in the dataset? 7.2.7 R packages Often, the suite of functions that “come with” R are not enough to do an analysis. Sometimes, the suite of functions that “come with” R are not very convenient. In these cases, R packages come to the rescue. These are “add ons”, each coming with their own suite of functions and objects, usually designed to do one type of task. CRAN stores many R packages that. It’s easy to install packages from CRAN using the install.packages() function. Run the following lines of code to install the tibble and gapminder packages. (But don’t include install.packages() lines in your scripts—it’s not very nice to others!) install.packages(&quot;tibble&quot;) install.packages(&quot;palmerpenguins&quot;) tibble: a data frame with some useful “bells and whistles” palmerpenguins: a package with the penguins dataset (as a tibble!) After you install a package, you need to load it to use it. Use the library() function to load a package. (Note: Do not use the similar function require() to load packages. This has some different, undesirable, behavior for normal usage.) Run the following lines of code to load the packages. (Put these in your scripts, and near the top.) library(tibble) library(palmerpenguins) You can explore the objects in a package in the Environment pane. Try the following two approaches to access information about the tibble package. Run the lines one-at-a-time. Vignettes are your friend, but do not always exist. ?tibble browseVignettes(package = &quot;tibble&quot;) Print out the penguins object to screen. It’s a tibble—how does it differ from a data frame in terms of how it’s printed? "],["markdown.html", "Part 8 Markdown 8.1 Markdown syntax 8.2 RMarkdown", " Part 8 Markdown Markdown is a lightweight syntax for writing documents. Markdown documents can contain text, formatting, images, links, and more! Some cheat sheets for “quick reference”: GitHub’s markdown cheatsheet RStudio’s RMarkdown cheatsheet Further reading: The Rmd website has a fantastic walk-through tutorial that gives a great overview of RMarkdown. There’s also a nice overview video on the Rmd website, too. Yihui’s Rmd book for lots more on RMarkdown. Other explorations of this content: Interactive tutorial for learning markdown. The Happy Git with R: Rmd test drive. 8.1 Markdown syntax Markdown is plain text with a straightforward, readable way of marking up your text. Let’s see GitHub’s cheat sheet. We will use RStudio to convert Markdown to output formats like HTML or PDF. 8.1.1 Make a Markdown document Make a new Markdown file in RStudio, then save it as exploring_markdown.md. Add some text, such as introducing yourself and what your favorite animal is. Mark up the text with some markdown features (e.g., bold, italic, bullets, a link to a URL on the internet). Use the file extension .md for regular Markdown files with no R code in them. 8.1.2 Render exploring_markdown.Rmd We can use RStudio to convert our plain text Markdown document into various output formats. Above the script editor in RStudio, click the Preview or Knit button and convert your file to HTML. 8.1.3 Output formats There are generally two prominent file types to display documents of various types: pdf: This is useful if you intend to print your work onto a physical sheet of paper, or for presentation slides. If this is not the primary purpose, then try to avoid it, because formatting things so that it fits to the page can be more effort than its worth (unless you’re making presentation slides). - Example: Most journals articles and preprints. html: This is what you see when you visit a webpage. Content does not need to be partitioned to pages. - Example: My website main page, and its corresponding html file. We’ll be treating pdf and html files as output that should not be edited. Markdown is the source that is edited. 8.1.4 Word processor formats It is also possible to output files to word processor formats, such as Word (.docx), LibreOffice/OpenDocument (.odt), or Rich Text (.rtf). You can also output to other slideshow software, such as PowerPoint (.pptx) or LibreOffice/OpenDocument Slides (.odp). We aren’t going to use these in this class because we will focus on making fully reproducible documents. There are times when you have to use these formats (e.g., a journal requires Word, a conference requires PowerPoint, your advisor or collaborator requires Word). If you have to do this, try to avoid editing your R output in these formats. If you need to make revisions, go back and make changes to the source code, rather than the rendered document. In your general work, you can find a balance working with automated output and Word documents, but we are going to focus on fully reproducible documents in this class. 8.1.5 Other Output Formats RMarkdown can be rended to many other formats that we won’t have time to cover (see the RMarkdown documentation and the pandoc documentation). 8.2 RMarkdown RMarkdown (Rmd) combines Markdown and R scripts into one! It includes code chunks with R code (or other languages) that is run before the document is knitted. Here’s RStudio’s cheat sheet on Rmd. You can see that it has more features than “regular” markdown! 8.2.1 Code Chunks The parts of your document inside the “fences” ``` are code chunks. When you render the RMarkdown document, R will run the code in the chunks and show the output in the rendered document. You can run the code from a chunk interactively by placing your cursor on the line and typing Ctrl/Cmd + Enter/Return or by clicking the green “play” button at the top right of the code chunk. Add a new code chunk by doing one of these: Clicking the Insert button and choosing R or by typing -&gt; “R” Typing Mac: Cmd + Option + I or Windows: Ctrl + Alt + I Manually typing three back ticks followed by {r} in curly brackets: ```{r}, then typing three back ticks on a later line to “close” the code block: ```. Add a code chunk near the top of the file and load the tibble package. library(tibble) library(knitr) If you don’t have knitr installed, install it with install.packages(\"knitr\"). 8.2.2 Rendering output In a new code chunk, convert the mtcars data frame to a tibble using the tibble::as_tibble() function and assign it as a new object (e.g., called mtcars_tbl). Print it out by typing its name or using the print() function. When you print with just the print() function, your table will look like R console script in your output HTML or PDF. To make your tables look nicer in the output, use the knitr::kable() function to convert the results to a Markdown table. In a new code chunk, print the mtcars_tbl using knitr::kable(). We will explore other table tools in future classes. Add some markdown commentary about the tables you are showing. Your markdown commentary needs to go outside of the code chunks. You can also include R code “in-line” with markdown text. This is useful, for example, in your results section of a paper to report the results of your analyses without having to copy-paste them (and make errors). Add an in-line code chunk specifying the number of rows of the mtcars dataset like this: The `mtcars` dataset has 32 rows. Now, “Knit” to HTML. "],["github.html", "Part 9 GitHub and Version Control 9.1 Learning Objectives 9.2 Resources 9.3 Git and GitHub 9.4 Understanding the GitHub flow 9.5 Making a GitHub Repository 9.6 Navigating a GitHub Repository 9.7 Cloning a GitHub repo to your computer 9.8 RStudio Projects 9.9 The Version Control Workflow", " Part 9 GitHub and Version Control 9.1 Learning Objectives By the end of this module, you will be able to: Understand the benefits of a version control workflow Commit file changes to GitHub Navigate the commit history of a repository and a file on GitHub 9.2 Resources If you want to learn more about Git and GitHub, check out: The GitHub “Hello World” is a nice intro activity A short video explaining what GitHub is Git and GitHub learning resources Understanding the GitHub flow How to use GitHub branches Interactive Git training materials 9.3 Git and GitHub We will be using GitHub a lot in this course. GitHub is similar to Dropbox, OneDrive, or other cloud storage services. There are 3 key benefits of GitHub compared to those other services. GitHub is a version control platform. It is designed to track exactly what changes you make and when you make them. You can browse the history GitHub is designed for code. You can track what changes you make line by line You can integrate your code with automated tools for error checking, testing, etc. GitHub is designed for collaborative coding. One master copy People can make simultaneous edits No emailing files back and forth Tools for checking and verifying each other’s code We will practice using GitHub for collaboration through the weekly homework. 9.4 Understanding the GitHub flow The GitHub flow is a lightweight workflow that allows you to experiment and collaborate on your projects easily, without the risk of losing your previous work. 9.4.1 Repositories (“Repos”) A repository is where your project work happens–think of it as your project folder. It contains all of your project’s files and revision history. You can work within a repository alone or invite others to collaborate with you on those files. You should make a new GitHub repo for each project. In this class, you will make a repo for your lab activities and one for each of your portfolio pieces. 9.4.2 Cloning When a repository is created with GitHub, it’s stored remotely in the ☁️. You can clone a repository to create a local copy on your computer and then use Git to sync the two. This is similar to tools like Dropbox, but designed for code with a detail change history. Cloning a repository also pulls down all the repository data that GitHub has at that point in time, including all versions of every file and folder for the project! This can be helpful if you experiment with your project and then realize you liked a previous version more. To learn more about cloning, read “Cloning a Repository”. 9.4.3 Committing and pushing Committing and pushing are how you can add the changes you made on your local machine to the remote repository in GitHub. That way your instructor and/or teammates can see your latest work when you’re ready to share it. Any changes you make on your local computer aren’t “final” until you “commit” them (lock them in and write them into the repo history) and “push” those commits up to the GitHub cloud. You can make a commit when you have made changes to your project that you want to “checkpoint.” You can also add a helpful commit message to remind yourself or your teammates what work you did (e.g. “Added a README with information about our project”). Once you have a commit or multiple commits that you’re ready to add to your repository, you can use the push command to add those changes to your remote repository. Committing and pushing may feel new at first, but I promise you’ll get used to it 🙂 9.5 Making a GitHub Repository Let’s make a GitHub repository that you will use for this class. On https://GitHub.com, make a new repository called “progdata-class”. As you make it, do the following: Give your repo a useful description. Make it Public. Check “Add a README file” Check “Add .gitignore” and select “R” from the dropdown. 9.6 Navigating a GitHub Repository Let’s take a look at various parts of a GitHub repo. Files and folders/directories (“code”) History Issues There are other important features of GitHub (branches, issues) that we aren’t going to use in class for now. Talk to me if you want to learn more. Let’s practice editing a file with GitHub. Make a new file on your participation repository and edit it: - Click on the &quot;Create New File&quot; button on your repository&#39;s home page. - Call it `navigating_github.md` - Write something (e.g., what&#39;s your favorite color?). - Commit (&quot;save&quot;) the file by clicking on green &quot;commit new file&quot; button at the bottom of the page. - Now type something else (e.g., what&#39;s your favorite animal?) and commit the change. Now look at your commit history in the “History” tab. 9.7 Cloning a GitHub repo to your computer You can download a copy of your repository to your computer (cloning it), make changes (commits) there, then push them back to GitHub. Click big green “Code” button at the top of your repo page and choose “Open with GitHub Desktop”. You can also do this from the File menu in GitHub Desktop. Note where the repository folder is being saved on your computer. Save it somewhere easy to find and not in a cloud folder. Go to the folder on your computer. Any changes you make to files in this folder are tracked by the GitHub Desktop app and can be committed and pushed up to GitHub. Open your navigating_github.md file, make a change, then save, commit, and push it. Go look at your change on GitHub. 9.8 RStudio Projects RStudio projects are a way to manage which folder R runs in on your computer, where it looks for files to read in, and where it writes its output. In RStudio, click File → New Project… → Existing Directory. Navigate to your GitHub repo folder. This will add an RStudio Project (.Rproj) file to the folder. 9.8.1 The working directory When you open R, it “runs” in some folder on your computer. This is the place it will look for files to import and write files as output. Think about where your Rmd output files end up when you knit them. If you have R/RStudio closed, and you open a .R or .Rmd file, R/RStudio will start in the folder holding that file. If you open R/RStudio from the Windows Start menu, the Mac dock, the Mac Spotlight, etc., R/Studio will start in its default location (probably your user home directory, see Tools → Global Options → General → Default working directory…). When I say “R/Studio will start in…”, what I am referring to is R’s “working directory”. Like I say above, this is the place R will look for files to import and write files as output. You can check what R’s current working directory is using the getwd() function: getwd() #&gt; [1] &quot;/Users/runner/work/progdata-class/progdata-class&quot; You can also change the working directory using the setwd() function: setwd(file.path(&quot;path&quot;, &quot;to&quot;, &quot;folder&quot;)) Do not use setwd()! You should always write your R scripts so that the entire project is self-contained in a folder. All of the scripts, folders, data, output, etc. should all “live” within this project folder. We will talk a lot about how to do this throughout the semester. For now, we will start by working with RStudio Projects to make this easier. When you double click on a .Rproj file, it: Opens a new fresh R session, with The working directory set to the location of the .Rproj file, and No connection whatsoever to any other R sessions you already have open You can also set specfic options for each RStudio project (e.g., number of spaces to insert when you type Tab, etc.). Let’s practice closing RStudio and re-opening it by opening the RStudio project. Run getwd() to see where R is running. 9.9 The Version Control Workflow Now, let’s practice the workflow to work with your files with GitHub. 9.9.1 Editing a file and making a commit Open the README.md file in your local git folder by clicking on it in the Files pane. README.md is a special file that will show when viewing a folder on GitHub. Use README files to describe the contents of a folder and what’s going on there. Type “Hello world” and save the file. Now, go back to the GitHub Desktop app. It shows you the files in the git repo folder that have changed since the last commit. Any files shown here have had changes made. (Remember, you need to save the file in RStudio [so the title on the Source tab isn’t blue] before they appear in this list.) Changes shown here are not saved in the repo history until you commit them. If you’ve changed several files but want to only commit changes for some, uncheck the box next to the files you don’t want to commit yet. You can view the changes in more detail by selecting the file name. If you want to undo any changes and revert back to the previous committed version of a file, right click on the file name and click “Revert changes”. To commit your changes, type a at the bottom of the right column describing what you’ve changed. Always give informative commit messages (help out future you!). When you are ready, click the “Commit” button. Now, you’ve made the commit locally, but you need to “push” it to GitHub so that it shows up there as well. Click the “Push” (up arrow) button. Now go check out the file on GitHub online. 9.9.2 Fetching or Pulling Changes from the Remote Repo One of the amazing things about git is that it can track changes made to a file at different locations or on different computers and reconcile them together. (This is how it is so useful for collaboration!) Let’s see how that works. First, let’s make a change directly on the GitHub website. Edit README.md and type “oops!” at the top. Commit your change. Go back to GitHub Desktop Click the “Fetch Origin” button. See how the file on your local computer changed to reflect the change you made online. Let’s fix that “oops!”. Delete it, commit your change (give an informative commit message), and push your changes back online. 9.9.3 The General Workflow You will use this basic workflow througout the semester (and your coding career). When you sit down to start working: Open GitHub Desktop and select your repo. Click the Fetch Origin button to fetch remote changes and get your local repo copy up to date. Open you RStudio Project. Make changes to your files. Commit your changes. Push your commits back up to the remote repo (GitHub). You should get in the habit of commit early, commit often. Don’t wait until you are completely done with your work to commit the changes. Make small commits as you go. This makes it much easier to go back if you accidentally break something and need to revert. "],["lab-1-getting-started-with-rstudio-git-and-markdown.html", "Part 10 Lab 1: Getting started with RStudio, Git, and Markdown 10.1 Part 1: Set up R and RStudio 10.2 Part 2: Make a README.md file with Markdown 10.3 Part 3: Practice some R fucntions 10.4 Part 4: Make a GitHub Repository for your work", " Part 10 Lab 1: Getting started with RStudio, Git, and Markdown In this lab you will do 4 things: Get acquainted with RStudio and customize it to your liking. Practice writing a document with Markdown. Practice using some basic functions in R. Make a GitHub repository to save and share your work. 10.1 Part 1: Set up R and RStudio First, install two packages we will use a lot in class: tidyverse palmerpenguins Next, customize some RStudio settings: Click Tools → Global Options… 1. On the first page, under Workspace: Uncheck Restore .RData into workspace at startup Set Save workspace to .RData on exit to Never Under History: Uncheck Always save history (even when not saving .RData) Click the Advanced button at the top. Under Other: Check Show .Last.value in environment listing Now, adjust the appearance of RStudio to your liking: In the Pane Layout window, you can rearrange RStudio’s 4 panes. In the Appearance window, you can change your font size, code typeface, and RStudio color scheme. - I use the Tomorrow Night Bright color scheme. In the Code window, you can change various ways about how code is inserted and shown. Change 3 settings from their defaults: On the Editing page, under General, check Use native pipe operator |&gt; (requires R 4.1+) On the Editing page, under Execution ,set Ctrl/Cmd+Enter executes to Multiple consecutive R lines On the Display page, check Rainbow parentheses In the RMarkdown window, you can change how RMarkdown is shown while you edit it. - Uncheck Show output inline for all RMarkdown documents. Go ahead and customize your RStudio appearance (color theme, font, etc.) and pane layout to whatever you find appealing. You can always come back and change these later until you find a setup you like. 10.2 Part 2: Make a README.md file with Markdown In RStudio, make a Markdown file from the File menu. Save this file with the name README.md. README should be in all caps. This file will be the introduction to your class GitHub repo. Use it to introduce yourself and practice your Markdown skills. The beginning of the README should contain a very brief description as to what the repository is (a sentence or two), so that a visitor landing on the repository can orient themselves. You should also help the visitor navigate your repository (in whatever way you think is most appropriate). Then, introduce yourself briefly. When writing your README, be sure to showcase at least five functionalities of GitHub-flavored markdown. The markdown cheatsheet might help here, or, the Help menu in RStudio will bring up a Markdown Quick Reference at any time. Here’s a sample README file that goes way above and beyond what I’m looking for (aside from describing the repo). 10.3 Part 3: Practice some R fucntions In RStudio, make an R script file from the File menu. In this file, you will explore the palmerpenguins::penguins dataset. Use some R functions to explore this data frame a bit. For example, what is the general structure of the dataset? How many rows does it have? How many colums? What are the descriptive statistics for some of the important variables? You should: 1. Demonstrate the use of at least three functions. 2. Write code comments with # describing what the functions you are using do. Save your R script with a useful name, such as lab01_exploring-penguins.R. 10.4 Part 4: Make a GitHub Repository for your work On GitHub.com, make a new repository called “progdata-class”. As you make it, do the following: Give your repo a useful description. Make it Public. Check “Add a README file” Check “Add .gitignore” and select “R” from the dropdown. Clone your repo to the GitHub Desktop app. If you are using a Mac anywhere, also do this: 6. In RStudio, open the .gitignore file. Add “.DS_Store” (no quotes) to the end of the file. Save the file and commit the change. Now, add your files for this lab to your repo and commit them. Remember, commit early, commit often! Make small changes at a time. Write a useful commit message for each commit. Add your README.md file and commit it. Add your data exploration R script to a folder called “Lab 1” and commit it. Add any other scripts or files you made in class to the Lab 1 folder as well. Push your changes up to GitHub. Finally, copy a link to your repo to the thread on the class Teams. "],["plotting-with-ggplot2.html", "Part 11 Plotting with ggplot2 11.1 Learning Objectives 11.2 Resources", " Part 11 Plotting with ggplot2 Set up the workspace: # Load required packages library(tidyverse) # loads ggplot2 and other tidyverse packages library(gapminder) # loads the gapminder dataset # Set a default figure size knitr::opts_chunk$set(fig.width = 5, fig.height = 4, fig.align = &quot;center&quot;) 11.1 Learning Objectives By the end of this lesson, you will be able to: Have a sense of why we’re learning ggplot2 Understand the importance of statistical graphics in communicating information Identify the components of the grammar of graphics underlying ggplot2 Use different geometric objects and aesthetics to explore various plot types 11.2 Resources Here are some good walkthroughs that introduce ggplot2: r4ds: data-vis chapter. The ggplot2 book, Chapter 2 Jenny Bryant’s ggplot2 tutorial Andrew Heiss’s data visualization course Here are some good resource to use as a reference: ggplot2 cheatsheet R Graphics Cookbook "],["plotting-in-r.html", "Part 12 Plotting in R 12.1 Just Plot It", " Part 12 Plotting in R TL;DR: We’re using ggplot2 in progdata. R has several frameworks for building graphics. One of the earliest advantages when R (and its predecessor S) were introduced in the 1980s was its professional plotting capabilities. However, the “base R” plotting methods, mostly accessed using the plot() function, can be involved, and requires a lot of “drawing by hand”. That said, for simple “quick looks” at data, base R plots can be useful: plot(airquality[,1:4]) The most widely used modern plotting tool in R is ggplot2, which provides a very powerful framework for making plots. It has a theoretical underpinning, too, based on the Grammar of Graphics, first described by Leland Wilkinson in his “Grammar of Graphics” book. With ggplot2, you can make a great many type of plots with minimal code. It’s been a hit in and outside of the R community. A big advantage of ggplot2 is that many people have written extensions for it, such as ggdist (for plotting distributions and intervals), gganimate (for animations and GIFs), plotly for interactive graphs, and see (ready-built visualizations for many models). 12.1 Just Plot It The human visual cortex is a powerful thing. If you’re wanting to point someone’s attention to a bunch of numbers, you probably won’t get any “aha” moments by displaying a large table like this, either in a report or (especially!) a presentation. Make a plot to communicate your message! Florence Nightingale was a pioneer in many fields (e.g., nursing, sanitation, public health, statistics). Her contributions to statistics (a field generally prohibited to women in her era) were centered around communicating data using visualization. Picture of Florence Nightingale During the Crimean Wars, she convinced the British military to implement a variety of sanitation measures through effective and innovative data visualization. Florence Nightingale’s Rose Diagram If you really feel the need to tell your audience every number exactly, consider putting your table in an appendix. Because chances are, the reader doesn’t care about the exact numeric values. Or, perhaps you just want to point out one or a few numbers, in which case you can put that number directly on a plot. "],["the-grammar-of-graphics.html", "Part 13 The Grammar of Graphics 13.1 Example: Scatterplot grammar 13.2 Activity: Bar chart grammar", " Part 13 The Grammar of Graphics You can think of the grammar of graphics as a systematic approach for describing the components of a graph. It breaks down “classic” plots into individual components that let us make more complex, nuanced, and informative graphic through novel combinations. The grammar of graphics has seven components (the ones in bold are required explicitly ggplot2): Data The data that you’re feeding into a plot. Aesthetic mappings How are variables (columns) from your data connect to a visual dimension? Horizontal (x) positioning, vertical (y) positioning, size, color, shape, etc. These visual dimensions are called “aesthetics” Geometric objects What are the objects that are actually drawn on the plot? A point, a line, a bar, a histogram, a density, etc. Scales How is a variable mapped to its aesthetic? Will it be mapped linearly? On a log scale? Something else? This includes things like the color scale e.g., c(control, treatment_1, treatment_2) -&gt; c(“blue”, “green”, “red”) Statistical transformations Whether and how the data are combined/transformed before being plotted e.g., in a bar chart, data are transformed into their frequencies; in a box-plot, data are transformed to a five-number summary. Coordinate system This is a specification of how the position aesthetics (x and y) are depicted on the plot. For example, rectangular/Cartesian, or polar coordinates. Facet This is a specification of data variables that partition the data into smaller “sub plots”, or panels. 13.1 Example: Scatterplot grammar For example, consider the following plot from the gapminder data set. For now, don’t focus on the code, just the graph itself. ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(alpha = 0.1) + scale_x_continuous( name = &quot;GDP per capita&quot;, trans = &quot;log10&quot;, labels = scales::dollar_format() ) + theme_bw() + scale_y_continuous(&quot;Life expectancy&quot;) This scatterplot has the following components of the grammar of graphics. Grammar Component Specification data gapminder aesthetic mapping x: gdpPercap, y: lifeExp geometric object points scale x: log10, y: linear statistical transform none coordinate system rectangular faceting none Note that x and y aesthetics are required for scatterplots (or “point” geometric objects). Each geometric object has its own required set of aesthetics. 13.2 Activity: Bar chart grammar Consider the following plot. Don’t concern yourself with the code at this point. gapminder |&gt; filter(year == 2007) |&gt; mutate(continent = fct_infreq(continent)) |&gt; ggplot() + aes(x = continent, fill = continent) + geom_bar() + guides(fill = &quot;none&quot;) + theme_bw() Fill in the seven grammar components for this plot. Grammar Component Specification data gapminder aesthetic mapping FILL_THIS_IN geometric object FILL_THIS_IN scale FILL_THIS_IN statistical transform FILL_THIS_IN coordinate system FILL_THIS_IN faceting FILL_THIS_IN "],["working-with-ggplot2.html", "Part 14 Working with ggplot2", " Part 14 Working with ggplot2 First, the ggplot2 package comes with the tidyverse meta-package. You can just load tidyverse, and it will also load ggplot2. Let’s use the above scatterplot as an example to see how to use the ggplot() function. First, we will pass one argument to ggplot() function itself: - data: the data frame containing your plotting data ggplot(gapminder) After this, we add additional functions as layers to add features to the plot and control their appearance. The first thing we will add is aes(). The aes() function tells R to look in the data for variable names to map them to different aesthetic features, such as x-position, y-position, or color. ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) Next, we want to add some geometric shapes to the plot. These use functions with the form geom_SOMETHING(). Different plot shapes (geom_SOMETHING) accept different aes() arguments. These are listed in the help file for the geom function: ?geom_point, ?geom_line, etc. ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point() You can control aesthetics using constants or variables from outside data by specifying them outside of aes(). For example, to make all of the shapes of plot blue, you can add: color = \"blue\". ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(color = &quot;blue&quot;) There’s a bit of overplotting (overlapping symbols), so let’s also make the points semi-transparent. This is controlled using the alpha argument (you need 1/alpha points overlaid to achieve a solid point). ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(color = &quot;blue&quot;, alpha = .1) For now, that’s the only geom that we want to add. Now, let’s specify a scale transformation, because the plot would really benefit if the x-axis is on a logarithmic scale. These functions take the form scale_AESTHETIC_TYPE(). As usual, you can tweak this layer, too, using this function’s arguments. In this example, we’re re-naming the x-axis (the name argument), transform the values (the trans argument), and changing the labels to have a dollar format (the labels argument). ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(color = &quot;blue&quot;, alpha = .1) + scale_x_continuous( name = &quot;GDP per capita&quot;, trans = &quot;log10&quot;, labels = scales::dollar_format() ) I’m tired of seeing the ugly default gray background, so I’ll add a theme() layer. I like theme_bw() (you can tweak themes later, too!). Then, I’ll re-label the y-axis using the ylab() function. Et voilà! ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(color = &quot;blue&quot;, alpha = .1) + scale_x_continuous( name = &quot;GDP per capita&quot;, trans = &quot;log10&quot;, labels = scales::dollar_format() ) + theme_bw() + ylab(&quot;Life expectancy&quot;) "],["activity-build-some-plots.html", "Part 15 Activity: Build some plots! 15.1 Make a Line Chart 15.2 Make a Scatterplot 15.3 Assigning a ggplot2 object and adding to it 15.4 Fix Me!", " Part 15 Activity: Build some plots! 15.1 Make a Line Chart The following code makes a data frame called mauna that contains time series data of CO\\(_2\\) concentrations collected monthly at the Mauna Loa vocanic observation station. Execute this code to store the data in mauna: (mauna &lt;- tsibble::as_tsibble(co2) |&gt; rename(month = index, conc = value)) #&gt; # A tsibble: 468 x 2 [1M] #&gt; month conc #&gt; &lt;mth&gt; &lt;dbl&gt; #&gt; 1 1959 Jan 315. #&gt; 2 1959 Feb 316. #&gt; 3 1959 Mar 316. #&gt; 4 1959 Apr 318. #&gt; 5 1959 May 318. #&gt; 6 1959 Jun 318 #&gt; 7 1959 Jul 316. #&gt; 8 1959 Aug 315. #&gt; 9 1959 Sep 314. #&gt; 10 1959 Oct 313. #&gt; # … with 458 more rows Produce a line chart showing the concentration over time. Specifically, the plot should have the following grammar components: Grammar Component Specification data mauna aesthetic mapping x: month, y: conc geometric object lines scale yearmonth statistical transform none coordinate system rectangular faceting none Fill in the blanks to obtain the plot: ggplot(FILL_THIS_IN) + aes(FILL_THIS_IN, FILL_THIS_IN) FILL_THIS_IN() + tsibble::scale_x_yearmonth() 15.2 Make a Scatterplot Use the palmerpenguins::penguins data to make a scatterplot with the following specifications: Grammar Component Specification data palmerpenguins::penguins aesthetic mapping x: body_mass_g, y: bill_depth_mm geometric object points, smoothed lines scale linear statistical transform none coordinate system rectangular faceting none ggplot(FILL_THIS_IN) + aes(FILL_THIS_IN, FILL_THIS_IN) FILL_THIS_IN() + geom_smooth() You can control aesthetics for individual layers by adding an aes() inside the layer function, like this: geom_point(aes(color = COLOR_VARIABLE)) Modify your code above so that the points (but not the smooth line) have their color mapped to species: ggplot(FILL_THIS_IN) + aes(FILL_THIS_IN, FILL_THIS_IN) FILL_THIS_IN() + geom_smooth() Now, instead, map color in the global aes() call for the plot. What happens? ggplot(FILL_THIS_IN) + aes(FILL_THIS_IN, FILL_THIS_IN) FILL_THIS_IN() + geom_smooth() Things to keep in mind: Aesthetics mapped outside of a specific layer apply globally Aesthetics mapped inside a geom layer apply only to that layer. If the same aesthetic appears both globally and in a layer, the layer-specific one wins 15.3 Assigning a ggplot2 object and adding to it You can store the output of the plot in a variable. Assign the mauan plot above to a variable named p, then add a layer to p that adds a dark green smoothed line to the plot. FILL_THIS_IN p + FILL_THIS_IN() 15.4 Fix Me! What’s wrong with the following code? Fix it. ggplot(gapminder) + geom_point(x = gdpPercap, y = lifeExp, alpha = 0.1) What’s wrong with this code? Fix it. ggplot(cars) + geom_point(aes(x = speed, y = dist, color = &quot;blue&quot;)) "],["example-geoms-and-plots.html", "Part 16 Example geoms and Plots 16.1 Bar chart 16.2 Stacked bar chart 16.3 Pie chart 16.4 Histogram 16.5 Density 16.6 Dot plot 16.7 Scatterplots 16.8 More scatterplots 16.9 Building complex plots 16.10 Scatterplots for change 16.11 Comparing distributions 16.12 Scatterplot matrix 16.13 Smoothers and Exploratory Data Analysis", " Part 16 Example geoms and Plots 16.1 Bar chart theme_set(theme_classic()) ggplot(penguins) + aes(x = species) + geom_bar() 16.2 Stacked bar chart ggplot(penguins) + aes(y = island, color = fct_rev(species), fill = fct_rev(species), label = fct_rev(species)) + stat_count(orientation = &quot;y&quot;) + guides(color = guide_none(), fill = guide_none()) + ylab(NULL) + stat_count(geom = &quot;label&quot;, color = &quot;white&quot;) 16.3 Pie chart ggplot(penguins) + aes(x = factor(1), fill = species, label = species) + geom_bar(width = 1) + stat_count(geom = &quot;text&quot;, size = 5, color = &quot;white&quot;, position = position_stack(vjust = .5) ) + guides(y = guide_none(), x = guide_none(), fill = guide_none()) + xlab(NULL) + ylab(NULL) + coord_polar(theta = &quot;y&quot;) + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank()) 16.4 Histogram ggplot(penguins) + aes(x = bill_length_mm) + geom_histogram(binwidth = 1) #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). 16.5 Density ggplot(penguins) + aes(x = bill_length_mm) + geom_density() #&gt; Warning: Removed 2 rows containing non-finite values (stat_density). 16.6 Dot plot ggplot(penguins) + aes(x = bill_length_mm) + geom_dotplot(binwidth = 1, dotsize = .5) + guides(y = guide_none()) #&gt; Warning: Removed 2 rows containing non-finite values (stat_bindot). 16.7 Scatterplots # Scatterplot 1 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm) + geom_point() #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Scatterplot 2 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species, shape = species) + geom_point() #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Scatterplot 3 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species, shape = sex, size = body_mass_g) + geom_point() #&gt; Warning: Removed 11 rows containing missing values (geom_point). 16.8 More scatterplots # Scatterplot 4 ggplot(round(alr4::Heights)) + aes(x = mheight, y = dheight) + geom_point() # Scatterplot 5 ggplot(round(alr4::Heights)) + aes(x = mheight, y = dheight) + geom_jitter(height = .3, width = .3) 16.9 Building complex plots # Scatterplot 6 ggplot(penguins) + aes(x = species, y = flipper_length_mm, fill = species, color = species) + geom_point() #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Scatterplot 7 ggplot(penguins) + aes(x = species, y = flipper_length_mm, fill = species, color = species) + geom_jitter(height = 0, width = .4) #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Scatterplot 8 ggplot(penguins) + aes(x = species, y = flipper_length_mm, fill = species, color = species) + geom_jitter(height = 0, width = .4) + geom_boxplot(color = &quot;black&quot;, alpha = .5) #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Raincloud plot ggplot(na.omit(penguins)) + aes(y = species, x = flipper_length_mm, fill = species, color = species) + geom_jitter(height = .15) + geom_boxplot(color = &quot;black&quot;, alpha = .5, width = .1, size = .5) + ggdist::stat_slab(height = .3, color = &quot;black&quot;, size = .2, alpha = .5, position = position_nudge(y = .2)) 16.10 Scatterplots for change df &lt;- data.frame( id = 1:30, before = rnorm(30), after = rnorm(30)) df &lt;- tidyr::pivot_longer( df, -id, names_to = &quot;time&quot;, values_to = &quot;score&quot;) ggplot(df) + aes(x = time, y = score, group = id) + geom_point() + geom_line() 16.11 Comparing distributions df &lt;- data.frame( g = c(rep(&quot;a&quot;, times = 100), rep(&quot;b&quot;, times = 100), rep(&quot;c&quot;, times = 100), rep(&quot;d&quot;, times = 100), rep(&quot;e&quot;, times = 100)), z = c(rnorm(100, mean = 0, sd = 1), rnorm(100, mean = 1, sd = 2), rnorm(100, mean = 2, sd = 3), rnorm(100, mean = 3, sd = 4), rnorm(100, mean = 4, sd = 5)) ) # Overlapping densities ggplot(df) + aes(x = z, group = g, fill = g) + geom_density(size = .2, alpha = .5) # Ridge plot ggplot(df) + aes(x = z, y = g, fill = g) + ggridges::geom_density_ridges( size = .2, alpha = .5, scale = 4 ) #&gt; Picking joint bandwidth of 1.07 16.12 Scatterplot matrix penguins_focal &lt;- penguins[, c(&quot;species&quot;, &quot;bill_length_mm&quot;, &quot;flipper_length_mm&quot;, &quot;sex&quot;)] pairs(penguins_focal) GGally::ggpairs( penguins_focal, mapping = aes(color = species, alpha = .5), lower = list( continuous = &quot;smooth_loess&quot;, combo = &quot;facethist&quot;, discrete = &quot;facetbar&quot;, na = &quot;na&quot; ) ) + theme_classic() #&gt; Registered S3 method overwritten by &#39;GGally&#39;: #&gt; method from #&gt; +.gg ggplot2 #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). #&gt; Warning: Removed 2 rows containing non-finite values (stat_density). #&gt; Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : #&gt; Removed 2 rows containing missing values #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). #&gt; Warning: Removed 2 rows containing non-finite values (stat_density). #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). 16.13 Smoothers and Exploratory Data Analysis # Smoother 1 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth(color = &quot;black&quot;, fill = &quot;black&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 2 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth(color = &quot;black&quot;, fill = &quot;black&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;orange&quot;, fill = &quot;orange&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 3 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 4 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth() + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, linetype = &quot;dashed&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 5 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth(method = &quot;lm&quot;, linetype = &quot;dashed&quot;) + geom_smooth(color = &quot;black&quot;, fill = &quot;black&quot;, alpha = .2) + geom_smooth(method = &quot;lm&quot;, color = &quot;orange&quot;, fill = &quot;orange&quot;, alpha = .2) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 6 ggplot(penguins) + aes(x = flipper_length_mm, y = bill_length_mm, color = species, fill = species, shape = species, linetype = species) + geom_point(alpha = .7) + geom_smooth() + theme_bw() + theme(legend.position = &quot;bottom&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). "],["lab02.html", "Part 17 Lab 2: Global plastic waste 17.1 Learning goals 17.2 Getting started 17.3 Warm up 17.4 Exercises 17.5 Finishing Up", " Part 17 Lab 2: Global plastic waste Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010. Additionally, National Geographic ran a data visualization communication contest on plastic waste as seen here. 17.1 Learning goals Visualizing numerical and categorical data and interpreting visualizations Recreating visualizations Getting more practice using with R, RStudio, Git, and GitHub 17.2 Getting started Download this RMarkdown template for the lab. Save it to your class GitHub repo with a name like lab-02-plastic-waste.Rmd. In future labs, you will make your own RMarkdown documents from scratch. Download this dataset and save it to your class GitHub repo in a folder called data. 17.2.1 Packages We’ll use the tidyverse package for this analysis. Add code to load the tidyverse package to the setup chunk at the top of the document. 17.2.2 Data The following code will read in the data you saved to your repo. Add this to your document. plastic_waste &lt;- read_csv(here::here(&quot;data&quot;, &quot;plastic-waste.csv&quot;)) You can view this dataset using the dplyr::glimpse(), head(), and View(). Try these out in the Console. The variable descriptions are as follows: code: 3 Letter country code entity: Country name continent: Continent name year: Year gdp_per_cap: GDP per capita constant 2011 international $, rate plastic_waste_per_cap: Amount of plastic waste per capita in kg/day mismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day mismanaged_plastic_waste: Tonnes of mismanaged plastic waste coastal_pop: Number of individuals living on/near coast total_pop: Total population according to Gapminder 17.3 Warm up Notice that some cells in the data have the value NA — what does this mean? 17.4 Exercises Let’s start by taking a look at the distribution of plastic waste per capita in 2010. ggplot(data = plastic_waste) + aes(x = plastic_waste_per_cap) + geom_histogram(binwidth = 0.2) #&gt; Warning: Removed 51 rows containing non-finite values (stat_bin). One country stands out as an unusual observation at the top of the distribution. One way of identifying this country is to filter the data for countries where plastic waste per capita is greater than 3.5 kg/person. We will cover this function next week. For now, what do you think this code does? plastic_waste |&gt; filter(plastic_waste_per_cap &gt; 3.5) #&gt; # A tibble: 1 × 10 #&gt; code entity continent year gdp_per_cap plastic_waste_p… mismanaged_plast… #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 TTO Trinida… North Ame… 2010 31261. 3.6 0.19 #&gt; # … with 3 more variables: mismanaged_plastic_waste &lt;dbl&gt;, coastal_pop &lt;dbl&gt;, #&gt; # total_pop &lt;dbl&gt; Did you expect this result? You might consider doing some research on Trinidad and Tobago to see why plastic waste per capita is so high there, or whether this is a data error. 17.4.1 Exercise 1 Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? NOTE: From this point onwards, the plots and the output of the code are not displayed in the lab instructions, but you can and should the code and view the results yourself. Another way of visualizing numerical data is using density plots. Adapt your code above to use density plots instead of histograms. The y-axes for histograms and densities differ by default. Histograms have the raw counts. Densities have the density (think of it like proportion). If you want to put density plots and histograms on the same plot, we need to tell them to have the same y-axis. Plot histograms and densities on the same plot. In the geom_density() function, add aes(y = after_stat(count)) to tell it to put counts, not densities on the y-axis. Make just a density plot of plastic waste by continent Coloring the density curves by continent. The resulting plot may be a little difficult to read, so let’s also fill the curves in with colors as well. Make the fill color somewhat transparent to make the overlapping distributions easier to see. You may need to try several different transparency levels to find one that looks niece. Describe why we defined the color and fill of the curves by mapping aesthetics with aes() but defined the alpha level directly in the geom. 🧶 ✅ ⬆️ Now is a good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 17.4.2 Exercise 2 Yet another way to visualize differences in plastic waste distributions across continents is box plots. Make a plot with continent on the x-axis, plastic waste on the-axis, and fill of the box plots by continent. Adjust this plot so that it also shows individual data points. Add a density curve for each continent as well (i.e., make a “raincloud plot”). What does the density or data points show that the boxplot does not? 17.4.3 Exercise 3 Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. Color the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? Visualize the relationship between plastic waste per capita and total population; and between plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly associated? Add trend lines (either loess smooths or linear trends) to these plots. 🧶 ✅ ⬆️ Now is another good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 17.4.4 Bonus Recreate the following plot, and interpret what you see in context of the data. Hint: The x-axis is a calculated variable. One country with plastic waste per capita over 3 kg/day has been filtered out. And the data are not only represented with points on the plot but also a smooth curve. The term “smooth” should help you pick which geom to use. 17.5 Finishing Up 🧶 ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you’re happy with the final state of your work. Once you’re done, check to make sure your latest changes are on GitHub. "],["wrangle-yo-data-with-dplyr.html", "Part 18 Wrangle yo’ data with dplyr 18.1 Today’s Topics 18.2 Resources", " Part 18 Wrangle yo’ data with dplyr 18.1 Today’s Topics Today we’ll get started with learning to “wrangle” data— that is, to subset it, rearrange it, transform it, summarize it, and otherwise make it ready for analysis. We are going to be working with the dplyr package. Specifically, we’re going to consider three lessons today: Intro to dplyr syntax The |&gt; pipe and the dplyr advantage filter; relational/comparison and logical operators in R Specific dplyr functions we will cover select() arrange() filter() mutate() summarize() group_by() grouped mutate() grouped summarize() recode() across() rowwise() 18.2 Resources stat545: dplyr-intro stat545: dplyr-single r4ds: transform chapter. Here are some supplementary resources: A similar resource to the r4ds one above is the intro to dplyr vignette. Want to read more about piping? See r4ds: pipes. Some advanced topics you might find useful: For window functions and how dplyr handles them, see the window-functions vignette for the dplyr package. For time series data, see the tsibble demo "],["selecting-and-sorting-data-frames.html", "Part 19 Selecting and sorting data frames 19.1 Learning Objectives 19.2 Preamble 19.3 Demonstration", " Part 19 Selecting and sorting data frames 19.1 Learning Objectives Here are the concepts we’ll be exploring in this lesson: tidyverse dplyr functions: select() arrange() piping By the end of this lesson, students are expected to be able to: subset and rearrange data with dplyr use piping (|&gt;) when implementing function chains 19.2 Preamble Let’s talk about: The history of dplyr: plyr Don’t use both in one script! My recommendation, don’t use plyr at all at this point. tibbles are a special type of data frame The tidyverse Package functions and masking Load the tidyverse package: library(tidyverse) 19.3 Demonstration Let’s get started with an exercise: Download this Rmd worksheet. Save it to your class GitHub repo with a name like exercise-dplyr.Rmd. Open the worksheet in RStudio. Follow along in the .Rmd file until the Back to Guide section. "],["the-pipe-advantage.html", "Part 20 The pipe advantage 20.1 Learning Objectives 20.2 Compare nested functions to pipe chains 20.3 The workflow: 20.4 Basic principles: 20.5 Tangent: Base R workflow", " Part 20 The pipe advantage 20.1 Learning Objectives By the end of this lesson, you will: Have a sense of why dplyr is advantageous compared to the “base R” way with respect to good coding practice. Why? Having this in the back of your mind will help you identify qualities of and produce a readable analysis. 20.2 Compare nested functions to pipe chains Self-documenting code. This is where the tidyverse shines. Example of dplyr vs base R: gapminder[gapminder$country == &quot;Cambodia&quot;, c(&quot;year&quot;, &quot;lifeExp&quot;)] vs. gapminder |&gt; filter(country == &quot;Cambodia&quot;) |&gt; select(year, lifeExp) Morning Routine Pipie 20.3 The workflow: Wrangle your data with dplyr first Separate steps with |&gt; Pipe |&gt; your data into a plot/analysis 20.4 Basic principles: Do one thing at a time Transform variables OR select variables OR filter cases Chain multiple operations together using the pipe |&gt; Use readable object and variable names Subset a dataset (i.e., select variables) by name, not by “magic numbers” Note that you need to use the assignment operator &lt;- to store changes! 20.5 Tangent: Base R workflow We are jumping right into the tidyverse way of doing things in R, instead of the base R way of doing things. Our first week was about “just enough” base R to get you started. If you feel that you want more practice here, take a look at the R intro stat videos by MarinStatsLectures. "],["filtering-and-mutating-data-frames.html", "Part 21 Filtering and mutating data frames 21.1 Learning Objectives 21.2 R Operators 21.3 Demonstration", " Part 21 Filtering and mutating data frames 21.1 Learning Objectives Here are the concepts we’ll be exploring in this lesson: Relational/comparison operators Logical operators dplyr functions: filter mutate By the end of this lesson, you will be able to: Predict the output of R code containing the above operators. Explain the difference between &amp;/&amp;&amp; and |/||, and name a situation where one should be used over the other. Subsetting and transforming data using filter and mutate 21.2 R Operators Arithmetic operators allow us to carry out mathematical operations: Operator Description + Add - Subtract * Multiply / Divide ^ Exponent %/% Integer division %% Modulus (remainder from integer division) Relational operators allow us to compare values: Operator Description &lt; Less than &gt; Greater than &lt;= Less than or equal to &gt;= Greater than or equal to == Equal to != Not equal to Arithmetic and relational operators work on vectors. There is another very useful relational function, %in%: c(1, 2, 3, 4, 5) %in% c(1, 2) Logical operators allow us to carry out boolean operations: Operator Description ! Not | Or (element_wise) &amp; And (element-wise) || Or &amp;&amp; And The difference between | and || is that || evaluates only the first element of the two vectors, whereas | evaluates element-wise. 21.3 Demonstration Continue along with the worksheet until Back to Guide Again. "],["grouping-and-summarizing-data.html", "Part 22 Grouping and summarizing data 22.1 summarize() 22.2 group_by() 22.3 Grouped summarize() 22.4 Grouped mutate() 22.5 Function types", " Part 22 Grouping and summarizing data 22.1 summarize() Like mutate(), the summarize() function also creates new columns, but the calculations that make the new columns must reduce down to a single number. For example, let’s compute the mean and standard deviation of life expectancy in the gapminder data set: gapminder |&gt; summarize( mean = mean(lifeExp), sd = sd(lifeExp) ) Notice that all other columns were dropped. This is necessary, because there’s no obvious way to compress the other columns down to a single row. This is unlike mutate(), which keeps all columns, and more like transmute(), which drops all other columns. As it is, this is useful for creating summary tables, but it’s more useful in the context of grouping, coming up next. 22.2 group_by() The true power of dplyr lies in its ability to group a tibble, with the group_by() function. As usual, this function takes in a tibble and returns a (grouped) tibble. Let’s group the gapminder dataset by continent and year: gapminder |&gt; group_by(continent, year) The only thing different from a regular tibble is the indication of grouping variables above the tibble. This means that the tibble is recognized as having “chunks” defined by unique combinations of continent and year: Asia in 1952 is one chunk. Asia in 1957 is another chunk. Europe in 1952 is another chunk. etc… Notice that the data frame isn’t rearranged by chunk! The grouping is something stored internally about the grouped tibble. Now that the tibble is grouped, operations that you do on a grouped tibble will be done independently within each chunk, as if no other chunks exist. You can also create new variables and group by that variable simultaneously. Try splitting life expectancy by “small” and “large” using 60 as a threshold: gapminder |&gt; group_by(smallLifeExp = lifeExp &lt; 60) 22.3 Grouped summarize() Want to compute the mean and standard deviation for each year for every continent? No problem: gapminder |&gt; group_by(continent, year) |&gt; summarize(mu = mean(lifeExp), sigma = sd(lifeExp)) Notice: The grouping variables are kept in the tibble, because their values are unique within each chunk (by definition of the chunk!) With each call to summarize(), the grouping variables are “peeled back” from last grouping variable to first. This means the above tibble is now only grouped by continent. What happens when we reverse the grouping? gapminder |&gt; group_by(year, continent) |&gt; # Different order summarize(mu = mean(lifeExp), sigma = sd(lifeExp)) The grouping columns are switched, and now the tibble is grouped by year instead of continent. dplyr has a bunch of convenience functions that help us write code more eloquently. We could use group_by() and summarize() with length() to find the number of entries each country has: gapminder |&gt; group_by(country) |&gt; transmute(n = length(country)) Or, we can use the more elegant dplyr::n() to count the number of rows in each group: gapminder |&gt; group_by(country) |&gt; summarize(n = n()) Or better yet, if this is all we want, just use dplyr::count(): gapminder |&gt; count(country) 22.4 Grouped mutate() Want to get the increase in GDP per capita for each country? No problem: gap_inc &lt;- gapminder |&gt; arrange(year) |&gt; group_by(country) |&gt; mutate(gdpPercap_inc = gdpPercap - lag(gdpPercap)) print(gap_inc) The tibble is still grouped by country. Drop the NAs with another convenience function, this time supplied by the tidyr package (another tidyverse package that we’ll see soon): gap_inc |&gt; tidyr::drop_na() You can specify specific columns to drop NAs from in the drop_na() function. 22.5 Function types We’ve seen cases of transforming variables using mutate() and summarize(), both with and without group_by(). How can you know what combination to use? Here’s a summary based on one of three types of functions. Function type Explanation Examples In dplyr Vectorized functions These take a vector, and operate on each component independently to return a vector of the same length. In other words, they work element-wise. cos(), sin(), log(), exp(), round() mutate() Aggregate functions These take a vector, and return a vector of length 1 mean(), sd(), length() summarize(), esp with group_by(). Window Functions these take a vector, and return a vector of the same length that depends on the vector as a whole. lag(), rank(), cumsum() mutate(), esp with group_by() "],["advanced-dplyr-functions.html", "Part 23 Advanced dplyr functions 23.1 recode() 23.2 across() 23.3 Complex recoding plus across() 23.4 rowwise()", " Part 23 Advanced dplyr functions 23.1 recode() recode() is useful for recoding categorical variables. Unlike most of the other function in dplyr, recode() is backwards in it’s syntax: recode(.x, old = new) Lets take a look at recoding different variables using the psychTools::bfi dataset: In the dataset, our gender variable has values 1 and 2. This is a little vague since we don’t know what 1 or 2 is in respect to gender. dat_bfi &lt;- psychTools::bfi |&gt; rownames_to_column(var = &quot;.id&quot;) dat_bfi |&gt; mutate( gender = recode(gender, &quot;1&quot; = &quot;man&quot;, &quot;2&quot; = &quot;woman&quot;) ) |&gt; select(.id, gender, education) |&gt; head() Note that for numeric values on the left side of =, you need to wrap them in “quotes” or backticks; however, that’s not necessary for character values We can also specify a .default value within our recode(). For example, say we want to have just “HS or less” versus “more than HS” dat_bfi |&gt; mutate( education = recode(education, &quot;1&quot; = &quot;HS&quot;, &quot;2&quot; = &quot;HS&quot;, .default = &quot;More than HS&quot;) ) |&gt; select(.id, gender, education) |&gt; head() Another neat feature of the recode() function is the .missing value. If we would rather convert NA values to something more explicit, we can specify that in the .missing argument. dat_bfi |&gt; mutate( education = recode( education, &quot;1&quot; = &quot;HS&quot;, &quot;2&quot; = &quot;HS&quot;, .default = &quot;More than HS&quot;, .missing = &quot;(Unknown)&quot; ) ) |&gt; select(.id, gender, education) |&gt; head() Or we can use tidyr::replace_na() dat_bfi |&gt; mutate( education = replace_na(education, replace = &quot;(Unknown)&quot;) ) |&gt; select(.id, gender, education) |&gt; head() 23.2 across() The across function allows us to apply transformations across multiple columns Say we wanted to look at the mean of each agreeable variable between gender groups: dat_bfi |&gt; group_by(gender) |&gt; summarize( across( A1:A5, mean, na.rm = TRUE ) ) If we want to put the function name mean, togther with all of its arguments, we can write it as an anonymous function: dat_bfi |&gt; group_by(gender) |&gt; summarize( across( A1:A5, \\(x) mean(x, na.rm = TRUE) ) ) What if we wanted to include the standard deviation as well? We can pass a list of functions into across() dat_bfi |&gt; group_by(gender) |&gt; summarize( across( A1:A5, list( mean = \\(x) mean(x, na.rm = TRUE), sd = \\(x) sd(x, na.rm = TRUE) ) ) ) 23.3 Complex recoding plus across() Now sometimes with our scales we may encounter variables that are reverse scored. dat_bfi |&gt; mutate( A1r = recode( A1, &quot;6&quot; = 1, &quot;5&quot; = 2, &quot;4&quot; = 3, &quot;3&quot; = 4, &quot;2&quot; = 5, &quot;1&quot; = 6 ) ) |&gt; select(A1, A1r) |&gt; head() # or dat_bfi |&gt; mutate(A1r = max(A1, na.rm = TRUE) - A1 + min(A1, na.rm = TRUE)) |&gt; select(A1, A1r) |&gt; head() However, we can implement some more complex code that will reverse recode() in one fell swoop! We start with either specifying our columns that need reverse coding or get it from a data dictionary: reversed &lt;- c(&quot;A1&quot;, &quot;C4&quot;, &quot;C5&quot;, &quot;E1&quot;, &quot;E2&quot;, &quot;O2&quot;, &quot;O5&quot;) # or dict &lt;- psychTools::bfi.dictionary |&gt; as_tibble(rownames = &quot;item&quot;) reversed &lt;- dict |&gt; filter(Keying == -1) |&gt; pull(item) Putting it all together: dat_bfi |&gt; mutate(across( all_of(reversed), \\(x) recode(x, &quot;6&quot; = 1, &quot;5&quot; = 2, &quot;4&quot; = 3, &quot;3&quot; = 4, &quot;2&quot; = 5, &quot;1&quot; = 6), .names = &quot;{.col}r&quot; )) |&gt; head() The .names argument tells how to name the new columns. If you omit .names, the columns will be modified in place. In .names, the {.col} bit means “the column name”, and any text around that (here the letter r) is added to the name. 23.4 rowwise() rowwise() is a special group_by(). It tells R to treat each row of a data frame as its own group. rowwise() is useful for computing summary scores across items for each person. For example, to compute total scores for each person in the dat_bfi data: dat_bfi |&gt; rowwise() |&gt; mutate( .id = .id, A_total = mean(c_across(A1:A5), na.rm = TRUE), C_total = mean(c_across(C1:C5), na.rm = TRUE), E_total = mean(c_across(E1:E5), na.rm = TRUE), N_total = mean(c_across(N1:N5), na.rm = TRUE), O_total = mean(c_across(O1:O5), na.rm = TRUE), .before = everything() ) |&gt; head() The c_across() function combines c() and across() into one. It is like c() and creates a vector ala c(1, 3, 5, 7), but you can use the same options for selecting column names as select(). The .before argument says where to put the new columns you mutate(). everything() means “all the columns have I haven’t named yet”, so .before = everything() means put the new columns at the beginning of the data frame. "],["lab03.html", "Part 24 Lab 3: Explore gapminder with ggplot2 and dplyr 24.1 Getting started 24.2 Exercise 1: Basic dplyr 24.3 Exercise 2: Explore two variables with dplyr and ggplot2 24.4 Bonus Exercise: Recycling (Optional)", " Part 24 Lab 3: Explore gapminder with ggplot2 and dplyr In this lab, you will explore the gapminder dataset by making summary tables and visualizations. 24.1 Getting started Make a new RMarkdown script for this lab. In the setup chunk at the top of your scripts, load the packages needed for this lab. These include tidyverse and gapminder. 24.2 Exercise 1: Basic dplyr Use dplyr functions to achieve the following. For each exercise, print your result. 24.2.1 1.1 Use filter() to subset the gapminder data to three countries of your choice in the 1970’s. 24.2.2 1.2 Start with the original gapminder data and use the pipe operator |&gt; to first do the above filter and then select the “country” and “gdpPercap” variables. 24.2.3 1.3 Make a new variable in gapminder for the change in life expectancy from the previous measurement for that country. Filter this table to show all of the entries that have experienced a drop in life expectancy. Save this result as a new object. Hint: you might find the lag() or diff() functions useful. 24.2.4 1.4 Filter gapminder so that it shows the max GDP per capita experienced by each country. Hint: you might find the max() function useful here. 24.2.5 1.5 Produce a scatterplot of Canada’s life expectancy vs. GDP per capita using ggplot2. In your plot, put GDP per capita on a log scale. 24.3 Exercise 2: Explore two variables with dplyr and ggplot2 Use gapminder, palmerpenguins::penguins, or another dataset of your choice. (Check out a dataset from the datasets or psych R package if you want! 24.3.1 2.1 Pick two quantitative variables to explore. Make a summary table of descriptive statistics for these variables using summarize(). Include whatever staistics you feel appropriate (mean, median sd, range, etc.). Make a scatterplot of these variables using ggplot(). 24.3.2 2.2 Pick one categorical variable and one quantitative variable to explore. Make a summary table giving the sample size (hint: n()) and descriptive statistics for the quantitative variable by group. Make one or more useful plots to visualize these variables. Try to make a raincloud plot with points, boxplots, and densities for each group. 24.4 Bonus Exercise: Recycling (Optional) Evaluate this code and describe the result. The goal was to get the data for Rwanda and Afghanistan. Does this work? Why or why not? If not, what is the correct way to do this? filter(gapminder, country == c(&quot;Rwanda&quot;, &quot;Afghanistan&quot;)) "],["lab04.html", "Part 25 Lab 4: Personality and green reputation 25.1 Getting started", " Part 25 Lab 4: Personality and green reputation In this lab, You will analyze data looking at the relationship between green reputation and three personality traits– compassion, intellectual curiosity, and openness to experiences. The dataset includes data from students and non-students. 25.1 Getting started Make a new RMarkdown script for this lab. In the setup chunk at the top of your scripts, load the packages needed for this lab. Download the 2 data files for this lab and save them in the data folder of your GitHub folder. green_dictionary green_data Add a chunk with these lines chunk to your RMarkdown script to import the data. dictionary &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;green_dictionary.csv&quot;)) green_data &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;green_data.csv&quot;)) For your assignment, do the following. Inspect the item responses (e.g., with graphs or by summarizing distinct values). Is anything unusual? Compute total scores for the four scales. Recode variables as needed. Rescale the variables so that they go from 0-100 instead of the original range. Name the recaled variables *_pomp. Make plots that illustrate the distributions of the 4 POMP-scored variables. Make scatterplots showing the relationships between green reputation and each personality trait. Include trend lines for students and non-students. What do these plots show? Compare green reputation for students and non-students using a rainfall plot (bar + density + data points). Compute a summary table of means, SDs, medians, minima, and maxima for the four total scores for students and non-students. "],["tidy-data-and-pivoting.html", "Part 26 Tidy Data and Pivoting 26.1 Orientation", " Part 26 Tidy Data and Pivoting library(tidyverse) 26.1 Orientation 26.1.1 Today Today’s concept is tidy data and the tidyr package. Reshaping data by pivoting with tidyr::pivot_longer() and tidyr::pivot_wider(). 26.1.2 Resources For concepts of tidy data: Jenny Bryan’s intro to tidy data is short and sweet. the repo this links to has some useful exercises too, but uses the older spread() and gather() functions. tidyr vignette on tidy data. Hadley’s paper on tidy data provides a thorough investigation. For pivoting with tidyr, check out the pivot vignette. "],["tidy-data.html", "Part 27 Tidy data 27.1 Untidy Examples", " Part 27 Tidy data A data set is tidy if: Each row is an observation appropriate for the analysis; Each column is a variable; Each cell is a value. This means that each value belongs to exactly one variable and one observation. Why bother? Because doing computations with untidy data can be a nightmare. Computations become simple with tidy data. Whether or not a data set is “tidy” depends on the type of analysis you are doing or plot you are making. It depends on how you define your “observation” and “variables” for the current analysis. haireye &lt;- as_tibble(HairEyeColor) |&gt; count(Hair, Eye, wt = n) |&gt; rename(hair = Hair, eye = Eye) As an example, consider this example derived from the datasets::HairEyeColor dataset, containing the number of people having a certain hair and eye color. If one observation is identified by a hair-eye color combination, then the tidy dataset is: haireye |&gt; print() If one observation is identified by a single person, then the tidy dataset has one pair of values per person, and one row for each person. We can use the handy tidyr::uncount() function, the opposite of dplyr::count(): haireye |&gt; tidyr::uncount(n) |&gt; print() 27.1 Untidy Examples The following are examples of untidy data. They’re untidy for either of the cases considered above, but for discussion, let’s take a hair-eye color combination to be one observational unit. Note that untidy does not always mean “bad”, just inconvenient for the analysis you want to do. Untidy Example 1: The following table is untidy because there are multiple observations per row. It’s too wide. Imagine calculating the total number of people with each hair color. You can’t just group_by() and summarize(), here! This sort of table is common when presenting results. It’s easy for humans to read, but hard for computers to work with. Untidy data is usually that way because it was structured for human, not machine, reading. Untidy Example 2: The following table is untidy for the same reason as Example 1—multiple observations are contained per row. It’s too wide. Untidy Example 3: This is untidy because each observational unit is spread across multiple columns. It’s too long. In fact, we needed to add an identifier for each observation, otherwise we would have lost which row belongs to which observation! Does red hair ever occur with blue eyes? Can’t just filter(hair == \"red\", eye == \"blue\")! Untidy Example 4: Just when you thought a data set couldn’t get any longer! Now, each variable has its own row: hair color, eye color, and n. This is the sort of format that is common pulling data from the web or other “Big Data” sources. "],["pivoting-data.html", "Part 28 Pivoting data 28.1 Univariate pivoting", " Part 28 Pivoting data The task of making tidy data is about making data either longer, by stacking two or more rows, or wider, by putting one or more columns alongside each other based on groups. This is called pivoting. Usually the task of tidying data involves lengthening, and usually the task of widening is useful for turning data into something more friendly for human eyes. Sometimes, you will see data described as being in “long” or “wide” formats. Those terms aren’t that useful—“long” and “wide” are relative terms. The easiest and most powerful way to widen or lengthen data are with the functions tidyr::pivot_wider() and tidyr::pivot_longer(). History: R has seen many attempts at reshaping that have progressively gotten better. First came the reshape and reshape2 packages. Both were finicky. Used function names that I could never remember: melt() and cast(). Then, tidyr package replaced these. The tidyr::spread() and tidyr::gather() functions provided a simple interface. I still couldn’t remember these names. We will use tidyr::pivot_longer() and tidyr::pivot_wider(). The “pivot” functions also have similar names to the SQL “PIVOT” and “UNPIVOT” functions. 28.1 Univariate pivoting Let’s start with pivoting in the simplest case where only one variable is “out of place”. We’ll use the hair and eye color example from before, using the untidy data version from Example 1: haireye_untidy &lt;- haireye |&gt; mutate(eye = str_c(eye, &quot;_eyed&quot;)) |&gt; pivot_wider(id_cols = hair, names_from = eye, values_from = n) haireye_untidy The eye color variable is spread out across columns. To fix this, we need to convert the eye color columns to two columns: one column to hold the eye color (column names), one column to hold the values. Doing this, we obtain: Let’s dig into that! 28.1.1 pivot_longer() pivot_longer() takes a data frame, and returns a data frame. The main arguments after the data argument that we’ll need are: cols: the columns we want to pivot into a single column. Give the column names names_to: the old column names are going to be stored in a new column. What should this new column be named? values_to: the values in the old columns are going to stored in a new column. What should this new column be named? Possibly the trickiest bit is in identifying the column names. We could list all of them: haireye_untidy |&gt; pivot_longer(cols = c(Blue_eyed, Brown_eyed, Green_eyed, Hazel_eyed), names_to = &quot;eye&quot;, values_to = &quot;n&quot;) That can be a little tedious. We could identify a range. This is efficient, but not so robust if the data changes. haireye_untidy |&gt; pivot_longer(cols = Blue_eyed:Hazel_eyed, names_to = &quot;eye&quot;, values_to = &quot;n&quot;) Better is to use helper functions from the tidyselect package. In this case, we know the columns contain the text “eyed”, so let’s use tidyselect::contains(): haireye_untidy |&gt; pivot_longer(cols = contains(&quot;eyed&quot;), names_to = &quot;eye&quot;, values_to = &quot;n&quot;) Yet another way is to indicate everything except the hair column: haireye_untidy |&gt; pivot_longer(cols = -hair, names_to = &quot;eye&quot;, values_to = &quot;n&quot;) 28.1.2 pivot_wider() Let’s say we want go from a longer data frame to a shorter data frame. This is the opposite of what we did above. We might want to do this: To make a table for presentation With longitudinal or family data, to to go from multilevel models (which need longer data; each row is an observation) to SEM analyses (which need wider data; each row is an individual or family) For example, if we want to go from: To: We need to: Take the column eye and make each unique entry a new column Take the column n and make these values in the new eye columns. pivot_wider() is the reverse of pivot_longer(). Like pivot_longer(), pivot_wider() takes a data frame and returns a data frame. The main arguments after the data argument that we’ll need are: id_cols: The columns you would like to keep in place. By default, everything except the ones in names_from and values_from. For example, the identifier number for the observation. names_from: The new column names are coming from an old column. Which column is this? values_from: The column values are coming from an old column. Which column is this? haireye |&gt; pivot_wider(id_cols = hair, names_from = eye, values_from = n) "],["lab-5a-univariate-pivoting.html", "Part 29 Lab 5A: Univariate pivoting 29.1 Exercise 1: Univariate Pivoting", " Part 29 Lab 5A: Univariate pivoting Copy this code into your script to import the data for this lab. library(tidyverse) lotr &lt;- read_csv(&quot;https://raw.githubusercontent.com/jennybc/lotr-tidy/master/data/lotr_tidy.csv&quot;) |&gt; rename(Species = Race) 29.1 Exercise 1: Univariate Pivoting Consider the Lord of the Rings data: lotr Would you say this data is in tidy format? Widen the data so that we see the words spoken by each species, by putting species as its own column. (lotr_wide &lt;- lotr |&gt; pivot_wider(FILL_THIS_IN = c(Film, Gender), FILL_THIS_IN = Species, FILL_THIS_IN = Words)) Re-lengthen the wide LOTR data from Question 2 above. lotr_wide |&gt; pivot_longer(FILL_THIS_IN = FILL_THIS_IN, names_to = FILL_THIS_IN, values_to = FILL_THIS_IN) "],["multivariate-pivoting.html", "Part 30 Multivariate pivoting 30.1 Multiple variables in column names 30.2 Multiple variables in column names 30.3 pivot_wider()", " Part 30 Multivariate pivoting Now let’s consider the case when more than one variable are “out of place”. Perhaps there are multiple variables per row and/or multiple observations per row. 30.1 Multiple variables in column names Consider this subset of the who data: WHO &lt;- who |&gt; select(country:year, starts_with(&quot;new_&quot;)) |&gt; rename_with(~ stringr::str_replace(.x, &quot;f&quot;, &quot;f_&quot;), starts_with(&quot;new_&quot;)) |&gt; rename_with(~ stringr::str_replace(.x, &quot;m&quot;, &quot;m_&quot;), starts_with(&quot;new_&quot;)) knitr::kable(WHO, rownames = FALSE) country, iso2, iso3, and year are already variables, so they can be left as is. But the columns from new_sp_m_014 to new_ep_f_65 encode four variables in their names: The new prefix indicates these are counts of new cases (versus total cases). This dataset only contains new cases, so we’ll ignore it here because it’s constant. sp/rel/ep describe how the case was diagnosed. m/f gives the gender. 014/1524/2535/3544/4554/65 supplies the age range. We can break these variables up by specifying multiple column names in names_to, and then providing names_sep. WHO |&gt; pivot_longer( cols = new_sp_m_014:new_ep_f_65, names_to = c(&quot;diagnosis&quot;, &quot;gender&quot;, &quot;age&quot;), names_prefix = &quot;new_&quot;, names_sep = &quot;_&quot;, values_to = &quot;count&quot; ) In the names_to argument, we now tell it the names of the new columns that will store each part of the existing column names. We give the column names in order corresponding to how they appear in the existing column names. As with univariate pivoting, values_to gives the name of the new column that will store the cell values. 30.2 Multiple variables in column names Consider these family data. fam_dat &lt;- tribble( ~family, ~dob_child1, ~dob_child2, ~gender_child1, ~gender_child2, 1L, &quot;1998-11-26&quot;, &quot;2000-01-29&quot;, 1L, 2L, 2L, &quot;1996-06-22&quot;, NA, 2L, NA, 3L, &quot;2002-07-11&quot;, &quot;2004-04-05&quot;, 2L, 2L, 4L, &quot;2004-10-10&quot;, &quot;2009-08-27&quot;, 1L, 1L, 5L, &quot;2000-12-05&quot;, &quot;2005-02-28&quot;, 2L, 1L, ) fam_dat &lt;- fam_dat |&gt; mutate_at(vars(starts_with(&quot;dob&quot;)), parse_date) fam_dat In these data, we have two pieces of information (or values) for each child: their gender and their dob (date of birth). These need to go into separate columns in the result. Again we supply multiple variables to names_to, using names_sep to split up each variable name. The names_to vector gives the names of the new columns that will store each part of the existing column names. We give the column names in order corresponding to how they appear in the existing column names. Note the special name .value: .value takes the place of the values_to argument. It tells pivot_longer() to get the name of the column that will hold the cell values from that part of the existing column name. fam_dat |&gt; pivot_longer( cols = -family, names_to = c(&quot;.value&quot;, &quot;child&quot;), names_sep = &quot;_&quot;, values_drop_na = TRUE ) Let’s also clean up the child column: fam_dat_long &lt;- fam_dat |&gt; pivot_longer( cols = -family, names_to = c(&quot;.value&quot;, &quot;child&quot;), names_sep = &quot;_&quot;, values_drop_na = TRUE ) |&gt; mutate(child = stringr::str_replace(child, &quot;child&quot;, &quot;&quot;)) |&gt; mutate(child = as.integer(child)) 30.3 pivot_wider() You can also pivot_wider() while using multiple columns to supply variable names: id_cols: as usual. names_from: the new variable names are coming from old columns. Which old columns? names_sep: What character should you separate the entries of the old columns by? values_from: as usual. fam_dat_long |&gt; pivot_wider(id_cols = family, names_from = c(child, gender), names_prefix = &quot;child&quot;, names_sep = &quot;_gender&quot;, values_from = dob) Or using multiple columns to supply new values: If variables are spread out amongst rows and columns (for example, “sepal width” has “sepal” in a column, and “width” as a column name), here’s how we can use pivot_wider(): id_cols: as usual names_from: Which column contains the part of the variable? names_sep: As before, what character should you separate the entries of the old columns by? values_from: Which column names contain the other part of the variable? fam_dat_long |&gt; pivot_wider(id_cols = family, names_from = child, names_prefix = &quot;child&quot;, names_sep = &quot;_&quot;, values_from = c(dob, gender)) "],["lab-5b-multivariate-pivoting.html", "Part 31 Lab 5B: Multivariate pivoting", " Part 31 Lab 5B: Multivariate pivoting Copy this code into your script to import the data for this lab. library(tidyverse) set.seed(123) missing_w2_parent &lt;- sample(1:500, 30) missing_w2_child &lt;- c(missing_w2_parent[1:5], sample(1:500, 25)) family &lt;- read_csv( &quot;https://raw.githubusercontent.com/bwiernik/progdata/main/inst/tutorials/data/family_data.csv&quot; ) |&gt; mutate( across( starts_with(&quot;w2&quot;) &amp; contains(&quot;parent&quot;), ~ ifelse(family_id %in% missing_w2_parent, NA_real_, .x) ), across( starts_with(&quot;w2&quot;) &amp; contains(&quot;child&quot;), ~ ifelse(family_id %in% missing_w2_child, NA_real_, .x) ) ) You’re working on a longitudinal study of parent-child relationships. You have collected data from 500 families over 2 waves. In each wave, both the child and parent completed measures of communication behavior and relationship satisfaction. family |&gt; DT::datatable(rownames = FALSE) Reshape the dataset to a “longer” format. Make each row 1 score Have columns for family_id, family_member, wave, scale, and score. family_longest &lt;- family |&gt; pivot_longer() print(family_longest) Reshape the dataset to a “longer” format. Make each row 1 person Have columns for family_id, family_member, wave, comm, and satis. family_long &lt;- family |&gt; pivot_longer() print(family_long) Some families are missing wave 2 data for parent, child, or both. Which families are missing wave 2 data for at least one person? Question: Is is easier to easier to find the missing data in the wide or long format? "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
