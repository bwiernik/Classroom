[["index.html", "Programming with Data Spring 2022 About This Guide Brenton’s Notes", " Programming with Data Spring 2022 Brenton M. Wiernik 2022-04-08 About This Guide Welcome to the class guide for Programming with Data for Spring 2022! This guide organizes what we will be doing in each class meeting. So you can expect it to be updated regularly – in fact, the date listed above is the last time this guide was updated. This course was developed in part based on the resources provided by Jenny Bryan found at https://stat545.com and by Mason Garrison found at https://datascience4psych.github.io/DataScience4Psych/. A playlist of videos from Mason Garrison covering many of the topics we will explore in the course is available here. Brenton’s Notes This website is constantly changing. The source code for this website in the class [repo][course_repo]. I encourage you to contribute to the course code. If you catch typos, errors, please issue a pull request with the fixes. If you find cool/useful resources, please add them. How to use these notes This document is broken down into multiple chapters. Use the table of contents on the left side of the screen to navigate, and use the hamburger icon (horizontal bars) at the top of the document to open or close the table of contents. At the top of the document, you’ll see additional icons which you can click to search the document, change the size, font or color scheme of the page. The document will be updated (unpredictably) throughout the semester. Every module corresponds to a week-ish’s worth of material. Most modules are dedicated to improving a specific skill or at the very least dedicated to a specific theme. Within each module, there are embedded videos, slides, activities, labs, and tutorials. The skills developed in each module build upon skills you’ve developed in previous modules. Although these notes share some of the features of a textbook, they are neither comprehensive nor completely original. The main purpose is to give you all a set of common materials on which to draw during the course. In class, we will sometimes do things outside the notes. The idea here is that you will read the materials and try to learn from them, just as you will attend classes and try to learn from them. "],["attribution.html", "Attribution Major Attributions Additional Attributions", " Attribution This class leans heavily on other peoples’ materials and ideas. Major Attributions Mason Garrison’s Data Scientists for Psychologists course; Jenny Bryan’s (jennybryan.org) STAT 545 and Happy Git with R; Joe Rodgers’s PSY 8751 Exploratory and Graphical Data Analysis Course Mine Çetinkaya-Rundel’s Data Science in a Box. Additional Attributions Academic.io’s AWESOME DATA SCIENCE Julia Fukuyama’s EXPLORATORY DATA ANALYSIS Benjamin Soltoff’s Computing for the Social Sciences Grant McDermott’s course materials on environmental economics and data science Thomas E. Love Karl Broman EMILY SUZANNE CLARK’s Rubric for Unessays Ariel Muldoon’s tutorial on simulations "],["license.html", "License", " License This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. This information is a human-readable summary of (and not a substitute for) the license. Please see https://creativecommons.org/licenses/by-sa/4.0/legalcode for the full legal text. You are free to: Share—copy and redistribute the material in any medium or format Remix—remix, transform, and build upon the material for any purpose, even commercially. The licensor cannot revoke these freedoms as long as you follow the license terms. Under the following terms: Attribution—You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. ShareAlike—If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original. No additional restrictions—You may not apply legal terms or technological measures that legally restrict others from doing anything the license permits. Notices: You do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation. No warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material. "],["colophon.html", "Colophon", " Colophon These notes was written in bookdown inside RStudio. The website is hosted with github, The complete source is available from github. The book style was designed by Desirée De Leon. This version of the notes was built with: #&gt; Finding R package dependencies ... Done! #&gt; setting value #&gt; version R version 4.1.3 (2022-03-10) #&gt; os macOS Big Sur/Monterey 10.16 #&gt; system x86_64, darwin17.0 #&gt; ui X11 #&gt; language (EN) #&gt; collate en_US.UTF-8 #&gt; ctype en_US.UTF-8 #&gt; tz UTC #&gt; date 2022-04-08 #&gt; pandoc 2.17.1.1 @ /usr/local/bin/ (via rmarkdown) Along with these packages: package loadedversion date source anytime anytime NA 2020-08-27 CRAN (R 4.1.0) askpass askpass NA 2019-01-13 CRAN (R 4.1.0) assertthat assertthat 0.2.1 2019-03-21 CRAN (R 4.1.0) backports backports 1.4.1 2021-12-13 CRAN (R 4.1.0) base64enc base64enc NA 2015-07-28 CRAN (R 4.1.0) bayestestR bayestestR NA 2022-04-08 repository (https://github.com/easystats/bayestestR@088bdc8184ec2e53a37d04f45d0677c646a2429b) BH BH NA 2021-01-11 CRAN (R 4.1.0) bit bit NA 2020-08-04 CRAN (R 4.1.0) bit64 bit64 NA 2020-08-30 CRAN (R 4.1.0) blob blob NA 2021-07-23 CRAN (R 4.1.0) bookdown bookdown 0.24 2021-09-02 CRAN (R 4.1.0) broom broom 0.7.10 2021-10-31 CRAN (R 4.1.0) callr callr 3.7.0 2021-04-20 CRAN (R 4.1.0) cellranger cellranger 1.1.0 2016-07-27 CRAN (R 4.1.0) cli cli 3.1.1 2022-01-20 CRAN (R 4.1.3) clipr clipr NA 2020-10-08 CRAN (R 4.1.0) colorspace colorspace 2.0-2 2021-06-24 CRAN (R 4.1.0) correlation correlation NA 2022-04-08 repository (https://github.com/easystats/correlation@73471db03fe91accf9f32c102dfa5041c26564dc) cpp11 cpp11 NA 2021-11-30 CRAN (R 4.1.0) crayon crayon 1.4.2 2021-10-29 CRAN (R 4.1.0) crosstalk crosstalk NA 2021-11-04 CRAN (R 4.1.0) curl curl NA 2021-06-23 CRAN (R 4.1.0) data.table data.table NA 2021-09-27 CRAN (R 4.1.0) datawizard datawizard NA 2022-04-08 repository (https://github.com/easystats/datawizard@a00e797d48794c76618ba4386423cab64eff7858) DBI DBI 1.1.1 2021-01-15 CRAN (R 4.1.0) dbplyr dbplyr 2.1.1 2021-04-06 CRAN (R 4.1.0) digest digest 0.6.29 2021-12-01 CRAN (R 4.1.0) distributional distributional NA 2021-02-02 CRAN (R 4.1.0) dplyr dplyr 1.0.7 2021-06-18 CRAN (R 4.1.0) DT DT 0.20 2021-11-15 CRAN (R 4.1.0) dtplyr dtplyr NA 2021-12-05 CRAN (R 4.1.0) easystats easystats NA 2022-04-08 repository (https://github.com/easystats/easystats@9072d79d436240b20a820c07e8711cdd8c7f9b4a) effectsize effectsize NA 2022-04-08 repository (https://github.com/easystats/effectsize@0aeafb70f47c1b8fea78b54112d471287fd9ef31) ellipsis ellipsis 0.3.2 2021-04-29 CRAN (R 4.1.0) evaluate evaluate 0.14 2019-05-28 CRAN (R 4.1.0) fansi fansi 1.0.2 2022-01-14 CRAN (R 4.1.3) farver farver NA 2021-02-28 CRAN (R 4.1.0) fastmap fastmap 1.1.0 2021-01-25 CRAN (R 4.1.0) forcats forcats 0.5.1 2021-01-27 CRAN (R 4.1.0) fs fs 1.5.2 2021-12-08 CRAN (R 4.1.0) gapminder gapminder NA 2017-10-31 CRAN (R 4.1.0) gargle gargle NA 2021-07-02 CRAN (R 4.1.0) generics generics 0.1.1 2021-10-25 CRAN (R 4.1.0) ggdist ggdist NA 2021-11-30 CRAN (R 4.1.0) ggplot2 ggplot2 3.3.5 2021-06-25 CRAN (R 4.1.0) glue glue 1.6.1 2022-01-22 CRAN (R 4.1.3) googledrive googledrive NA 2021-07-08 CRAN (R 4.1.0) googlesheets4 googlesheets4 NA 2021-07-21 CRAN (R 4.1.0) gtable gtable 0.3.0 2019-03-25 CRAN (R 4.1.0) haven haven 2.4.3 2021-08-04 CRAN (R 4.1.0) HDInterval HDInterval NA 2020-05-23 CRAN (R 4.1.0) here here NA 2020-12-13 CRAN (R 4.1.0) highr highr NA 2021-04-16 CRAN (R 4.1.0) hms hms 1.1.1 2021-09-26 CRAN (R 4.1.0) htmltools htmltools 0.5.2 2021-08-25 CRAN (R 4.1.0) htmlwidgets htmlwidgets 1.5.4 2021-09-08 CRAN (R 4.1.0) httr httr 1.4.2 2020-07-20 CRAN (R 4.1.0) ids ids NA 2017-05-31 CRAN (R 4.1.0) insight insight NA 2022-04-08 repository (https://github.com/easystats/insight@20588c646676baa94f14483e1a5c354ba25f78f7) isoband isoband NA 2021-07-13 CRAN (R 4.1.0) jquerylib jquerylib 0.1.4 2021-04-26 CRAN (R 4.1.0) jsonlite jsonlite 1.7.3 2022-01-17 CRAN (R 4.1.3) knitr knitr 1.37 2021-12-16 CRAN (R 4.1.3) labeling labeling NA 2020-10-20 CRAN (R 4.1.0) later later NA 2021-08-18 CRAN (R 4.1.0) lattice lattice NA 2021-09-22 CRAN (R 4.1.3) lazyeval lazyeval NA 2019-03-15 CRAN (R 4.1.0) lifecycle lifecycle 1.0.1 2021-09-24 CRAN (R 4.1.0) lubridate lubridate 1.8.0 2021-10-07 CRAN (R 4.1.0) magrittr magrittr 2.0.1 2020-11-17 CRAN (R 4.1.0) MASS MASS NA 2021-05-03 CRAN (R 4.1.0) Matrix Matrix NA 2021-06-01 CRAN (R 4.1.0) mgcv mgcv NA 2021-10-06 CRAN (R 4.1.0) mime mime NA 2021-09-28 CRAN (R 4.1.0) modelbased modelbased NA 2022-04-08 repository (https://github.com/easystats/modelbased@0d5600abdda3c789dc0a95f1ab32823536f522a6) modelr modelr 0.1.8 2020-05-19 CRAN (R 4.1.0) munsell munsell 0.5.0 2018-06-12 CRAN (R 4.1.0) nlme nlme NA 2021-09-07 CRAN (R 4.1.0) numDeriv numDeriv NA 2019-06-06 CRAN (R 4.1.0) openssl openssl NA 2021-12-19 CRAN (R 4.1.3) palmerpenguins palmerpenguins NA 2020-07-23 CRAN (R 4.1.0) parameters parameters NA 2022-04-08 repository (https://github.com/easystats/parameters@a33b414e5ffb358e069946e82fd88fac55b51e50) patchwork patchwork NA 2020-12-17 CRAN (R 4.1.0) performance performance NA 2022-04-08 repository (https://github.com/easystats/performance@95e916dd28dfc7dc7da772aeed6ff2d81eb47dc9) pillar pillar 1.6.4 2021-10-18 CRAN (R 4.1.0) pkgconfig pkgconfig 2.0.3 2019-09-22 CRAN (R 4.1.0) prettyunits prettyunits 1.1.1 2020-01-24 CRAN (R 4.1.0) processx processx 3.5.2 2021-04-30 CRAN (R 4.1.0) progress progress NA 2019-05-16 CRAN (R 4.1.0) promises promises NA 2021-02-11 CRAN (R 4.1.0) ps ps 1.6.0 2021-02-28 CRAN (R 4.1.0) purrr purrr 0.3.4 2020-04-17 CRAN (R 4.1.0) R6 R6 2.5.1 2021-08-19 CRAN (R 4.1.0) rappdirs rappdirs NA 2021-01-31 CRAN (R 4.1.0) RColorBrewer RColorBrewer NA 2014-12-07 CRAN (R 4.1.0) Rcpp Rcpp 1.0.7 2021-07-07 CRAN (R 4.1.0) readr readr 2.1.1 2021-11-30 CRAN (R 4.1.0) readxl readxl 1.3.1 2019-03-13 CRAN (R 4.1.0) rematch rematch NA 2016-04-21 CRAN (R 4.1.0) rematch2 rematch2 NA 2020-05-01 CRAN (R 4.1.0) report report NA 2022-04-08 repository (https://github.com/easystats/report@6b18deb2ec70fb370211d24156590453f3a8caec) reprex reprex 2.0.1 2021-08-05 CRAN (R 4.1.0) rlang rlang 0.4.12 2021-10-18 CRAN (R 4.1.0) rmarkdown rmarkdown 2.11 2021-09-14 CRAN (R 4.1.0) rprojroot rprojroot 2.0.2 2020-11-15 CRAN (R 4.1.0) rstudioapi rstudioapi 0.13.0-9000 2022-04-08 Github (rstudio/rstudioapi@96fad1d75b5cacd9a0fe9ff7f58d140f5cf4300b) rvest rvest 1.0.2 2021-10-16 CRAN (R 4.1.0) scales scales 1.1.1 2020-05-11 CRAN (R 4.1.0) see see NA 2022-04-08 repository (https://github.com/easystats/see@b173eb5d888247931f6e454f5107b8e2e42498de) selectr selectr NA 2019-11-20 CRAN (R 4.1.0) stringi stringi 1.7.6 2021-11-29 CRAN (R 4.1.0) stringr stringr 1.4.0 2019-02-10 CRAN (R 4.1.0) sys sys NA 2020-07-23 CRAN (R 4.1.0) tibble tibble 3.1.6 2021-11-07 CRAN (R 4.1.0) tidyr tidyr 1.1.4 2021-09-27 CRAN (R 4.1.0) tidyselect tidyselect 1.1.1 2021-04-30 CRAN (R 4.1.0) tidyverse tidyverse 1.3.1 2021-04-15 CRAN (R 4.1.0) tinytex tinytex NA 2021-12-19 CRAN (R 4.1.3) tsibble tsibble NA 2021-12-03 CRAN (R 4.1.0) tweetrmd tweetrmd 0.0.9 2022-04-08 Github (gadenbuie/tweetrmd@075102b6a02fa4ea26741a9db7ca8afd728350a1) tzdb tzdb 0.2.0 2021-10-27 CRAN (R 4.1.0) utf8 utf8 1.2.2 2021-07-24 CRAN (R 4.1.0) uuid uuid NA 2021-11-01 CRAN (R 4.1.0) vctrs vctrs 0.3.8 2021-04-29 CRAN (R 4.1.0) viridisLite viridisLite NA 2021-04-13 CRAN (R 4.1.0) vroom vroom NA 2021-11-30 CRAN (R 4.1.0) withr withr 2.4.3 2021-11-30 CRAN (R 4.1.0) xfun xfun 0.29 2021-12-14 CRAN (R 4.1.3) xml2 xml2 1.3.3 2021-11-30 CRAN (R 4.1.0) yaml yaml 2.2.1 2020-02-01 CRAN (R 4.1.0) "],["getting-started.html", "Getting Started 0.1 Big Ideas 0.2 Course Modality", " Getting Started This overview is designed to orient you to the class. Programming with Data (Progdata) introduces on the principles of data science, including: data wrangling, modeling, visualization, and communication. In this class, we link those principles to psychological methods and open science practices by emphasizing exploratory analyses and description, rather than confirmatory analyses and hypotheses. Through the semester, we will work our way through many topics covered in Wickham and Grolemund’s R for Data Science text and develop proficiency with tidyverse. This class emphasizes replication and reproducibility. Progdata is a practical skilled-based class and should be useful to students aiming for academia as well as those interested in industry. Applications of these methods can be applied to a full range of psychological areas, including perception (e.g, eye-tracking data), neuroscience (e.g., visualizing neural networks), and individual differences (e.g., personality assessment). 0.1 Big Ideas This class covers the following broad five areas: Reproducibility; Replication; Robust Methods; Resplendent Visualizations; and R Programming. 0.2 Course Modality The course is designed to be flexible to fit each of your unique schedules and situations. The course is scheduled to meet in person 2 days a week. As Covid-related precautions evolve, this schedule is subject to change. The class can function fully asynchronously and remotely if needed. If you are sick, do not come to in person class meetings! All assignments and projects for the class can be completed at your own pace and are due as part of your portfolio at the end of the semester. 0.2.1 Successful Asynchronous Learning This video can help you to be a successful asynchronous learner. Much of this information comes from Northeastern University’s Tips for Taking Online Classes. 0.2.1.1 Productivity During Lockdown "],["syllabus.html", "Syllabus 0.3 Materials 0.4 Assignment Instructions 0.5 Due dates", " Syllabus You can find the class syllabus here. 0.3 Materials 0.3.1 Hardware If you have a laptop that can run R, please bring it to class. This class requires that you have access to a computer outside of class. This could be a laptop or a computer available in a campus computer lab. 0.3.2 Books and Videos In addition to class materials and lab activities, there will also be videos and sections of the book R for Data Science that will be shared to support your learning. R for Data Science text 0.3.3 Software 0.3.3.1 R and RStudio R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of UNIX platforms, Windows, and MacOS. RStudio is a free integrated development environment (IDE), a powerful user interface or dashboard for R. 0.3.3.2 Git and Github Git is a version control system. Its original purpose was to help groups of developers work collaboratively on big software projects. Git manages the evolution of a set of files – called a repository – in a structured way. Think of it like the “Track Changes” features from Microsoft Word. Github is a free IDE and hosting service for Git. As a USF student, you should be able to access the GitHub Student Developer Pack for free. It includes a free PRO upgrade for your github account. 0.4 Assignment Instructions 0.4.1 Lab Activities Each week, there will be opportunities to practice your programming skills in guided activities and problems. You should complete these activities each week and submit them via your GitHub repository for the class. Of the 15 lab activities, you must complete 12 to earn an A grade. You may choose which activities (if any) to skip. For each lab activity, your code should (1) be able to run on any computer without user input, (2) produce the desired outcome, and (3) use clear coding style and logic so that readers can understand it 0.4.2 Peer Code Review Each week, you will be assigned to review 2 classmates’ code and provide constructive feedback. You should both (1) test the code to ensure that it produces correct output and (2) complete a code review report to provide feedback on program design, coding style, and documentation. Code reviews should be completed in a timely manner so that your classmates’ can benefit from the feedback. You may review either a lab activity or portfolio piece. You must complete 10 code reviews to earn an A grade. You may choose which assigned reviews (if any) to skip. As a class, please ensure that each person gets at least one review for each of their activities. 0.4.3 Portfolio Pieces By the end of the semester, you should also have completed 3 larger “portfolio pieces”. These pieces can be one of the following: Data analysis report: Explore and analyze a dataset of interest to you to derive useful or interesting insights. You must demonstrate the skills you are learning in the course (data wrangling, data visualization, modeling, web scraping, text mining, etc.) and present one or more key insights that can be learned from the data. Your project should demonstrate good habits with respect to reproducibility, clear coding style and logic, and effective visualization and communication. You may choose (or construct) any dataset of interest to you. If you are involved in research, please feel free to use data from these projects. Otherwise, there are many datasets available online you can work with, or you can build a dataset from the web yourself. We will explore portfolio piece ideas and potential data sources throughout the semester. Professional website: Create and deploy a website highlighting your skills, experience, and professional portfolio, as appropriate to your professional goals. Cheat sheet: Create a cheat sheet in the style of https://www.rstudio.com/resources/cheatsheets/ for a package, procedure, or workflow. For example, you could make a cheat sheet for a package we discuss in class or another package, or a cheat sheet for a common workflow or procedure for your lab group. Tutorial or other resource: Write a blog post, tutorial, or other resource teaching readers an analytic, programming, visualization, experimental, or workflow method. Shiny app: Build a Shiny app implementing a method of interest (e.g., power analysis, visualizing a certain type of data, fitting a certain type of model). At least 1 of your portfolio pieces must be a Data Analysis Report. Graduate students must complete an additional portfolio piece; this one must be a website, tutorial, or shiny app. 0.5 Due dates All course products are due by the end of the semester, but you should not wait until the last week to start on them! Spaced practice is much more effective for learning than cramming, and I can’t give you useful feedback if I only see things at the end of the semester. Aim to complete lab activities as you go and to complete a portfolio piece every few weeks. "],["welcome-to-data-science.html", "Part 1 Welcome to Data Science 1.1 Module Materials", " Part 1 Welcome to Data Science This module is designed to introduce you to some of the tools and workflow we will use in the course (and in data science and programming more broadly). Each section of this model has links to some videos related to the topic. You can find the module playlist here. Most of the slides used to make the videos in this module can be found here. 1.1 Module Materials Videos Located in the subchapters of this module Slidedecks Welcome Slides Meet the toolkit Suggested Readings All subchapters of this module, including R basics and workflow R4DS Book Introduction Data exploration Introduction Optional: Short Happy Git If Short Happy Git is too much, start with Oh My Git For more depth, Happy Git with R Activities Bechdal Test Oh My Git Lab Hello R "],["what-is-data-science.html", "Part 2 What is Data Science? 2.1 See for yourselves!", " Part 2 What is Data Science? You can follow along with the slides here if they do not appear below. 2.1 See for yourselves! I’ve embedded a few examples below. 2.1.1 Shiny App 2.1.2 Hans Rosling The video below is the shorter version. Hans Rosling’s 200 Countries, 200 Years, 4 Minutes - The Joy of Stats You can find a longer talk-length version below. 2.1.3 Social Media Social media contains a ton of great (and terrible examples of data science in action. These examples range from entire subreddits, such as /r/DataisBeautiful (be sure to check out the highest voted posts) to celebrity tweets about data scientists. YASSSSSSSSSS MY LOVE STEVE IS BACK!!! #KornackiThirstcontinues pic.twitter.com/ynK4D87Bhr&mdash; Leslie Jones 🦋 (@Lesdoggg) January 5, 2021 Good reasons to not be a Data Scientist:- It is a lot of work- Literally nobody will know what you&#39;re talking about- In the end, your computer will be your only real friend&mdash; 🔥 Kareem Carr 🔥 (@kareem_carr) January 22, 2021 2.1.4 Read for yourselves! Link Preview What is Data Science @ O’reilly Data scientists combine entrepreneurship with patience, the willingness to build data products incrementally, the ability to explore, and the ability to iterate over a solution. They are inherently interdiscplinary. They can tackle all aspects of a problem, from initial data collection and data conditioning to drawing conclusions. They can think outside the box to come up with new ways to view the problem, or to work with very broadly defined problems: “here’s a lot of data, what can you make from it?” What is Data Science @ Quora Data Science is a combination of a number of aspects of Data such as Technology, Algorithm development, and data interference to study the data, analyze it, and find innovative solutions to difficult problems. Basically Data Science is all about Analyzing data and driving for business growth by finding creative ways. The sexiest job of 21st century Data scientists today are akin to Wall Street “quants” of the 1980s and 1990s. In those days people with backgrounds in physics and math streamed to investment banks and hedge funds, where they could devise entirely new algorithms and data strategies. Then a variety of universities developed master’s programs in financial engineering, which churned out a second generation of talent that was more accessible to mainstream firms. The pattern was repeated later in the 1990s with search engineers, whose rarefied skills soon came to be taught in computer science programs. Wikipedia Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data. Data science is related to data mining, machine learning and big data. How to Become a Data Scientist Data scientists are big data wranglers, gathering and analyzing large sets of structured and unstructured data. A data scientist’s role combines computer science, statistics, and mathematics. They analyze, process, and model data then interpret the results to create actionable plans for companies and other organizations. a very short history of #datascience The story of how data scientists became sexy is mostly the story of the coupling of the mature discipline of statistics with a very young one–computer science. The term “Data Science” has emerged only recently to specifically designate a new profession that is expected to make sense of the vast stores of big data. But making sense of data has a long history and has been discussed by scientists, statisticians, librarians, computer scientists and others for years. The following timeline traces the evolution of the term “Data Science” and its use, attempts to define it, and related terms. "],["meet-our-toolbox.html", "Part 3 Meet our toolbox! 3.1 R and RStudio", " Part 3 Meet our toolbox! You can follow along with the slides here if they do not appear below. I recommend installing R, Rstudio, git, and github before starting the Bechdal activity 3.1 R and RStudio 3.1.1 Install R and RStudio &quot;https://www.youtube.com/watch?v=kVIZGCT5p9U&quot; %&gt;% embed_url() %&gt;% use_align(&quot;center&quot;) Install R, a free software environment for statistical computing and graphics from CRAN, the Comprehensive R Archive Network. I highly recommend you install a precompiled binary distribution for your operating system – use the links up at the top of the CRAN page linked above! Install RStudio’s IDE (stands for integrated development environment), a powerful user interface for R. Get the Open Source Edition of RStudio Desktop. You can run either the Preview version or the official releases available here. RStudio comes with a text editor, so there is no immediate need to install a separate stand-alone editor. RStudio can interface with Git(Hub). However, you must do all the Git(Hub) set up described elsewhere before you can take advantage of this. If you have a pre-existing installation of R and/or RStudio, I highly recommend that you reinstall both and get as current as possible. It can be considerably harder to run old software than new. When you upgrade R, you generally also need to update any packages you have installed. 3.1.2 Testing testing Do whatever is appropriate for your OS to launch RStudio. You should get a window similar to the screenshot you see here, but yours will be more boring because you haven’t written any code or made any figures yet! Put your cursor in the pane labeled Console, which is where you interact with the live R process. Create a simple object with code like x &lt;- 3 * 4 (followed by enter or return). Then inspect the x object by typing x followed by enter or return. You should see the value 12 print to screen. If yes, you’ve succeeded in installing R and RStudio. 3.1.3 Add-on packages R is an extensible system and many people share useful code they have developed as a package via CRAN and GitHub. To install a package from CRAN, for example the dplyr package for data manipulation, here is one way to do it in the R console (there are others). install.packages(&quot;dplyr&quot;, dependencies = TRUE) By including dependencies = TRUE, we are being explicit and extra-careful to install any additional packages the target package, dplyr in the example above, needs to have around. You could use the above method to install the following packages, all of which we will use: palmerpenguins, package webpage 3.1.4 Further resources The above will get your basic setup ready but here are some links if you are interested in reading a bit further. How to Use RStudio RStudio’s leads for learning R R FAQ R Installation and Administration More about add-on packages in the R Installation and Administration Manual "],["bechdal.html", "Part 4 Bechdel Activity", " Part 4 Bechdel Activity You can find the materials for the Bechdel activity here. The compiled version should look something like the following… "],["thoughtful-workflow.html", "Part 5 Thoughtful Workflow 5.1 R Markdown 5.2 Git and Github 5.3 Getting Help with R", " Part 5 Thoughtful Workflow At this point, I recommend you pause and think about your workflow. I give you permission to spend some time and energy sorting this out! It can be as or more important than learning a new R function or package. The experts don’t talk about this much, because they’ve already got a workflow; it’s something they do almost without thinking. Working through subsequent material in R Markdown documents, possibly using Git and GitHub to track and share your progress, is a great idea and will leave you more prepared for your future data analysis projects. Typing individual lines of R code is but a small part of data analysis and it pays off to think holistically about your workflow. If you want a lot more detail on workflows, you can wander over to the optional bit on r basics and workflow. 5.1 R Markdown If you are in the mood to be entertained, start the video from the beginning. But if you’d rather just get on with it, start watching at 6:52. You can follow along with the slides here if they do not appear below. R Markdown is an accessible way to create computational documents that combine prose and tables and figures produced by R code. An introductory R Markdown workflow, including how it intersects with Git, GitHub, and RStudio, is now maintained within the Happy Git site: Test drive R Markdown 5.2 Git and Github XKCD on Git First, it’s important to realize that Git and GitHub are distinct things. GitHub is an online hosting platform that provides an array of services built on top of the Git system. (Similar platforms include Bitbucket and GitLab.) Just like we don’t need Rstudio to run R code, we don’t need GitHub to use Git… But, it will make our lives so much easier. Git can be very powerful and useful, but it can also take some getting used to. In this class, we are going to work with some of its most basic functions. We will do all of our interfacing with Git using the GitHub app and website. You can follow along with the slides here if they do not appear below. 5.2.1 What is Github? 5.2.2 Git Git is a distributed Version Control System (VCS). It is a useful tool for easily tracking changes to your code, collaborating, and sharing. (Wait, what?) Okay, try this: Imagine if Dropbox and the “Track changes” feature in MS Word had a baby. Git would be that baby. In fact, it’s even better than that because Git is optimized for the things that social scientists and data scientists spend a lot of time working on (e.g. code). The learning curve is worth it – I promise you! With Git, you can track the changes you make to your project so you always have a record of what you’ve worked on and can easily revert back to an older version if need be. It also makes working with others easier -— groups of people can work together on the same project and merge their changes into one final source! GitHub is a way to use the same power of Git all online with an easy-to-use interface. It’s used across the software world and beyond to collaborate and maintain the history of projects. There’s a high probability that your favorite app, program or package is built using Git-based tools. (RStudio is a case in point.) Scientists and academic researchers are starting to use it as well. Benefits of version control and collaboration tools aside, Git(Hub) helps to operationalize the ideals of open science and reproducibility. Journals have increasingly strict requirements regarding reproducibility and data access. GH makes this easy (DOI integration, off-the-shelf licenses, etc.). I run my entire lab on GH; this entire course is running on github; these lecture notes are hosted on github… 5.3 Getting Help with R You can follow along with the slides here if they do not appear below. "],["r_basics.html", "Part 6 R basics and workflows 6.1 Basics of working with R at the command line and RStudio goodies 6.2 Workspace and working directory 6.3 Saving an R script 6.4 Script housekeeping 6.5 To do before next class", " Part 6 R basics and workflows Who is R? Why is R troubling PhD students?@AcademicChatter #AcademicTwitter&mdash; Dr. Marie Curie (@CurieDr) January 31, 2021 There is an implicit contract with computer and scripting languages. The computer will do tedious tasks for you. In return, you must be explicit in your instructions. The computer does not have the ability to extrapolate. So we have to work within the range of what it does understand. And that is where this entire course comes into play. We are learning how to communicate in a way that it does understand. So let’s begin! 6.1 Basics of working with R at the command line and RStudio goodies Launch RStudio/R. Notice the default panes: Console (entire left) Environment/History (tabbed in upper right) Files/Plots/Packages/Help (tabbed in lower right) FYI: you can change the default location of the panes, among many other things: Customizing RStudio. Go into the Console, where we interact with the live R process. Make an assignment and then inspect the object you just created: x &lt;- 3 * 4 x #&gt; [1] 12 All R statements where you create objects – “assignments” – have this form: objectName &lt;- value and in my head I hear, e.g., “x gets 12”. You will make lots of assignments. The &lt;- is more commonly used by R programmers. You can also use = instead. There are some small differences between the two, but they rarely come up. Feel free to use whichever you prefer. For typing &lt;-, try RStudio’s keyboard shortcut: Alt + - (the minus sign). Notice that RStudio auto-magically surrounds &lt;- with spaces, which demonstrates a useful code formatting practice. Code is miserable to read on a good day. Give your eyes a break and use spaces. RStudio offers many handy keyboard shortcuts. Also, Alt+Shift+K brings up a keyboard shortcut reference card. 6.1.1 Object names Object names cannot start with a digit and cannot contain certain other characters such as a comma or a space. You will be wise to adopt a convention for demarcating words in names. i_use_snake_case evenOthersUseCamelCase dont.use.periods Don’t use periods. I recommend snake_case or camelCase. Make another assignment: this_is_a_really_long_name &lt;- 2.5 To inspect this assignment, try out RStudio’s completion facility: type the first few characters, press TAB, add characters until you disambiguate, then press return. Make another assignment: brenton_rocks &lt;- 2 ^ 3 Let’s try to inspect: brentonrocks #&gt; Error in eval(expr, envir, enclos): object &#39;brentonrocks&#39; not found brent_rocks #&gt; Error in eval(expr, envir, enclos): object &#39;brent_rocks&#39; not found Here’s where that implicit contract comes in. The computer (and R) will do amazing things, if we can ask it to do those things in a way it understands. Typos matter. Case matters. Precision matters. We have to work within it’s narrow range of ability. 6.1.2 Functions R has a mind-blowing collection of built-in functions that are accessed like so: functionName(arg1 = val1, arg2 = val2, and so on) Let’s try using seq() which makes regular sequences of numbers and, while we’re at it, demo more helpful features of RStudio. Type se and hit TAB. A pop up shows you possible completions. Specify seq() by typing more to disambiguate or using the up/down arrows to select. Notice the floating tool-tip-type help that pops up, reminding you of a function’s arguments. If you want even more help, press F1 as directed to get the full documentation in the help tab of the lower right pane. Now open the parentheses and notice the automatic addition of the closing parenthesis and the placement of cursor in the middle. Type the arguments 1, 10 and hit return. RStudio also exits the parenthetical expression for you. IDEs are great. seq(1, 10) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 The above also demonstrates something about how R resolves function arguments. You can always specify in name = value form. But if you do not, R attempts to resolve by position. So above, it is assumed that we want a sequence from = 1 that goes to = 10. Because we didn’t specify step size, the default value of by in the function definition is used. In this case, the default is 1. For functions I call often, I might use this resolve by position for the first argument or maybe the first two. After that, I always use name = value. Make this assignment and notice similar help with quotation marks. yo &lt;- &quot;hello world&quot; If you just make an assignment, you don’t get to see the value, so then you’re tempted to immediately inspect. y &lt;- seq(1, 10) y #&gt; [1] 1 2 3 4 5 6 7 8 9 10 This common action can be shortened by surrounding the assignment with parentheses, which causes assignment and “print to screen” to happen. (y &lt;- seq(1, 10)) #&gt; [1] 1 2 3 4 5 6 7 8 9 10 Not all functions have (or require) arguments: date() #&gt; [1] &quot;Fri Apr 8 04:50:58 2022&quot; Now look at your workspace – in the upper right pane. The workspace is where user-defined objects accumulate. You can also get a listing of these objects with commands: objects() #&gt; [1] &quot;all_pkgs&quot; &quot;brenton_rocks&quot; #&gt; [3] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [5] &quot;install_quietly&quot; &quot;pretty_install&quot; #&gt; [7] &quot;sample_no_surprises&quot; &quot;session&quot; #&gt; [9] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [11] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [13] &quot;y&quot; &quot;yo&quot; ls() #&gt; [1] &quot;all_pkgs&quot; &quot;brenton_rocks&quot; #&gt; [3] &quot;check_quietly&quot; &quot;ds4p_urls&quot; #&gt; [5] &quot;install_quietly&quot; &quot;pretty_install&quot; #&gt; [7] &quot;sample_no_surprises&quot; &quot;session&quot; #&gt; [9] &quot;shhh_check&quot; &quot;slide_url&quot; #&gt; [11] &quot;this_is_a_really_long_name&quot; &quot;x&quot; #&gt; [13] &quot;y&quot; &quot;yo&quot; If you want to remove the object named y, you can do this: rm(y) To remove everything: rm(list = ls()) or click the broom in RStudio’s Environment pane. 6.2 Workspace and working directory One day you will need to quit R, go do something else and return to your analysis later. One day you will have multiple analyses going that use R and you want to keep them separate. One day you will need to bring data from the outside world into R and send numerical results and figures from R back out into the world. To handle these real life situations, you need to make two decisions: What part your analysis is “real”, i.e. will you save it as your lasting record of what happened? Where does your analysis “live”? 6.2.1 Workspace, .RData Start to quit RStudio, but don’t finish yet! Quit R/RStudio, either from the menu, using a keyboard shortcut, or by typing q() in the Console. You’ll get a prompt like this: Save workspace image to ~/.Rdata? This is offering to save your R workspace (all of the objects you made) so that you can reload them later. This seems okay, but how are you going to remember where each of those objects came from? It’s a recipe for irreproducible disaster! Let’s change some settings in RStudio to encourage a more reliable workflow. In RStudio, click Tools -&gt; Global options… In the window that pops up, on the General pane, Uncheck “Restore .RData into workspace at startup Set “Save workspace to .RData on exit” to “Never” Uncheck “Always save history (even when not saving .RData)” 6.2.2 Working directory Any process running on your computer has a notion of its “working directory”. In R, this is where R will look, by default, for files you ask it to load. It also where, by default, any files you write to disk will go. You can explicitly check your working directory with: getwd() It is also displayed at the top of the RStudio console. For right now, we will let R set its own working directory at startup. We will adopt a more reliable workflow for organizing our projects in a few weeks. You might sometimes see something like this in someone’s script. setwd(&quot;C:\\\\Users\\\\brenton\\\\Documents\\\\myCoolProject&quot;) This sets the working directory in the R session to that file path. Do not do this! It ensures that the script only works on your computer! We will explore more reliable ways to control the working directory in a few weeks. 6.3 Saving an R script Usually, we want to save the analyses we run so we can re-run them later or refer back to them. We do that with scripts. Let’s make an R script. Click File -&gt; New File -&gt; R Script. A new part of the RStudio window appears. The Script pane. Copy and paste the following code into your new script. # what do these lines do? a &lt;- 2 b &lt;- -3 sig_sq &lt;- 0.5 # what about these? x &lt;- runif(40) y &lt;- a + b * x + rnorm(40, sd = sqrt(sig_sq)) (avg_x &lt;- mean(x)) #&gt; [1] 0.474 # these lines save some output... write(avg_x, &quot;avg_x.txt&quot;) plot(x, y) abline(a, b, col = &quot;purple&quot;) dev.print(pdf, &quot;toy_line_plot.pdf&quot;) #&gt; quartz_off_screen #&gt; 2 Run the lines of your script by selecting them and clicking the Run button in RStudio or by typing Ctrl/Cmd + Shift + Enter/Return. Now let’s save the file. Click on the floppy disk to save. Give it a name ending in .R, I used toy-line.R. Now which folder the file will be saved in. By default, it will go in the current working directory. Quit RStudio. Go to the folder where you saved the file and it there. Restart RStudio. Notice that the the files you had open are restored by default. That’s helpful. Let’s change our script to make the sample size easily editable. At the top of your script, assign a new sample size to n, e.g. n &lt;- 80. Then, replace all the hard-coded 40s with n. Change some other minor-but-detectable stuff, e.g. alter the slope of the line b, the color of the line, … whatever. Practice the different ways to re-run the code: Walk through line by line by keyboard shortcut (Command + Enter) Walk line by line with the mouse (click “Run” in the upper right corner of editor pane). Select multiple lines and run using the keyboard shortcut or Run button. Visit your figure in your computer’s file system and view it to verify that the PDF is changing as you expect. Note that you have edited this figure using only code. You never clicked your mouse, typed a file name, or used the keyboard. This means that you can reproduce the figure (or change it) easily again in the future with with no head-scratching! 6.4 Script housekeeping Always save your R scripts with a .R so that your computer knows what to do with them. In R, comments start with one or more # symbols. Use them. RStudio helps you (de)comment selected lines with Ctrl+Shift+C (Windows and Linux) or Command+Shift+C (Mac). To be sure your code is doing what you expect, a good habit is to restart your R session and run your script from the top. To do that, click Session -&gt; Restart R. Try it! Avoid using the mouse for pieces of your analytical workflow, such as loading a dataset or saving a figure. Terribly important for reproducibility and for making it possible to retrospectively determine how a numerical table or PDF was actually produced (searching on local disk on filename, among .R files, will lead to the relevant script). 6.5 To do before next class Make an account on GitHub. Make your username recognizable! Please put up a profile photo or image on GitHub—it makes the class community more personable and easier to work with. Finish any in-class activities listed in today’s section of the guidebook that you didn’t get done. Install the software stack for this course, as indicated below. Optionally, register for the Student Developer Pack with GitHub for a bunch of free perks for students! 6.5.1 Software Stack Installation Install R and RStudio. R here: https://cran.r-project.org RStudio here: https://www.rstudio.com/products/rstudio/download/ Commentary on installing this stuff can be found at stat545.com: r-rstudio-install Install GitHub: https://desktop.github.com/ "],["r_scipts.html", "Part 7 R Scripts 7.1 Assign an Object 7.2 Types of Objects and Operations", " Part 7 R Scripts Usually, we want to save the analyses we run so we can re-run them later or refer back to them. We do that with scripts. Let’s make an R script. Click File -&gt; New File -&gt; R Script. A new part of the RStudio window appears. The Script pane. This is where you can write a series of lines of code than you can then execute (send to the Console to run) later. 7.1 Assign an Object Pick a number, assign it to an object called favorite_number, and print it. favorite_number &lt;- 42 favorite_number #&gt; [1] 42 Run these lines of code by placing your cursor on the line and either clicking the Run button or typing Ctrl/Cmd + Shift + Enter/Return. 7.2 Types of Objects and Operations Now let’s explore some basic types of objects and operations in R. 7.2.1 Vectors Vectors store multiple entries of one data type, like numbers or characters. You’ll discover that they show up just about everywhere in R. Let’s collect some data and store this in a vector called times. How many hours did you sleep last night? Drop your answer in the chat. Here’s starter code: times &lt;- c() The c() function is how we make a vector in R. The “c” stands for “concatenate”. Operations happen component-wise. Change those times to minutes. How can we “save” the results? All parts of a vector have the same type. There are many types of variables in R. The most common types are: numeric (numbers) 1. double (numbers with decimal values; 2, 3.4, 1000) 1. integer (1L, 2L, 100L) character (words or strings; \"a\", \"foo\", \"lastname\") logical (TRUE or FALSE) factor (categorical variable; factor(c(\"control\", \"experiment\"))) 7.2.2 Functions R comes with many many functions. Functions take one or more inputs and return one or more outputs. You can think of functions as prewritten R programs. Functions all take the general form: functionName(arg1 = val1, arg2 = val2, and so on) To call a function, type its name, then parentheses (). Inside the (), type the arguments and values to use as input. What’s the average sleep time? Let’s compute that using the mean() function. mean(times) #&gt; [1] 7.9 To learn how a function works, we can look at its help file. Open the help file for mean() by typing ?mean or help(mean) in the console. The help file will includes the following: A brief description of the function A list of the arguments and how to call the function A detailed description of the arguments (Optionally) Other usage details Coded examples We can see that mean() has 4 arguments: x: A vector to compute the mean of trim: the fraction (0 to 0.5) of observations to be trim from each end na.rm: TRUE or FALSE–should missing values be removed before computing? ...: Other arguments. More on that later. Default values for arugments are given in the Usage section by =. If an argument has no default (like x in mean()), it usually means it’s required. Let’s compute the trimmed mean of times, dropping 10% of values from each end. You can either enter arguments in order: mean(times, .1) #&gt; [1] 7.94 Or by name: mean(times, trim = .1) #&gt; [1] 7.94 It’s good practice to name all arguments after the first (or maybe second if its clear). Try out some other functions, such as sd(), range(), and length(). Much of R is about becoming familiar with R’s “vocabulary”. A nice list can be found in Advanced R - Vocabulary. 7.2.3 Comparisons How many people slept less than 6 hours? Let’s answer that using comparisons. We can compare the values of times to another value using &lt;. times &lt; 6 #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE Comparisons return a vector of logical values. We can do other logical comparisons: times &gt; 6 #&gt; [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #&gt; [13] FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE times == 5 #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #&gt; [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE times &lt;= 7 #&gt; [1] TRUE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE TRUE FALSE FALSE #&gt; [13] TRUE TRUE TRUE FALSE FALSE FALSE FALSE TRUE times != 2 #&gt; [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE #&gt; [16] TRUE TRUE TRUE TRUE TRUE We can combine multiple comparisons using &amp; (AND), | (OR), and ! (NOT). (times &lt; 4) | (times &gt; 9) #&gt; [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE #&gt; [13] FALSE FALSE FALSE TRUE FALSE TRUE FALSE FALSE Try out these functions that work with logical values. any(times &lt; 6) #&gt; [1] TRUE all(times &lt; 6) #&gt; [1] FALSE which(times &lt; 6) #&gt; [1] 20 7.2.4 Subsetting Use [] to subset values from a vector. You can subset with an integer (by position) or with logicals. times[4] #&gt; [1] 7 times[c(2, 5)] #&gt; [1] 8 7 times[-6] #&gt; [1] 6 8 9 7 7 8 9 9 7 8 10 6 6 6 10 9 10 9 5 times[-c(2, 3)] #&gt; [1] 6 7 7 9 8 9 9 7 8 10 6 6 6 10 9 10 9 5 times[4:8] #&gt; [1] 7 7 9 8 9 times[times &gt;= 7] #&gt; [1] 8 9 7 7 9 8 9 9 7 8 10 10 9 10 9 Subset times: Extract the third entry. Extract everything except the third entry. Extract the second and fourth entry. The fourth and second entry. Extract the second through fifth entry. Extract all entries that are less than 4 hours Why does this work? Logical subsetting! 7.2.5 Modifying a vector You can change the vector by combining [] with &lt;-. Let’s say that we learned that the second time was incorrect and we wanted to replace it with missing data. In R, missing data is noted by NA. Replace the second entry in times with NA. Now, let’s “cap” all entries that are bigger than 8 at 8 hours. If this is more than one value, why don’t we need to match the number of values? Recycling! Be careful of recycling! Let’s compute the mean of these new times: mean(times) #&gt; [1] 7.9 What happened? How do we compute the mean of the non-missing values? 7.2.6 Data frames We usually work with more than one variable at a time. When we do that, we will work with data frames. A data frame is a list of vectors, all of the same length. R has some data frames “built in”. For example, some car data is attached to the variable name mtcars. Print mtcars to screen. Notice the tabular format. Your turn: Finish the exercises of this section: Use some of these built-in R functions to explore mtcars head() tail() str() nrow() ncol() summary() row.names() names() What’s the first column name in the mtcars dataset? Which column number is named \"wt\"? With data frames,each column is its own vector. You can extract a vector by name using $. For example, we can extract the cyl column with mtcars$cyl. You can also extract columns using [[]]. For example, try mtcars[[\"cyl\"]] or mtcars[[2]]. Extract the vector of mpg values. What’s the mean mpg of all cars in the dataset? 7.2.7 R packages Often, the suite of functions that “come with” R are not enough to do an analysis. Sometimes, the suite of functions that “come with” R are not very convenient. In these cases, R packages come to the rescue. These are “add ons”, each coming with their own suite of functions and objects, usually designed to do one type of task. CRAN stores many R packages that. It’s easy to install packages from CRAN using the install.packages() function. Run the following lines of code to install the tibble and gapminder packages. (But don’t include install.packages() lines in your scripts—it’s not very nice to others!) install.packages(&quot;tibble&quot;) install.packages(&quot;palmerpenguins&quot;) tibble: a data frame with some useful “bells and whistles” palmerpenguins: a package with the penguins dataset (as a tibble!) After you install a package, you need to load it to use it. Use the library() function to load a package. (Note: Do not use the similar function require() to load packages. This has some different, undesirable, behavior for normal usage.) Run the following lines of code to load the packages. (Put these in your scripts, and near the top.) library(tibble) library(palmerpenguins) You can explore the objects in a package in the Environment pane. Try the following two approaches to access information about the tibble package. Run the lines one-at-a-time. Vignettes are your friend, but do not always exist. ?tibble browseVignettes(package = &quot;tibble&quot;) Print out the penguins object to screen. It’s a tibble—how does it differ from a data frame in terms of how it’s printed? "],["markdown.html", "Part 8 Markdown 8.1 Markdown syntax 8.2 RMarkdown", " Part 8 Markdown Markdown is a lightweight syntax for writing documents. Markdown documents can contain text, formatting, images, links, and more! Some cheat sheets for “quick reference”: GitHub’s markdown cheatsheet RStudio’s RMarkdown cheatsheet Further reading: The Rmd website has a fantastic walk-through tutorial that gives a great overview of RMarkdown. There’s also a nice overview video on the Rmd website, too. Yihui’s Rmd book for lots more on RMarkdown. Other explorations of this content: Interactive tutorial for learning markdown. The Happy Git with R: Rmd test drive. 8.1 Markdown syntax Markdown is plain text with a straightforward, readable way of marking up your text. Let’s see GitHub’s cheat sheet. We will use RStudio to convert Markdown to output formats like HTML or PDF. 8.1.1 Make a Markdown document Make a new Markdown file in RStudio, then save it as exploring_markdown.md. Add some text, such as introducing yourself and what your favorite animal is. Mark up the text with some markdown features (e.g., bold, italic, bullets, a link to a URL on the internet). Use the file extension .md for regular Markdown files with no R code in them. 8.1.2 Render exploring_markdown.Rmd We can use RStudio to convert our plain text Markdown document into various output formats. Above the script editor in RStudio, click the Preview or Knit button and convert your file to HTML. 8.1.3 Output formats There are generally two prominent file types to display documents of various types: pdf: This is useful if you intend to print your work onto a physical sheet of paper, or for presentation slides. If this is not the primary purpose, then try to avoid it, because formatting things so that it fits to the page can be more effort than its worth (unless you’re making presentation slides). - Example: Most journals articles and preprints. html: This is what you see when you visit a webpage. Content does not need to be partitioned to pages. - Example: My website main page, and its corresponding html file. We’ll be treating pdf and html files as output that should not be edited. Markdown is the source that is edited. 8.1.4 Word processor formats It is also possible to output files to word processor formats, such as Word (.docx), LibreOffice/OpenDocument (.odt), or Rich Text (.rtf). You can also output to other slideshow software, such as PowerPoint (.pptx) or LibreOffice/OpenDocument Slides (.odp). We aren’t going to use these in this class because we will focus on making fully reproducible documents. There are times when you have to use these formats (e.g., a journal requires Word, a conference requires PowerPoint, your advisor or collaborator requires Word). If you have to do this, try to avoid editing your R output in these formats. If you need to make revisions, go back and make changes to the source code, rather than the rendered document. In your general work, you can find a balance working with automated output and Word documents, but we are going to focus on fully reproducible documents in this class. 8.1.5 Other Output Formats RMarkdown can be rended to many other formats that we won’t have time to cover (see the RMarkdown documentation and the pandoc documentation). 8.2 RMarkdown RMarkdown (Rmd) combines Markdown and R scripts into one! It includes code chunks with R code (or other languages) that is run before the document is knitted. Here’s RStudio’s cheat sheet on Rmd. You can see that it has more features than “regular” markdown! 8.2.1 Code Chunks The parts of your document inside the “fences” ``` are code chunks. When you render the RMarkdown document, R will run the code in the chunks and show the output in the rendered document. You can run the code from a chunk interactively by placing your cursor on the line and typing Ctrl/Cmd + Enter/Return or by clicking the green “play” button at the top right of the code chunk. Add a new code chunk by doing one of these: Clicking the Insert button and choosing R or by typing -&gt; “R” Typing Mac: Cmd + Option + I or Windows: Ctrl + Alt + I Manually typing three back ticks followed by {r} in curly brackets: ```{r}, then typing three back ticks on a later line to “close” the code block: ```. Add a code chunk near the top of the file and load the tibble package. library(tibble) library(knitr) If you don’t have knitr installed, install it with install.packages(\"knitr\"). 8.2.2 Rendering output In a new code chunk, convert the mtcars data frame to a tibble using the tibble::as_tibble() function and assign it as a new object (e.g., called mtcars_tbl). Print it out by typing its name or using the print() function. When you print with just the print() function, your table will look like R console script in your output HTML or PDF. To make your tables look nicer in the output, use the knitr::kable() function to convert the results to a Markdown table. In a new code chunk, print the mtcars_tbl using knitr::kable(). We will explore other table tools in future classes. Add some markdown commentary about the tables you are showing. Your markdown commentary needs to go outside of the code chunks. You can also include R code “in-line” with markdown text. This is useful, for example, in your results section of a paper to report the results of your analyses without having to copy-paste them (and make errors). Add an in-line code chunk specifying the number of rows of the mtcars dataset like this: The `mtcars` dataset has 32 rows. Now, “Knit” to HTML. "],["github.html", "Part 9 GitHub and Version Control 9.1 Learning Objectives 9.2 Resources 9.3 Git and GitHub 9.4 Understanding the GitHub flow 9.5 Making a GitHub Repository 9.6 Navigating a GitHub Repository 9.7 Cloning a GitHub repo to your computer 9.8 RStudio Projects 9.9 The Version Control Workflow", " Part 9 GitHub and Version Control 9.1 Learning Objectives By the end of this module, you will be able to: Understand the benefits of a version control workflow Commit file changes to GitHub Navigate the commit history of a repository and a file on GitHub 9.2 Resources If you want to learn more about Git and GitHub, check out: The GitHub “Hello World” is a nice intro activity A short video explaining what GitHub is Git and GitHub learning resources Understanding the GitHub flow How to use GitHub branches Interactive Git training materials 9.3 Git and GitHub We will be using GitHub a lot in this course. GitHub is similar to Dropbox, OneDrive, or other cloud storage services. There are 3 key benefits of GitHub compared to those other services. GitHub is a version control platform. It is designed to track exactly what changes you make and when you make them. You can browse the history GitHub is designed for code. You can track what changes you make line by line You can integrate your code with automated tools for error checking, testing, etc. GitHub is designed for collaborative coding. One master copy People can make simultaneous edits No emailing files back and forth Tools for checking and verifying each other’s code We will practice using GitHub for collaboration through the weekly homework. 9.4 Understanding the GitHub flow The GitHub flow is a lightweight workflow that allows you to experiment and collaborate on your projects easily, without the risk of losing your previous work. 9.4.1 Repositories (“Repos”) A repository is where your project work happens–think of it as your project folder. It contains all of your project’s files and revision history. You can work within a repository alone or invite others to collaborate with you on those files. You should make a new GitHub repo for each project. In this class, you will make a repo for your lab activities and one for each of your portfolio pieces. 9.4.2 Cloning When a repository is created with GitHub, it’s stored remotely in the ☁️. You can clone a repository to create a local copy on your computer and then use Git to sync the two. This is similar to tools like Dropbox, but designed for code with a detail change history. Cloning a repository also pulls down all the repository data that GitHub has at that point in time, including all versions of every file and folder for the project! This can be helpful if you experiment with your project and then realize you liked a previous version more. To learn more about cloning, read “Cloning a Repository”. 9.4.3 Committing and pushing Committing and pushing are how you can add the changes you made on your local machine to the remote repository in GitHub. That way your instructor and/or teammates can see your latest work when you’re ready to share it. Any changes you make on your local computer aren’t “final” until you “commit” them (lock them in and write them into the repo history) and “push” those commits up to the GitHub cloud. You can make a commit when you have made changes to your project that you want to “checkpoint.” You can also add a helpful commit message to remind yourself or your teammates what work you did (e.g. “Added a README with information about our project”). Once you have a commit or multiple commits that you’re ready to add to your repository, you can use the push command to add those changes to your remote repository. Committing and pushing may feel new at first, but I promise you’ll get used to it 🙂 9.5 Making a GitHub Repository Let’s make a GitHub repository that you will use for this class. On https://GitHub.com, make a new repository called “progdata-class”. As you make it, do the following: Give your repo a useful description. Make it Public. Check “Add a README file” Check “Add .gitignore” and select “R” from the dropdown. 9.6 Navigating a GitHub Repository Let’s take a look at various parts of a GitHub repo. Files and folders/directories (“code”) History Issues There are other important features of GitHub (branches, issues) that we aren’t going to use in class for now. Talk to me if you want to learn more. Let’s practice editing a file with GitHub. Make a new file on your participation repository and edit it: - Click on the &quot;Create New File&quot; button on your repository&#39;s home page. - Call it `navigating_github.md` - Write something (e.g., what&#39;s your favorite color?). - Commit (&quot;save&quot;) the file by clicking on green &quot;commit new file&quot; button at the bottom of the page. - Now type something else (e.g., what&#39;s your favorite animal?) and commit the change. Now look at your commit history in the “History” tab. 9.7 Cloning a GitHub repo to your computer You can download a copy of your repository to your computer (cloning it), make changes (commits) there, then push them back to GitHub. Click big green “Code” button at the top of your repo page and choose “Open with GitHub Desktop”. You can also do this from the File menu in GitHub Desktop. Note where the repository folder is being saved on your computer. Save it somewhere easy to find and not in a cloud folder. Go to the folder on your computer. Any changes you make to files in this folder are tracked by the GitHub Desktop app and can be committed and pushed up to GitHub. Open your navigating_github.md file, make a change, then save, commit, and push it. Go look at your change on GitHub. 9.8 RStudio Projects RStudio projects are a way to manage which folder R runs in on your computer, where it looks for files to read in, and where it writes its output. In RStudio, click File → New Project… → Existing Directory. Navigate to your GitHub repo folder. This will add an RStudio Project (.Rproj) file to the folder. 9.8.1 The working directory When you open R, it “runs” in some folder on your computer. This is the place it will look for files to import and write files as output. Think about where your Rmd output files end up when you knit them. If you have R/RStudio closed, and you open a .R or .Rmd file, R/RStudio will start in the folder holding that file. If you open R/RStudio from the Windows Start menu, the Mac dock, the Mac Spotlight, etc., R/Studio will start in its default location (probably your user home directory, see Tools → Global Options → General → Default working directory…). When I say “R/Studio will start in…”, what I am referring to is R’s “working directory”. Like I say above, this is the place R will look for files to import and write files as output. You can check what R’s current working directory is using the getwd() function: getwd() #&gt; [1] &quot;/Users/runner/work/progdata-class/progdata-class&quot; You can also change the working directory using the setwd() function: setwd(file.path(&quot;path&quot;, &quot;to&quot;, &quot;folder&quot;)) Do not use setwd()! You should always write your R scripts so that the entire project is self-contained in a folder. All of the scripts, folders, data, output, etc. should all “live” within this project folder. We will talk a lot about how to do this throughout the semester. For now, we will start by working with RStudio Projects to make this easier. When you double click on a .Rproj file, it: Opens a new fresh R session, with The working directory set to the location of the .Rproj file, and No connection whatsoever to any other R sessions you already have open You can also set specfic options for each RStudio project (e.g., number of spaces to insert when you type Tab, etc.). Let’s practice closing RStudio and re-opening it by opening the RStudio project. Run getwd() to see where R is running. 9.9 The Version Control Workflow Now, let’s practice the workflow to work with your files with GitHub. 9.9.1 Editing a file and making a commit Open the README.md file in your local git folder by clicking on it in the Files pane. README.md is a special file that will show when viewing a folder on GitHub. Use README files to describe the contents of a folder and what’s going on there. Type “Hello world” and save the file. Now, go back to the GitHub Desktop app. It shows you the files in the git repo folder that have changed since the last commit. Any files shown here have had changes made. (Remember, you need to save the file in RStudio [so the title on the Source tab isn’t blue] before they appear in this list.) Changes shown here are not saved in the repo history until you commit them. If you’ve changed several files but want to only commit changes for some, uncheck the box next to the files you don’t want to commit yet. You can view the changes in more detail by selecting the file name. If you want to undo any changes and revert back to the previous committed version of a file, right click on the file name and click “Revert changes”. To commit your changes, type a at the bottom of the right column describing what you’ve changed. Always give informative commit messages (help out future you!). When you are ready, click the “Commit” button. Now, you’ve made the commit locally, but you need to “push” it to GitHub so that it shows up there as well. Click the “Push” (up arrow) button. Now go check out the file on GitHub online. 9.9.2 Fetching or Pulling Changes from the Remote Repo One of the amazing things about git is that it can track changes made to a file at different locations or on different computers and reconcile them together. (This is how it is so useful for collaboration!) Let’s see how that works. First, let’s make a change directly on the GitHub website. Edit README.md and type “oops!” at the top. Commit your change. Go back to GitHub Desktop Click the “Fetch Origin” button. See how the file on your local computer changed to reflect the change you made online. Let’s fix that “oops!”. Delete it, commit your change (give an informative commit message), and push your changes back online. 9.9.3 The General Workflow You will use this basic workflow througout the semester (and your coding career). When you sit down to start working: Open GitHub Desktop and select your repo. Click the Fetch Origin button to fetch remote changes and get your local repo copy up to date. Open you RStudio Project. Make changes to your files. Commit your changes. Push your commits back up to the remote repo (GitHub). You should get in the habit of commit early, commit often. Don’t wait until you are completely done with your work to commit the changes. Make small commits as you go. This makes it much easier to go back if you accidentally break something and need to revert. "],["lab-1-getting-started-with-rstudio-git-and-markdown.html", "Part 10 Lab 1: Getting started with RStudio, Git, and Markdown 10.1 Part 1: Set up R and RStudio 10.2 Part 2: Make a README.md file with Markdown 10.3 Part 3: Practice some R fucntions 10.4 Part 4: Make a GitHub Repository for your work", " Part 10 Lab 1: Getting started with RStudio, Git, and Markdown In this lab you will do 4 things: Get acquainted with RStudio and customize it to your liking. Practice writing a document with Markdown. Practice using some basic functions in R. Make a GitHub repository to save and share your work. 10.1 Part 1: Set up R and RStudio First, install two packages we will use a lot in class: tidyverse palmerpenguins Next, customize some RStudio settings: Click Tools → Global Options… 1. On the first page, under Workspace: Uncheck Restore .RData into workspace at startup Set Save workspace to .RData on exit to Never Under History: Uncheck Always save history (even when not saving .RData) Click the Advanced button at the top. Under Other: Check Show .Last.value in environment listing Now, adjust the appearance of RStudio to your liking: In the Pane Layout window, you can rearrange RStudio’s 4 panes. In the Appearance window, you can change your font size, code typeface, and RStudio color scheme. - I use the Tomorrow Night Bright color scheme. In the Code window, you can change various ways about how code is inserted and shown. Change 3 settings from their defaults: On the Editing page, under General, check Use native pipe operator |&gt; (requires R 4.1+) On the Editing page, under Execution ,set Ctrl/Cmd+Enter executes to Multiple consecutive R lines On the Display page, check Rainbow parentheses In the RMarkdown window, you can change how RMarkdown is shown while you edit it. - Uncheck Show output inline for all RMarkdown documents. Go ahead and customize your RStudio appearance (color theme, font, etc.) and pane layout to whatever you find appealing. You can always come back and change these later until you find a setup you like. 10.2 Part 2: Make a README.md file with Markdown In RStudio, make a Markdown file from the File menu. Save this file with the name README.md. README should be in all caps. This file will be the introduction to your class GitHub repo. Use it to introduce yourself and practice your Markdown skills. The beginning of the README should contain a very brief description as to what the repository is (a sentence or two), so that a visitor landing on the repository can orient themselves. You should also help the visitor navigate your repository (in whatever way you think is most appropriate). Then, introduce yourself briefly. When writing your README, be sure to showcase at least five functionalities of GitHub-flavored markdown. The markdown cheatsheet might help here, or, the Help menu in RStudio will bring up a Markdown Quick Reference at any time. Here’s a sample README file that goes way above and beyond what I’m looking for (aside from describing the repo). 10.3 Part 3: Practice some R fucntions In RStudio, make an R script file from the File menu. In this file, you will explore the palmerpenguins::penguins dataset. Use some R functions to explore this data frame a bit. For example, what is the general structure of the dataset? How many rows does it have? How many colums? What are the descriptive statistics for some of the important variables? You should: 1. Demonstrate the use of at least three functions. 2. Write code comments with # describing what the functions you are using do. Save your R script with a useful name, such as lab01_exploring-penguins.R. 10.4 Part 4: Make a GitHub Repository for your work On GitHub.com, make a new repository called “progdata-class”. As you make it, do the following: Give your repo a useful description. Make it Public. Check “Add a README file” Check “Add .gitignore” and select “R” from the dropdown. Clone your repo to the GitHub Desktop app. If you are using a Mac anywhere, also do this: 6. In RStudio, open the .gitignore file. Add “.DS_Store” (no quotes) to the end of the file. Save the file and commit the change. Now, add your files for this lab to your repo and commit them. Remember, commit early, commit often! Make small changes at a time. Write a useful commit message for each commit. Add your README.md file and commit it. Add your data exploration R script to a folder called “Lab 1” and commit it. Add any other scripts or files you made in class to the Lab 1 folder as well. Push your changes up to GitHub. Finally, copy a link to your repo to the thread on the class Teams. "],["plotting-with-ggplot2.html", "Part 11 Plotting with ggplot2 11.1 Learning Objectives 11.2 Resources", " Part 11 Plotting with ggplot2 Set up the workspace: # Load required packages library(tidyverse) # loads ggplot2 and other tidyverse packages library(gapminder) # loads the gapminder dataset # Set a default figure size knitr::opts_chunk$set(fig.width = 5, fig.height = 4, fig.align = &quot;center&quot;) 11.1 Learning Objectives By the end of this lesson, you will be able to: Have a sense of why we’re learning ggplot2 Understand the importance of statistical graphics in communicating information Identify the components of the grammar of graphics underlying ggplot2 Use different geometric objects and aesthetics to explore various plot types 11.2 Resources Here are some good walkthroughs that introduce ggplot2: r4ds: data-vis chapter. The ggplot2 book, Chapter 2 Jenny Bryant’s ggplot2 tutorial Andrew Heiss’s data visualization course Here are some good resource to use as a reference: ggplot2 cheatsheet R Graphics Cookbook "],["plotting-in-r.html", "Part 12 Plotting in R 12.1 Just Plot It", " Part 12 Plotting in R TL;DR: We’re using ggplot2 in progdata. R has several frameworks for building graphics. One of the earliest advantages when R (and its predecessor S) were introduced in the 1980s was its professional plotting capabilities. However, the “base R” plotting methods, mostly accessed using the plot() function, can be involved, and requires a lot of “drawing by hand”. That said, for simple “quick looks” at data, base R plots can be useful: plot(airquality[,1:4]) The most widely used modern plotting tool in R is ggplot2, which provides a very powerful framework for making plots. It has a theoretical underpinning, too, based on the Grammar of Graphics, first described by Leland Wilkinson in his “Grammar of Graphics” book. With ggplot2, you can make a great many type of plots with minimal code. It’s been a hit in and outside of the R community. A big advantage of ggplot2 is that many people have written extensions for it, such as ggdist (for plotting distributions and intervals), gganimate (for animations and GIFs), plotly for interactive graphs, and see (ready-built visualizations for many models). 12.1 Just Plot It The human visual cortex is a powerful thing. If you’re wanting to point someone’s attention to a bunch of numbers, you probably won’t get any “aha” moments by displaying a large table like this, either in a report or (especially!) a presentation. Make a plot to communicate your message! Florence Nightingale was a pioneer in many fields (e.g., nursing, sanitation, public health, statistics). Her contributions to statistics (a field generally prohibited to women in her era) were centered around communicating data using visualization. Picture of Florence Nightingale During the Crimean Wars, she convinced the British military to implement a variety of sanitation measures through effective and innovative data visualization. Florence Nightingale’s Rose Diagram If you really feel the need to tell your audience every number exactly, consider putting your table in an appendix. Because chances are, the reader doesn’t care about the exact numeric values. Or, perhaps you just want to point out one or a few numbers, in which case you can put that number directly on a plot. "],["the-grammar-of-graphics.html", "Part 13 The Grammar of Graphics 13.1 Example: Scatterplot grammar 13.2 Activity: Bar chart grammar", " Part 13 The Grammar of Graphics You can think of the grammar of graphics as a systematic approach for describing the components of a graph. It breaks down “classic” plots into individual components that let us make more complex, nuanced, and informative graphic through novel combinations. The grammar of graphics has seven components (the ones in bold are required explicitly ggplot2): Data The data that you’re feeding into a plot. Aesthetic mappings How are variables (columns) from your data connect to a visual dimension? Horizontal (x) positioning, vertical (y) positioning, size, color, shape, etc. These visual dimensions are called “aesthetics” Geometric objects What are the objects that are actually drawn on the plot? A point, a line, a bar, a histogram, a density, etc. Scales How is a variable mapped to its aesthetic? Will it be mapped linearly? On a log scale? Something else? This includes things like the color scale e.g., c(control, treatment_1, treatment_2) -&gt; c(“blue”, “green”, “red”) Statistical transformations Whether and how the data are combined/transformed before being plotted e.g., in a bar chart, data are transformed into their frequencies; in a box-plot, data are transformed to a five-number summary. Coordinate system This is a specification of how the position aesthetics (x and y) are depicted on the plot. For example, rectangular/Cartesian, or polar coordinates. Facet This is a specification of data variables that partition the data into smaller “sub plots”, or panels. 13.1 Example: Scatterplot grammar For example, consider the following plot from the gapminder data set. For now, don’t focus on the code, just the graph itself. ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(alpha = 0.1) + scale_x_continuous( name = &quot;GDP per capita&quot;, trans = &quot;log10&quot;, labels = scales::dollar_format() ) + theme_bw() + scale_y_continuous(&quot;Life expectancy&quot;) This scatterplot has the following components of the grammar of graphics. Grammar Component Specification data gapminder aesthetic mapping x: gdpPercap, y: lifeExp geometric object points scale x: log10, y: linear statistical transform none coordinate system rectangular faceting none Note that x and y aesthetics are required for scatterplots (or “point” geometric objects). Each geometric object has its own required set of aesthetics. 13.2 Activity: Bar chart grammar Consider the following plot. Don’t concern yourself with the code at this point. gapminder |&gt; filter(year == 2007) |&gt; mutate(continent = fct_infreq(continent)) |&gt; ggplot() + aes(x = continent, fill = continent) + geom_bar() + guides(fill = &quot;none&quot;) + theme_bw() Fill in the seven grammar components for this plot. Grammar Component Specification data gapminder aesthetic mapping FILL_THIS_IN geometric object FILL_THIS_IN scale FILL_THIS_IN statistical transform FILL_THIS_IN coordinate system FILL_THIS_IN faceting FILL_THIS_IN "],["working-with-ggplot2.html", "Part 14 Working with ggplot2", " Part 14 Working with ggplot2 First, the ggplot2 package comes with the tidyverse meta-package. You can just load tidyverse, and it will also load ggplot2. Let’s use the above scatterplot as an example to see how to use the ggplot() function. First, we will pass one argument to ggplot() function itself: - data: the data frame containing your plotting data ggplot(gapminder) After this, we add additional functions as layers to add features to the plot and control their appearance. The first thing we will add is aes(). The aes() function tells R to look in the data for variable names to map them to different aesthetic features, such as x-position, y-position, or color. ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) Next, we want to add some geometric shapes to the plot. These use functions with the form geom_SOMETHING(). Different plot shapes (geom_SOMETHING) accept different aes() arguments. These are listed in the help file for the geom function: ?geom_point, ?geom_line, etc. ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point() You can control aesthetics using constants or variables from outside data by specifying them outside of aes(). For example, to make all of the shapes of plot blue, you can add: color = \"blue\". ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(color = &quot;blue&quot;) There’s a bit of overplotting (overlapping symbols), so let’s also make the points semi-transparent. This is controlled using the alpha argument (you need 1/alpha points overlaid to achieve a solid point). ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(color = &quot;blue&quot;, alpha = .1) For now, that’s the only geom that we want to add. Now, let’s specify a scale transformation, because the plot would really benefit if the x-axis is on a logarithmic scale. These functions take the form scale_AESTHETIC_TYPE(). As usual, you can tweak this layer, too, using this function’s arguments. In this example, we’re re-naming the x-axis (the name argument), transform the values (the trans argument), and changing the labels to have a dollar format (the labels argument). ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(color = &quot;blue&quot;, alpha = .1) + scale_x_continuous( name = &quot;GDP per capita&quot;, trans = &quot;log10&quot;, labels = scales::dollar_format() ) I’m tired of seeing the ugly default gray background, so I’ll add a theme() layer. I like theme_bw() (you can tweak themes later, too!). Then, I’ll re-label the y-axis using the ylab() function. Et voilà! ggplot(gapminder) + aes(x = gdpPercap, y = lifeExp) + geom_point(color = &quot;blue&quot;, alpha = .1) + scale_x_continuous( name = &quot;GDP per capita&quot;, trans = &quot;log10&quot;, labels = scales::dollar_format() ) + theme_bw() + ylab(&quot;Life expectancy&quot;) "],["activity-build-some-plots.html", "Part 15 Activity: Build some plots! 15.1 Make a Line Chart 15.2 Make a Scatterplot 15.3 Assigning a ggplot2 object and adding to it 15.4 Fix Me!", " Part 15 Activity: Build some plots! 15.1 Make a Line Chart The following code makes a data frame called mauna that contains time series data of CO\\(_2\\) concentrations collected monthly at the Mauna Loa vocanic observation station. Execute this code to store the data in mauna: (mauna &lt;- tsibble::as_tsibble(co2) |&gt; rename(month = index, conc = value)) #&gt; # A tsibble: 468 x 2 [1M] #&gt; month conc #&gt; &lt;mth&gt; &lt;dbl&gt; #&gt; 1 1959 Jan 315. #&gt; 2 1959 Feb 316. #&gt; 3 1959 Mar 316. #&gt; 4 1959 Apr 318. #&gt; 5 1959 May 318. #&gt; 6 1959 Jun 318 #&gt; 7 1959 Jul 316. #&gt; 8 1959 Aug 315. #&gt; 9 1959 Sep 314. #&gt; 10 1959 Oct 313. #&gt; # … with 458 more rows Produce a line chart showing the concentration over time. Specifically, the plot should have the following grammar components: Grammar Component Specification data mauna aesthetic mapping x: month, y: conc geometric object lines scale yearmonth statistical transform none coordinate system rectangular faceting none Fill in the blanks to obtain the plot: ggplot(FILL_THIS_IN) + aes(FILL_THIS_IN, FILL_THIS_IN) FILL_THIS_IN() + tsibble::scale_x_yearmonth() 15.2 Make a Scatterplot Use the palmerpenguins::penguins data to make a scatterplot with the following specifications: Grammar Component Specification data palmerpenguins::penguins aesthetic mapping x: body_mass_g, y: bill_depth_mm geometric object points, smoothed lines scale linear statistical transform none coordinate system rectangular faceting none ggplot(FILL_THIS_IN) + aes(FILL_THIS_IN, FILL_THIS_IN) FILL_THIS_IN() + geom_smooth() You can control aesthetics for individual layers by adding an aes() inside the layer function, like this: geom_point(aes(color = COLOR_VARIABLE)) Modify your code above so that the points (but not the smooth line) have their color mapped to species: ggplot(FILL_THIS_IN) + aes(FILL_THIS_IN, FILL_THIS_IN) FILL_THIS_IN() + geom_smooth() Now, instead, map color in the global aes() call for the plot. What happens? ggplot(FILL_THIS_IN) + aes(FILL_THIS_IN, FILL_THIS_IN) FILL_THIS_IN() + geom_smooth() Things to keep in mind: Aesthetics mapped outside of a specific layer apply globally Aesthetics mapped inside a geom layer apply only to that layer. If the same aesthetic appears both globally and in a layer, the layer-specific one wins 15.3 Assigning a ggplot2 object and adding to it You can store the output of the plot in a variable. Assign the mauan plot above to a variable named p, then add a layer to p that adds a dark green smoothed line to the plot. FILL_THIS_IN p + FILL_THIS_IN() 15.4 Fix Me! What’s wrong with the following code? Fix it. ggplot(gapminder) + geom_point(x = gdpPercap, y = lifeExp, alpha = 0.1) What’s wrong with this code? Fix it. ggplot(cars) + geom_point(aes(x = speed, y = dist, color = &quot;blue&quot;)) "],["example-geoms-and-plots.html", "Part 16 Example geoms and Plots 16.1 Bar chart 16.2 Stacked bar chart 16.3 Pie chart 16.4 Histogram 16.5 Density 16.6 Dot plot 16.7 Scatterplots 16.8 More scatterplots 16.9 Building complex plots 16.10 Scatterplots for change 16.11 Comparing distributions 16.12 Scatterplot matrix 16.13 Smoothers and Exploratory Data Analysis", " Part 16 Example geoms and Plots 16.1 Bar chart theme_set(theme_classic()) ggplot(penguins) + aes(x = species) + geom_bar() 16.2 Stacked bar chart ggplot(penguins) + aes(y = island, color = fct_rev(species), fill = fct_rev(species), label = fct_rev(species)) + stat_count(orientation = &quot;y&quot;) + guides(color = guide_none(), fill = guide_none()) + ylab(NULL) + stat_count(geom = &quot;label&quot;, color = &quot;white&quot;) 16.3 Pie chart ggplot(penguins) + aes(x = factor(1), fill = species, label = species) + geom_bar(width = 1) + stat_count(geom = &quot;text&quot;, size = 5, color = &quot;white&quot;, position = position_stack(vjust = .5) ) + guides(y = guide_none(), x = guide_none(), fill = guide_none()) + xlab(NULL) + ylab(NULL) + coord_polar(theta = &quot;y&quot;) + theme(axis.text = element_blank(), axis.line = element_blank(), axis.ticks = element_blank()) 16.4 Histogram ggplot(penguins) + aes(x = bill_length_mm) + geom_histogram(binwidth = 1) #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). 16.5 Density ggplot(penguins) + aes(x = bill_length_mm) + geom_density() #&gt; Warning: Removed 2 rows containing non-finite values (stat_density). 16.6 Dot plot ggplot(penguins) + aes(x = bill_length_mm) + geom_dotplot(binwidth = 1, dotsize = .5) + guides(y = guide_none()) #&gt; Warning: Removed 2 rows containing non-finite values (stat_bindot). 16.7 Scatterplots # Scatterplot 1 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm) + geom_point() #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Scatterplot 2 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species, shape = species) + geom_point() #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Scatterplot 3 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species, shape = sex, size = body_mass_g) + geom_point() #&gt; Warning: Removed 11 rows containing missing values (geom_point). 16.8 More scatterplots # Scatterplot 4 ggplot(round(alr4::Heights)) + aes(x = mheight, y = dheight) + geom_point() # Scatterplot 5 ggplot(round(alr4::Heights)) + aes(x = mheight, y = dheight) + geom_jitter(height = .3, width = .3) 16.9 Building complex plots # Scatterplot 6 ggplot(penguins) + aes(x = species, y = flipper_length_mm, fill = species, color = species) + geom_point() #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Scatterplot 7 ggplot(penguins) + aes(x = species, y = flipper_length_mm, fill = species, color = species) + geom_jitter(height = 0, width = .4) #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Scatterplot 8 ggplot(penguins) + aes(x = species, y = flipper_length_mm, fill = species, color = species) + geom_jitter(height = 0, width = .4) + geom_boxplot(color = &quot;black&quot;, alpha = .5) #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Raincloud plot ggplot(na.omit(penguins)) + aes(y = species, x = flipper_length_mm, fill = species, color = species) + geom_jitter(height = .15) + geom_boxplot(color = &quot;black&quot;, alpha = .5, width = .1, size = .5) + ggdist::stat_slab(height = .3, color = &quot;black&quot;, size = .2, alpha = .5, position = position_nudge(y = .2)) 16.10 Scatterplots for change df &lt;- data.frame( id = 1:30, before = rnorm(30), after = rnorm(30)) df &lt;- tidyr::pivot_longer( df, -id, names_to = &quot;time&quot;, values_to = &quot;score&quot;) ggplot(df) + aes(x = time, y = score, group = id) + geom_point() + geom_line() 16.11 Comparing distributions df &lt;- data.frame( g = c(rep(&quot;a&quot;, times = 100), rep(&quot;b&quot;, times = 100), rep(&quot;c&quot;, times = 100), rep(&quot;d&quot;, times = 100), rep(&quot;e&quot;, times = 100)), z = c(rnorm(100, mean = 0, sd = 1), rnorm(100, mean = 1, sd = 2), rnorm(100, mean = 2, sd = 3), rnorm(100, mean = 3, sd = 4), rnorm(100, mean = 4, sd = 5)) ) # Overlapping densities ggplot(df) + aes(x = z, group = g, fill = g) + geom_density(size = .2, alpha = .5) # Ridge plot ggplot(df) + aes(x = z, y = g, fill = g) + ggridges::geom_density_ridges( size = .2, alpha = .5, scale = 4 ) #&gt; Picking joint bandwidth of 1.04 16.12 Scatterplot matrix penguins_focal &lt;- penguins[, c(&quot;species&quot;, &quot;bill_length_mm&quot;, &quot;flipper_length_mm&quot;, &quot;sex&quot;)] pairs(penguins_focal) GGally::ggpairs( penguins_focal, mapping = aes(color = species, alpha = .5), lower = list( continuous = &quot;smooth_loess&quot;, combo = &quot;facethist&quot;, discrete = &quot;facetbar&quot;, na = &quot;na&quot; ) ) + theme_classic() #&gt; Registered S3 method overwritten by &#39;GGally&#39;: #&gt; method from #&gt; +.gg ggplot2 #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). #&gt; Warning: Removed 2 rows containing non-finite values (stat_density). #&gt; Warning in ggally_statistic(data = data, mapping = mapping, na.rm = na.rm, : #&gt; Removed 2 rows containing missing values #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). #&gt; Warning: Removed 2 rows containing non-finite values (stat_density). #&gt; Warning: Removed 2 rows containing non-finite values (stat_boxplot). #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. #&gt; Warning: Removed 2 rows containing non-finite values (stat_bin). 16.13 Smoothers and Exploratory Data Analysis # Smoother 1 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth(color = &quot;black&quot;, fill = &quot;black&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 2 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth(color = &quot;black&quot;, fill = &quot;black&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;orange&quot;, fill = &quot;orange&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 3 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth() #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 4 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth() + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, linetype = &quot;dashed&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 5 ggplot(penguins) + aes(x = bill_length_mm, y = flipper_length_mm, fill = species, color = species) + geom_point() + geom_smooth(method = &quot;lm&quot;, linetype = &quot;dashed&quot;) + geom_smooth(color = &quot;black&quot;, fill = &quot;black&quot;, alpha = .2) + geom_smooth(method = &quot;lm&quot;, color = &quot;orange&quot;, fill = &quot;orange&quot;, alpha = .2) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). # Smoother 6 ggplot(penguins) + aes(x = flipper_length_mm, y = bill_length_mm, color = species, fill = species, shape = species, linetype = species) + geom_point(alpha = .7) + geom_smooth() + theme_bw() + theme(legend.position = &quot;bottom&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; #&gt; Warning: Removed 2 rows containing non-finite values (stat_smooth). #&gt; Warning: Removed 2 rows containing missing values (geom_point). "],["lab02.html", "Part 17 Lab 2: Global plastic waste 17.1 Learning goals 17.2 Getting started 17.3 Warm up 17.4 Exercises 17.5 Finishing Up", " Part 17 Lab 2: Global plastic waste Plastic pollution is a major and growing problem, negatively affecting oceans and wildlife health. Our World in Data has a lot of great data at various levels including globally, per country, and over time. For this lab we focus on data from 2010. Additionally, National Geographic ran a data visualization communication contest on plastic waste as seen here. 17.1 Learning goals Visualizing numerical and categorical data and interpreting visualizations Recreating visualizations Getting more practice using with R, RStudio, Git, and GitHub 17.2 Getting started Download this RMarkdown template for the lab. Save it to your class GitHub repo with a name like lab-02-plastic-waste.Rmd. In future labs, you will make your own RMarkdown documents from scratch. Download this dataset and save it to your class GitHub repo in a folder called data. 17.2.1 Packages We’ll use the tidyverse package for this analysis. Add code to load the tidyverse package to the setup chunk at the top of the document. 17.2.2 Data The following code will read in the data you saved to your repo. Add this to your document. plastic_waste &lt;- read_csv(here::here(&quot;data&quot;, &quot;plastic-waste.csv&quot;)) You can view this dataset using the dplyr::glimpse(), head(), and View(). Try these out in the Console. The variable descriptions are as follows: code: 3 Letter country code entity: Country name continent: Continent name year: Year gdp_per_cap: GDP per capita constant 2011 international $, rate plastic_waste_per_cap: Amount of plastic waste per capita in kg/day mismanaged_plastic_waste_per_cap: Amount of mismanaged plastic waste per capita in kg/day mismanaged_plastic_waste: Tonnes of mismanaged plastic waste coastal_pop: Number of individuals living on/near coast total_pop: Total population according to Gapminder 17.3 Warm up Notice that some cells in the data have the value NA — what does this mean? 17.4 Exercises Let’s start by taking a look at the distribution of plastic waste per capita in 2010. ggplot(data = plastic_waste) + aes(x = plastic_waste_per_cap) + geom_histogram(binwidth = 0.2) #&gt; Warning: Removed 51 rows containing non-finite values (stat_bin). One country stands out as an unusual observation at the top of the distribution. One way of identifying this country is to filter the data for countries where plastic waste per capita is greater than 3.5 kg/person. We will cover this function next week. For now, what do you think this code does? plastic_waste |&gt; filter(plastic_waste_per_cap &gt; 3.5) #&gt; # A tibble: 1 × 10 #&gt; code entity continent year gdp_per_cap plastic_waste_p… mismanaged_plast… #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 TTO Trinida… North Ame… 2010 31261. 3.6 0.19 #&gt; # … with 3 more variables: mismanaged_plastic_waste &lt;dbl&gt;, coastal_pop &lt;dbl&gt;, #&gt; # total_pop &lt;dbl&gt; Did you expect this result? You might consider doing some research on Trinidad and Tobago to see why plastic waste per capita is so high there, or whether this is a data error. 17.4.1 Exercise 1 Plot, using histograms, the distribution of plastic waste per capita faceted by continent. What can you say about how the continents compare to each other in terms of their plastic waste per capita? NOTE: From this point onwards, the plots and the output of the code are not displayed in the lab instructions, but you can and should the code and view the results yourself. Another way of visualizing numerical data is using density plots. Adapt your code above to use density plots instead of histograms. The y-axes for histograms and densities differ by default. Histograms have the raw counts. Densities have the density (think of it like proportion). If you want to put density plots and histograms on the same plot, we need to tell them to have the same y-axis. Plot histograms and densities on the same plot. In the geom_density() function, add aes(y = after_stat(count)) to tell it to put counts, not densities on the y-axis. Make just a density plot of plastic waste by continent Coloring the density curves by continent. The resulting plot may be a little difficult to read, so let’s also fill the curves in with colors as well. Make the fill color somewhat transparent to make the overlapping distributions easier to see. You may need to try several different transparency levels to find one that looks niece. Describe why we defined the color and fill of the curves by mapping aesthetics with aes() but defined the alpha level directly in the geom. 🧶 ✅ ⬆️ Now is a good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 17.4.2 Exercise 2 Yet another way to visualize differences in plastic waste distributions across continents is box plots. Make a plot with continent on the x-axis, plastic waste on the-axis, and fill of the box plots by continent. Adjust this plot so that it also shows individual data points. Add a density curve for each continent as well (i.e., make a “raincloud plot”). What does the density or data points show that the boxplot does not? 17.4.3 Exercise 3 Visualize the relationship between plastic waste per capita and mismanaged plastic waste per capita using a scatterplot. Describe the relationship. Color the points in the scatterplot by continent. Does there seem to be any clear distinctions between continents with respect to how plastic waste per capita and mismanaged plastic waste per capita are associated? Visualize the relationship between plastic waste per capita and total population; and between plastic waste per capita and coastal population. You will need to make two separate plots. Do either of these pairs of variables appear to be more strongly associated? Add trend lines (either loess smooths or linear trends) to these plots. 🧶 ✅ ⬆️ Now is another good time to knit your document and commit and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards. 17.4.4 Bonus Recreate the following plot, and interpret what you see in context of the data. Hint: The x-axis is a calculated variable. One country with plastic waste per capita over 3 kg/day has been filtered out. And the data are not only represented with points on the plot but also a smooth curve. The term “smooth” should help you pick which geom to use. 17.5 Finishing Up 🧶 ✅ ⬆️ Knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you’re happy with the final state of your work. Once you’re done, check to make sure your latest changes are on GitHub. "],["wrangle-yo-data-with-dplyr.html", "Part 18 Wrangle yo’ data with dplyr 18.1 Today’s Topics 18.2 Resources", " Part 18 Wrangle yo’ data with dplyr 18.1 Today’s Topics Today we’ll get started with learning to “wrangle” data— that is, to subset it, rearrange it, transform it, summarize it, and otherwise make it ready for analysis. We are going to be working with the dplyr package. Specifically, we’re going to consider three lessons today: Intro to dplyr syntax The |&gt; pipe and the dplyr advantage filter; relational/comparison and logical operators in R Specific dplyr functions we will cover select() arrange() filter() mutate() summarize() group_by() grouped mutate() grouped summarize() recode() across() rowwise() 18.2 Resources stat545: dplyr-intro stat545: dplyr-single r4ds: transform chapter. Here are some supplementary resources: A similar resource to the r4ds one above is the intro to dplyr vignette. Want to read more about piping? See r4ds: pipes. Some advanced topics you might find useful: For window functions and how dplyr handles them, see the window-functions vignette for the dplyr package. For time series data, see the tsibble demo "],["selecting-and-sorting-data-frames.html", "Part 19 Selecting and sorting data frames 19.1 Learning Objectives 19.2 Preamble 19.3 Demonstration", " Part 19 Selecting and sorting data frames 19.1 Learning Objectives Here are the concepts we’ll be exploring in this lesson: tidyverse dplyr functions: select() arrange() piping By the end of this lesson, students are expected to be able to: subset and rearrange data with dplyr use piping (|&gt;) when implementing function chains 19.2 Preamble Let’s talk about: The history of dplyr: plyr Don’t use both in one script! My recommendation, don’t use plyr at all at this point. tibbles are a special type of data frame The tidyverse Package functions and masking Load the tidyverse package: library(tidyverse) 19.3 Demonstration Let’s get started with an exercise: Download this Rmd worksheet. Save it to your class GitHub repo with a name like exercise-dplyr.Rmd. Open the worksheet in RStudio. Follow along in the .Rmd file until the Back to Guide section. "],["the-pipe-advantage.html", "Part 20 The pipe advantage 20.1 Learning Objectives 20.2 Compare nested functions to pipe chains 20.3 The workflow: 20.4 Basic principles: 20.5 Tangent: Base R workflow", " Part 20 The pipe advantage 20.1 Learning Objectives By the end of this lesson, you will: Have a sense of why dplyr is advantageous compared to the “base R” way with respect to good coding practice. Why? Having this in the back of your mind will help you identify qualities of and produce a readable analysis. 20.2 Compare nested functions to pipe chains Self-documenting code. This is where the tidyverse shines. Example of dplyr vs base R: gapminder[gapminder$country == &quot;Cambodia&quot;, c(&quot;year&quot;, &quot;lifeExp&quot;)] vs. gapminder |&gt; filter(country == &quot;Cambodia&quot;) |&gt; select(year, lifeExp) Morning Routine Pipie 20.3 The workflow: Wrangle your data with dplyr first Separate steps with |&gt; Pipe |&gt; your data into a plot/analysis 20.4 Basic principles: Do one thing at a time Transform variables OR select variables OR filter cases Chain multiple operations together using the pipe |&gt; Use readable object and variable names Subset a dataset (i.e., select variables) by name, not by “magic numbers” Note that you need to use the assignment operator &lt;- to store changes! 20.5 Tangent: Base R workflow We are jumping right into the tidyverse way of doing things in R, instead of the base R way of doing things. Our first week was about “just enough” base R to get you started. If you feel that you want more practice here, take a look at the R intro stat videos by MarinStatsLectures. "],["filtering-and-mutating-data-frames.html", "Part 21 Filtering and mutating data frames 21.1 Learning Objectives 21.2 R Operators 21.3 Demonstration", " Part 21 Filtering and mutating data frames 21.1 Learning Objectives Here are the concepts we’ll be exploring in this lesson: Relational/comparison operators Logical operators dplyr functions: filter mutate By the end of this lesson, you will be able to: Predict the output of R code containing the above operators. Explain the difference between &amp;/&amp;&amp; and |/||, and name a situation where one should be used over the other. Subsetting and transforming data using filter and mutate 21.2 R Operators Arithmetic operators allow us to carry out mathematical operations: Operator Description + Add - Subtract * Multiply / Divide ^ Exponent %/% Integer division %% Modulus (remainder from integer division) Relational operators allow us to compare values: Operator Description &lt; Less than &gt; Greater than &lt;= Less than or equal to &gt;= Greater than or equal to == Equal to != Not equal to Arithmetic and relational operators work on vectors. There is another very useful relational function, %in%: c(1, 2, 3, 4, 5) %in% c(1, 2) Logical operators allow us to carry out boolean operations: Operator Description ! Not | Or (element_wise) &amp; And (element-wise) || Or &amp;&amp; And The difference between | and || is that || evaluates only the first element of the two vectors, whereas | evaluates element-wise. 21.3 Demonstration Continue along with the worksheet until Back to Guide Again. "],["grouping-and-summarizing-data.html", "Part 22 Grouping and summarizing data 22.1 summarize() 22.2 group_by() 22.3 Grouped summarize() 22.4 Grouped mutate() 22.5 Function types", " Part 22 Grouping and summarizing data 22.1 summarize() Like mutate(), the summarize() function also creates new columns, but the calculations that make the new columns must reduce down to a single number. For example, let’s compute the mean and standard deviation of life expectancy in the gapminder data set: gapminder |&gt; summarize( mean = mean(lifeExp), sd = sd(lifeExp) ) Notice that all other columns were dropped. This is necessary, because there’s no obvious way to compress the other columns down to a single row. This is unlike mutate(), which keeps all columns, and more like transmute(), which drops all other columns. As it is, this is useful for creating summary tables, but it’s more useful in the context of grouping, coming up next. 22.2 group_by() The true power of dplyr lies in its ability to group a tibble, with the group_by() function. As usual, this function takes in a tibble and returns a (grouped) tibble. Let’s group the gapminder dataset by continent and year: gapminder |&gt; group_by(continent, year) The only thing different from a regular tibble is the indication of grouping variables above the tibble. This means that the tibble is recognized as having “chunks” defined by unique combinations of continent and year: Asia in 1952 is one chunk. Asia in 1957 is another chunk. Europe in 1952 is another chunk. etc… Notice that the data frame isn’t rearranged by chunk! The grouping is something stored internally about the grouped tibble. Now that the tibble is grouped, operations that you do on a grouped tibble will be done independently within each chunk, as if no other chunks exist. You can also create new variables and group by that variable simultaneously. Try splitting life expectancy by “small” and “large” using 60 as a threshold: gapminder |&gt; group_by(smallLifeExp = lifeExp &lt; 60) 22.3 Grouped summarize() Want to compute the mean and standard deviation for each year for every continent? No problem: gapminder |&gt; group_by(continent, year) |&gt; summarize(mu = mean(lifeExp), sigma = sd(lifeExp)) Notice: The grouping variables are kept in the tibble, because their values are unique within each chunk (by definition of the chunk!) With each call to summarize(), the grouping variables are “peeled back” from last grouping variable to first. This means the above tibble is now only grouped by continent. What happens when we reverse the grouping? gapminder |&gt; group_by(year, continent) |&gt; # Different order summarize(mu = mean(lifeExp), sigma = sd(lifeExp)) The grouping columns are switched, and now the tibble is grouped by year instead of continent. dplyr has a bunch of convenience functions that help us write code more eloquently. We could use group_by() and summarize() with length() to find the number of entries each country has: gapminder |&gt; group_by(country) |&gt; transmute(n = length(country)) Or, we can use the more elegant dplyr::n() to count the number of rows in each group: gapminder |&gt; group_by(country) |&gt; summarize(n = n()) Or better yet, if this is all we want, just use dplyr::count(): gapminder |&gt; count(country) 22.4 Grouped mutate() Want to get the increase in GDP per capita for each country? No problem: gap_inc &lt;- gapminder |&gt; arrange(year) |&gt; group_by(country) |&gt; mutate(gdpPercap_inc = gdpPercap - lag(gdpPercap)) print(gap_inc) The tibble is still grouped by country. Drop the NAs with another convenience function, this time supplied by the tidyr package (another tidyverse package that we’ll see soon): gap_inc |&gt; tidyr::drop_na() You can specify specific columns to drop NAs from in the drop_na() function. 22.5 Function types We’ve seen cases of transforming variables using mutate() and summarize(), both with and without group_by(). How can you know what combination to use? Here’s a summary based on one of three types of functions. Function type Explanation Examples In dplyr Vectorized functions These take a vector, and operate on each component independently to return a vector of the same length. In other words, they work element-wise. cos(), sin(), log(), exp(), round() mutate() Aggregate functions These take a vector, and return a vector of length 1 mean(), sd(), length() summarize(), esp with group_by(). Window Functions these take a vector, and return a vector of the same length that depends on the vector as a whole. lag(), rank(), cumsum() mutate(), esp with group_by() "],["advanced-dplyr-functions.html", "Part 23 Advanced dplyr functions 23.1 recode() 23.2 across() 23.3 Complex recoding plus across() 23.4 rowwise()", " Part 23 Advanced dplyr functions 23.1 recode() recode() is useful for recoding categorical variables. Unlike most of the other function in dplyr, recode() is backwards in it’s syntax: recode(.x, old = new) Lets take a look at recoding different variables using the psychTools::bfi dataset: In the dataset, our gender variable has values 1 and 2. This is a little vague since we don’t know what 1 or 2 is in respect to gender. dat_bfi &lt;- psychTools::bfi |&gt; rownames_to_column(var = &quot;.id&quot;) dat_bfi |&gt; mutate( gender = recode(gender, &quot;1&quot; = &quot;man&quot;, &quot;2&quot; = &quot;woman&quot;) ) |&gt; select(.id, gender, education) |&gt; head() Note that for numeric values on the left side of =, you need to wrap them in “quotes” or backticks; however, that’s not necessary for character values We can also specify a .default value within our recode(). For example, say we want to have just “HS or less” versus “more than HS” dat_bfi |&gt; mutate( education = recode(education, &quot;1&quot; = &quot;HS&quot;, &quot;2&quot; = &quot;HS&quot;, .default = &quot;More than HS&quot;) ) |&gt; select(.id, gender, education) |&gt; head() Another neat feature of the recode() function is the .missing value. If we would rather convert NA values to something more explicit, we can specify that in the .missing argument. dat_bfi |&gt; mutate( education = recode( education, &quot;1&quot; = &quot;HS&quot;, &quot;2&quot; = &quot;HS&quot;, .default = &quot;More than HS&quot;, .missing = &quot;(Unknown)&quot; ) ) |&gt; select(.id, gender, education) |&gt; head() Or we can use tidyr::replace_na() dat_bfi |&gt; mutate( education = replace_na(education, replace = &quot;(Unknown)&quot;) ) |&gt; select(.id, gender, education) |&gt; head() 23.2 across() The across function allows us to apply transformations across multiple columns Say we wanted to look at the mean of each agreeable variable between gender groups: dat_bfi |&gt; group_by(gender) |&gt; summarize( across( A1:A5, mean, na.rm = TRUE ) ) If we want to put the function name mean, togther with all of its arguments, we can write it as an anonymous function: dat_bfi |&gt; group_by(gender) |&gt; summarize( across( A1:A5, \\(x) mean(x, na.rm = TRUE) ) ) What if we wanted to include the standard deviation as well? We can pass a list of functions into across() dat_bfi |&gt; group_by(gender) |&gt; summarize( across( A1:A5, list( mean = \\(x) mean(x, na.rm = TRUE), sd = \\(x) sd(x, na.rm = TRUE) ) ) ) 23.3 Complex recoding plus across() Now sometimes with our scales we may encounter variables that are reverse scored. dat_bfi |&gt; mutate( A1r = recode( A1, &quot;6&quot; = 1, &quot;5&quot; = 2, &quot;4&quot; = 3, &quot;3&quot; = 4, &quot;2&quot; = 5, &quot;1&quot; = 6 ) ) |&gt; select(A1, A1r) |&gt; head() # or dat_bfi |&gt; mutate(A1r = max(A1, na.rm = TRUE) - A1 + min(A1, na.rm = TRUE)) |&gt; select(A1, A1r) |&gt; head() However, we can implement some more complex code that will reverse recode() in one fell swoop! We start with either specifying our columns that need reverse coding or get it from a data dictionary: reversed &lt;- c(&quot;A1&quot;, &quot;C4&quot;, &quot;C5&quot;, &quot;E1&quot;, &quot;E2&quot;, &quot;O2&quot;, &quot;O5&quot;) # or dict &lt;- psychTools::bfi.dictionary |&gt; as_tibble(rownames = &quot;item&quot;) reversed &lt;- dict |&gt; filter(Keying == -1) |&gt; pull(item) Putting it all together: dat_bfi |&gt; mutate(across( all_of(reversed), \\(x) recode(x, &quot;6&quot; = 1, &quot;5&quot; = 2, &quot;4&quot; = 3, &quot;3&quot; = 4, &quot;2&quot; = 5, &quot;1&quot; = 6), .names = &quot;{.col}r&quot; )) |&gt; head() The .names argument tells how to name the new columns. If you omit .names, the columns will be modified in place. In .names, the {.col} bit means “the column name”, and any text around that (here the letter r) is added to the name. 23.4 rowwise() rowwise() is a special group_by(). It tells R to treat each row of a data frame as its own group. rowwise() is useful for computing summary scores across items for each person. For example, to compute total scores for each person in the dat_bfi data: dat_bfi |&gt; rowwise() |&gt; mutate( .id = .id, A_total = mean(c_across(A1:A5), na.rm = TRUE), C_total = mean(c_across(C1:C5), na.rm = TRUE), E_total = mean(c_across(E1:E5), na.rm = TRUE), N_total = mean(c_across(N1:N5), na.rm = TRUE), O_total = mean(c_across(O1:O5), na.rm = TRUE), .before = everything() ) |&gt; head() The c_across() function combines c() and across() into one. It is like c() and creates a vector ala c(1, 3, 5, 7), but you can use the same options for selecting column names as select(). The .before argument says where to put the new columns you mutate(). everything() means “all the columns have I haven’t named yet”, so .before = everything() means put the new columns at the beginning of the data frame. "],["lab03.html", "Part 24 Lab 3: Explore gapminder with ggplot2 and dplyr 24.1 Getting started 24.2 Exercise 1: Basic dplyr 24.3 Exercise 2: Explore two variables with dplyr and ggplot2 24.4 Bonus Exercise: Recycling (Optional)", " Part 24 Lab 3: Explore gapminder with ggplot2 and dplyr In this lab, you will explore the gapminder dataset by making summary tables and visualizations. 24.1 Getting started Make a new RMarkdown script for this lab. In the setup chunk at the top of your scripts, load the packages needed for this lab. These include tidyverse and gapminder. 24.2 Exercise 1: Basic dplyr Use dplyr functions to achieve the following. For each exercise, print your result. 24.2.1 1.1 Use filter() to subset the gapminder data to three countries of your choice in the 1970’s. 24.2.2 1.2 Start with the original gapminder data and use the pipe operator |&gt; to first do the above filter and then select the “country” and “gdpPercap” variables. 24.2.3 1.3 Make a new variable in gapminder for the change in life expectancy from the previous measurement for that country. Filter this table to show all of the entries that have experienced a drop in life expectancy. Save this result as a new object. Hint: you might find the lag() or diff() functions useful. 24.2.4 1.4 Filter gapminder so that it shows the max GDP per capita experienced by each country. Hint: you might find the max() function useful here. 24.2.5 1.5 Produce a scatterplot of Canada’s life expectancy vs. GDP per capita using ggplot2. In your plot, put GDP per capita on a log scale. 24.3 Exercise 2: Explore two variables with dplyr and ggplot2 Use gapminder, palmerpenguins::penguins, or another dataset of your choice. (Check out a dataset from the datasets or psych R package if you want! 24.3.1 2.1 Pick two quantitative variables to explore. Make a summary table of descriptive statistics for these variables using summarize(). Include whatever staistics you feel appropriate (mean, median sd, range, etc.). Make a scatterplot of these variables using ggplot(). 24.3.2 2.2 Pick one categorical variable and one quantitative variable to explore. Make a summary table giving the sample size (hint: n()) and descriptive statistics for the quantitative variable by group. Make one or more useful plots to visualize these variables. Try to make a raincloud plot with points, boxplots, and densities for each group. 24.4 Bonus Exercise: Recycling (Optional) Evaluate this code and describe the result. The goal was to get the data for Rwanda and Afghanistan. Does this work? Why or why not? If not, what is the correct way to do this? filter(gapminder, country == c(&quot;Rwanda&quot;, &quot;Afghanistan&quot;)) "],["lab04.html", "Part 25 Lab 4: Personality and green reputation 25.1 Getting started", " Part 25 Lab 4: Personality and green reputation In this lab, You will analyze data looking at the relationship between green reputation and three personality traits– compassion, intellectual curiosity, and openness to experiences. The dataset includes data from students and non-students. 25.1 Getting started Make a new RMarkdown script for this lab. In the setup chunk at the top of your scripts, load the packages needed for this lab. Download the 2 data files for this lab and save them in the data folder of your GitHub folder. green_dictionary green_data Add a chunk with these lines chunk to your RMarkdown script to import the data. dictionary &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;green_dictionary.csv&quot;)) green_data &lt;- readr::read_csv(here::here(&quot;data&quot;, &quot;green_data.csv&quot;)) For your assignment, do the following. Inspect the item responses (e.g., with graphs or by summarizing distinct values). Is anything unusual? Compute total scores for the four scales. Recode variables as needed. Rescale the variables so that they go from 0-100 instead of the original range. Name the recaled variables *_pomp. Make plots that illustrate the distributions of the 4 POMP-scored variables. Make scatterplots showing the relationships between green reputation and each personality trait. Include trend lines for students and non-students. What do these plots show? Compare green reputation for students and non-students using a rainfall plot (bar + density + data points). Compute a summary table of means, SDs, medians, minima, and maxima for the four total scores for students and non-students. "],["tidy-data-and-pivoting.html", "Part 26 Tidy Data and Pivoting 26.1 Orientation", " Part 26 Tidy Data and Pivoting library(tidyverse) 26.1 Orientation 26.1.1 Today Today’s concept is tidy data and the tidyr package. Reshaping data by pivoting with tidyr::pivot_longer() and tidyr::pivot_wider(). 26.1.2 Resources For concepts of tidy data: Jenny Bryan’s intro to tidy data is short and sweet. the repo this links to has some useful exercises too, but uses the older spread() and gather() functions. tidyr vignette on tidy data. Hadley’s paper on tidy data provides a thorough investigation. For pivoting with tidyr, check out the pivot vignette. "],["tidy-data.html", "Part 27 Tidy data 27.1 Untidy Examples", " Part 27 Tidy data A data set is tidy if: Each row is an observation appropriate for the analysis; Each column is a variable; Each cell is a value. This means that each value belongs to exactly one variable and one observation. Why bother? Because doing computations with untidy data can be a nightmare. Computations become simple with tidy data. Whether or not a data set is “tidy” depends on the type of analysis you are doing or plot you are making. It depends on how you define your “observation” and “variables” for the current analysis. haireye &lt;- as_tibble(HairEyeColor) |&gt; count(Hair, Eye, wt = n) |&gt; rename(hair = Hair, eye = Eye) As an example, consider this example derived from the datasets::HairEyeColor dataset, containing the number of people having a certain hair and eye color. If one observation is identified by a hair-eye color combination, then the tidy dataset is: haireye |&gt; print() If one observation is identified by a single person, then the tidy dataset has one pair of values per person, and one row for each person. We can use the handy tidyr::uncount() function, the opposite of dplyr::count(): haireye |&gt; tidyr::uncount(n) |&gt; print() 27.1 Untidy Examples The following are examples of untidy data. They’re untidy for either of the cases considered above, but for discussion, let’s take a hair-eye color combination to be one observational unit. Note that untidy does not always mean “bad”, just inconvenient for the analysis you want to do. Untidy Example 1: The following table is untidy because there are multiple observations per row. It’s too wide. Imagine calculating the total number of people with each hair color. You can’t just group_by() and summarize(), here! This sort of table is common when presenting results. It’s easy for humans to read, but hard for computers to work with. Untidy data is usually that way because it was structured for human, not machine, reading. Untidy Example 2: The following table is untidy for the same reason as Example 1—multiple observations are contained per row. It’s too wide. Untidy Example 3: This is untidy because each observational unit is spread across multiple columns. It’s too long. In fact, we needed to add an identifier for each observation, otherwise we would have lost which row belongs to which observation! Does red hair ever occur with blue eyes? Can’t just filter(hair == \"red\", eye == \"blue\")! Untidy Example 4: Just when you thought a data set couldn’t get any longer! Now, each variable has its own row: hair color, eye color, and n. This is the sort of format that is common pulling data from the web or other “Big Data” sources. "],["pivoting-data.html", "Part 28 Pivoting data 28.1 Univariate pivoting", " Part 28 Pivoting data The task of making tidy data is about making data either longer, by stacking two or more rows, or wider, by putting one or more columns alongside each other based on groups. This is called pivoting. Usually the task of tidying data involves lengthening, and usually the task of widening is useful for turning data into something more friendly for human eyes. Sometimes, you will see data described as being in “long” or “wide” formats. Those terms aren’t that useful—“long” and “wide” are relative terms. The easiest and most powerful way to widen or lengthen data are with the functions tidyr::pivot_wider() and tidyr::pivot_longer(). History: R has seen many attempts at reshaping that have progressively gotten better. First came the reshape and reshape2 packages. Both were finicky. Used function names that I could never remember: melt() and cast(). Then, tidyr package replaced these. The tidyr::spread() and tidyr::gather() functions provided a simple interface. I still couldn’t remember these names. We will use tidyr::pivot_longer() and tidyr::pivot_wider(). The “pivot” functions also have similar names to the SQL “PIVOT” and “UNPIVOT” functions. 28.1 Univariate pivoting Let’s start with pivoting in the simplest case where only one variable is “out of place”. We’ll use the hair and eye color example from before, using the untidy data version from Example 1: haireye_untidy &lt;- haireye |&gt; mutate(eye = str_c(eye, &quot;_eyed&quot;)) |&gt; pivot_wider(id_cols = hair, names_from = eye, values_from = n) haireye_untidy The eye color variable is spread out across columns. To fix this, we need to convert the eye color columns to two columns: one column to hold the eye color (column names), one column to hold the values. Doing this, we obtain: Let’s dig into that! 28.1.1 pivot_longer() pivot_longer() takes a data frame, and returns a data frame. The main arguments after the data argument that we’ll need are: cols: the columns we want to pivot into a single column. Give the column names names_to: the old column names are going to be stored in a new column. What should this new column be named? values_to: the values in the old columns are going to stored in a new column. What should this new column be named? Possibly the trickiest bit is in identifying the column names. We could list all of them: haireye_untidy |&gt; pivot_longer(cols = c(Blue_eyed, Brown_eyed, Green_eyed, Hazel_eyed), names_to = &quot;eye&quot;, values_to = &quot;n&quot;) That can be a little tedious. We could identify a range. This is efficient, but not so robust if the data changes. haireye_untidy |&gt; pivot_longer(cols = Blue_eyed:Hazel_eyed, names_to = &quot;eye&quot;, values_to = &quot;n&quot;) Better is to use helper functions from the tidyselect package. In this case, we know the columns contain the text “eyed”, so let’s use tidyselect::contains(): haireye_untidy |&gt; pivot_longer(cols = contains(&quot;eyed&quot;), names_to = &quot;eye&quot;, values_to = &quot;n&quot;) Yet another way is to indicate everything except the hair column: haireye_untidy |&gt; pivot_longer(cols = -hair, names_to = &quot;eye&quot;, values_to = &quot;n&quot;) 28.1.2 pivot_wider() Let’s say we want go from a longer data frame to a shorter data frame. This is the opposite of what we did above. We might want to do this: To make a table for presentation With longitudinal or family data, to to go from multilevel models (which need longer data; each row is an observation) to SEM analyses (which need wider data; each row is an individual or family) For example, if we want to go from: To: We need to: Take the column eye and make each unique entry a new column Take the column n and make these values in the new eye columns. pivot_wider() is the reverse of pivot_longer(). Like pivot_longer(), pivot_wider() takes a data frame and returns a data frame. The main arguments after the data argument that we’ll need are: id_cols: The columns you would like to keep in place. By default, everything except the ones in names_from and values_from. For example, the identifier number for the observation. names_from: The new column names are coming from an old column. Which column is this? values_from: The column values are coming from an old column. Which column is this? haireye |&gt; pivot_wider(id_cols = hair, names_from = eye, values_from = n) "],["lab-5a-univariate-pivoting.html", "Part 29 Lab 5A: Univariate pivoting 29.1 Exercise 1: Univariate Pivoting", " Part 29 Lab 5A: Univariate pivoting Copy this code into your script to import the data for this lab. library(tidyverse) lotr &lt;- read_csv(&quot;https://raw.githubusercontent.com/jennybc/lotr-tidy/master/data/lotr_tidy.csv&quot;) |&gt; rename(Species = Race) 29.1 Exercise 1: Univariate Pivoting Consider the Lord of the Rings data: lotr Would you say this data is in tidy format? Widen the data so that we see the words spoken by each species, by putting species as its own column. (lotr_wide &lt;- lotr |&gt; pivot_wider(FILL_THIS_IN = c(Film, Gender), FILL_THIS_IN = Species, FILL_THIS_IN = Words)) Re-lengthen the wide LOTR data from Question 2 above. lotr_wide |&gt; pivot_longer(FILL_THIS_IN = FILL_THIS_IN, names_to = FILL_THIS_IN, values_to = FILL_THIS_IN) "],["multivariate-pivoting.html", "Part 30 Multivariate pivoting 30.1 Multiple variables in column names 30.2 Multiple variables in column names 30.3 pivot_wider()", " Part 30 Multivariate pivoting Now let’s consider the case when more than one variable are “out of place”. Perhaps there are multiple variables per row and/or multiple observations per row. 30.1 Multiple variables in column names Consider this subset of the who data: WHO &lt;- who |&gt; select(country:year, starts_with(&quot;new_&quot;)) |&gt; rename_with(~ stringr::str_replace(.x, &quot;f&quot;, &quot;f_&quot;), starts_with(&quot;new_&quot;)) |&gt; rename_with(~ stringr::str_replace(.x, &quot;m&quot;, &quot;m_&quot;), starts_with(&quot;new_&quot;)) knitr::kable(WHO, rownames = FALSE) country, iso2, iso3, and year are already variables, so they can be left as is. But the columns from new_sp_m_014 to new_ep_f_65 encode four variables in their names: The new prefix indicates these are counts of new cases (versus total cases). This dataset only contains new cases, so we’ll ignore it here because it’s constant. sp/rel/ep describe how the case was diagnosed. m/f gives the gender. 014/1524/2535/3544/4554/65 supplies the age range. We can break these variables up by specifying multiple column names in names_to, and then providing names_sep. WHO |&gt; pivot_longer( cols = new_sp_m_014:new_ep_f_65, names_to = c(&quot;diagnosis&quot;, &quot;gender&quot;, &quot;age&quot;), names_prefix = &quot;new_&quot;, names_sep = &quot;_&quot;, values_to = &quot;count&quot; ) In the names_to argument, we now tell it the names of the new columns that will store each part of the existing column names. We give the column names in order corresponding to how they appear in the existing column names. As with univariate pivoting, values_to gives the name of the new column that will store the cell values. 30.2 Multiple variables in column names Consider these family data. fam_dat &lt;- tribble( ~family, ~dob_child1, ~dob_child2, ~gender_child1, ~gender_child2, 1L, &quot;1998-11-26&quot;, &quot;2000-01-29&quot;, 1L, 2L, 2L, &quot;1996-06-22&quot;, NA, 2L, NA, 3L, &quot;2002-07-11&quot;, &quot;2004-04-05&quot;, 2L, 2L, 4L, &quot;2004-10-10&quot;, &quot;2009-08-27&quot;, 1L, 1L, 5L, &quot;2000-12-05&quot;, &quot;2005-02-28&quot;, 2L, 1L, ) fam_dat &lt;- fam_dat |&gt; mutate_at(vars(starts_with(&quot;dob&quot;)), parse_date) fam_dat In these data, we have two pieces of information (or values) for each child: their gender and their dob (date of birth). These need to go into separate columns in the result. Again we supply multiple variables to names_to, using names_sep to split up each variable name. The names_to vector gives the names of the new columns that will store each part of the existing column names. We give the column names in order corresponding to how they appear in the existing column names. Note the special name .value: .value takes the place of the values_to argument. It tells pivot_longer() to get the name of the column that will hold the cell values from that part of the existing column name. fam_dat |&gt; pivot_longer( cols = -family, names_to = c(&quot;.value&quot;, &quot;child&quot;), names_sep = &quot;_&quot;, values_drop_na = TRUE ) Let’s also clean up the child column: fam_dat_long &lt;- fam_dat |&gt; pivot_longer( cols = -family, names_to = c(&quot;.value&quot;, &quot;child&quot;), names_sep = &quot;_&quot;, values_drop_na = TRUE ) |&gt; mutate(child = stringr::str_replace(child, &quot;child&quot;, &quot;&quot;)) |&gt; mutate(child = as.integer(child)) 30.3 pivot_wider() You can also pivot_wider() while using multiple columns to supply variable names: id_cols: as usual. names_from: the new variable names are coming from old columns. Which old columns? names_sep: What character should you separate the entries of the old columns by? values_from: as usual. fam_dat_long |&gt; pivot_wider(id_cols = family, names_from = c(child, gender), names_prefix = &quot;child&quot;, names_sep = &quot;_gender&quot;, values_from = dob) Or using multiple columns to supply new values: If variables are spread out amongst rows and columns (for example, “sepal width” has “sepal” in a column, and “width” as a column name), here’s how we can use pivot_wider(): id_cols: as usual names_from: Which column contains the part of the variable? names_sep: As before, what character should you separate the entries of the old columns by? values_from: Which column names contain the other part of the variable? fam_dat_long |&gt; pivot_wider(id_cols = family, names_from = child, names_prefix = &quot;child&quot;, names_sep = &quot;_&quot;, values_from = c(dob, gender)) "],["lab-5b-multivariate-pivoting.html", "Part 31 Lab 5B: Multivariate pivoting", " Part 31 Lab 5B: Multivariate pivoting Copy this code into your script to import the data for this lab. library(tidyverse) set.seed(123) missing_w2_parent &lt;- sample(1:500, 30) missing_w2_child &lt;- c(missing_w2_parent[1:5], sample(1:500, 25)) family &lt;- read_csv( &quot;https://raw.githubusercontent.com/bwiernik/progdata/main/inst/tutorials/data/family_data.csv&quot; ) |&gt; mutate( across( starts_with(&quot;w2&quot;) &amp; contains(&quot;parent&quot;), ~ ifelse(family_id %in% missing_w2_parent, NA_real_, .x) ), across( starts_with(&quot;w2&quot;) &amp; contains(&quot;child&quot;), ~ ifelse(family_id %in% missing_w2_child, NA_real_, .x) ) ) You’re working on a longitudinal study of parent-child relationships. You have collected data from 500 families over 2 waves. In each wave, both the child and parent completed measures of communication behavior and relationship satisfaction. family |&gt; knitr::kable() Reshape the dataset to a “longer” format. Make each row 1 score Have columns for family_id, family_member, wave, scale, and score. family_longest &lt;- family |&gt; pivot_longer() print(family_longest) Reshape the dataset to a “longer” format. Make each row 1 person Have columns for family_id, family_member, wave, comm, and satis. family_long &lt;- family |&gt; pivot_longer() print(family_long) Some families are missing wave 2 data for parent, child, or both. Which families are missing wave 2 data for at least one person? Question: Is is easier to easier to find the missing data in the wide or long format? "],["tibble-joins-a-tale-of-two-tibbles.html", "Part 32 Tibble Joins: A Tale of Two Tibbles 32.1 Resources", " Part 32 Tibble Joins: A Tale of Two Tibbles Today’s topic is on operations with two or more tables. These operations are are used to combine two different data tables, compare two different data tables, verify or filter data in one data table against another, etc. 32.1 Resources Jenny Bryan’s join cheatsheet The dplyr “two-table verbs” vignette Relational Data chapter in “R for Data Science”. dplyr cheatsheet For an overview of operations involving multiple tibbles, check out Jenny Bryan’s Chapter 14 in stat545.com. For more activities, see the matrials from Rashedul Islam. "],["join-functions.html", "Part 33 Join Functions", " Part 33 Join Functions Often, we need to work with data living in more than one table. There are four main types of operations that can be done with two tables (as elaborated in r4ds Chapter 13 Introduction): Mutating joins add new columns to the “original” tibble. Filtering joins filter the “original” tibble’s rows. Set operations work as if each row is an element in a set. Binding stacks tables on top of or beside each other, with bind_rows() and bind_cols(). Let’s navigate to each of these three links, which lead to the relevant r4ds chapters, and go through the concepts there. These have excellent visuals to explain what’s going on. We will also look at the visuals of these concepts here. Then, let’s go through Jenny Bryans’s join cheatsheet for examples. "],["lab-06-join-those-tables.html", "Part 34 Lab 06: Join those tables! 34.1 Exercise 1: singer 34.2 Exercise 2: LOTR 34.3 Exercise 3: Set Operations", " Part 34 Lab 06: Join those tables! Load required packages: library(tidyverse) 34.1 Exercise 1: singer The package singer comes with two smallish data frames about songs. Let’s take a look at them (after minor modifications by renaming and shuffling): You can download the singer data from the class repo: songs &lt;- read_csv(&quot;https://raw.githubusercontent.com/bwiernik/progdata-class/master/data/singer/songs.csv&quot;) locations &lt;- read_csv(&quot;https://raw.githubusercontent.com/bwiernik/progdata-class/master/data/singer/loc.csv&quot;) (time &lt;- as_tibble(songs) |&gt; rename(song = title)) (album &lt;- as_tibble(locations) |&gt; select(title, everything()) |&gt; rename(album = release, song = title)) We really care about the songs in time. But, for which of those songs do we know its corresponding album? time |&gt; FILL_THIS_IN(album, by = FILL_THIS_IN) Go ahead and add the corresponding albums to the time tibble, being sure to preserve rows even if album info is not readily available. time |&gt; FILL_THIS_IN(album, by = FILL_THIS_IN) Which songs do we have “year”, but not album info? time |&gt; FILL_THIS_IN(album, by = &quot;song&quot;) Which artists are in time, but not in album? time |&gt; anti_join(album, by = &quot;FILL_THIS_IN&quot;) You’ve come across these two tibbles, and just wish all the info was available in one tibble. What would you do? FILL_THIS_IN |&gt; FILL_THIS_IN(FILL_THIS_IN, by = &quot;song&quot;) 34.2 Exercise 2: LOTR Load in three tibbles of data on the Lord of the Rings: fell &lt;- read_csv(&quot;https://raw.githubusercontent.com/jennybc/lotr-tidy/master/data/The_Fellowship_Of_The_Ring.csv&quot;) ttow &lt;- read_csv(&quot;https://raw.githubusercontent.com/jennybc/lotr-tidy/master/data/The_Two_Towers.csv&quot;) retk &lt;- read_csv(&quot;https://raw.githubusercontent.com/jennybc/lotr-tidy/master/data/The_Return_Of_The_King.csv&quot;) Stack these into a single tibble. FILL_THIS_IN(fell, FILL_THIS_IN) Which races are present in “The Fellowship of the Ring” (fell), but not in any of the other ones? fell |&gt; FILL_THIS_IN(FILL_THIS_IN, by = &quot;Race&quot;) |&gt; FILL_THIS_IN(FILL_THIS_IN, by = &quot;Race&quot;) 34.3 Exercise 3: Set Operations Let’s use three set functions: intersect, union and setdiff. We’ll work with two toy tibbles named y and z, similar to Data Wrangling Cheatsheet (y &lt;- tibble(x1 = LETTERS[1:3], x2 = 1:3)) (z &lt;- tibble(x1 = c(&quot;B&quot;, &quot;C&quot;, &quot;D&quot;), x2 = 2:4)) Rows that appear in both y and z FILL_THIS_IN(y, z) You collected the data in y on Day 1, and z in Day 2. Make a data set to reflect that. FILL_THIS_IN( mutate(y, day = &quot;Day 1&quot;), mutate(z, day = &quot;Day 2&quot;) ) The rows contained in z are bad! Remove those rows from y. FILL_THIS_IN(FILL_THIS_IN, FILL_THIS_IN) "],["file-inputoutput-io.html", "Part 35 File input/output (I/O) 35.1 Resources", " Part 35 File input/output (I/O) Today’s class is all about reading into R and writing out data files out of R. Because sometimes you want to work with your own damn data, instead of data about penguins, cars, life expectancy, or the Big Five personality scores of 2800 of your closest friends. 35.1 Resources 35.1.1 References and tutorials Jenny Bryan’s notes on file I/O Tutorial on importing Excel files Tutorial on relative paths and how RStudio treats .R script files and .Rmd files 35.1.2 Package documentation readr::read_csv() readxl::read_excel() googlesheets4::read_sheet() haven package for importing SPSS, SAS, Stata files rio::import() for importing all manner of formats with one command! here::here() "],["filesystems-and-paths.html", "Part 36 Filesystems and paths", " Part 36 Filesystems and paths Let’s first load the built-in gapminder dataset and the tidyverse: library(gapminder) library(tidyverse) Next, let’s filter the data to only 2007 and only the Asia continent and save it to a new object. (gap_asia_2007 &lt;- gapminder |&gt; filter(year == 2007, continent == &quot;Asia&quot;)) We can write this to a comma-separated value (csv) file with the command: write_csv(gap_asia_2007, &quot;exported_file.csv&quot;) But where did this file go? We should save the file in a sensible location. We need to practice controlling where R is running, where it looks for files, and where it writes out your files. To that end, let’s review the working directory and RStudio Projects. "],["the-working-directory-and-rstudio-projects.html", "Part 37 The working directory and RStudio Projects", " Part 37 The working directory and RStudio Projects When you open R, it “runs” in some folder on your computer. This is the place it will look for files to import and write files as output. Think about where your class files end up when you knit them. If you have R/RStudio closed, and you open a .R or .Rmd file, R/RStudio will start in the folder holding that file. If you open R/RStudio from the Windows Start menu, the Mac dock, the Mac Spotlight, etc., R/Studio will start in its default location (probably your user home directory, see Tools → Global Options → General → Default working directory…). Write all of your R scripts assuming: You are running them by opening a fresh new R session Don’t use rm(list = ls()) to clean the workspace–the workspace is already clean You need to load required pacakges with library() Don’t work on several different projects in one R session at the same time! You have all of the necessary packages installed Don’t include install.packages() calls unless they are commented out or otherwise set not to run The script will run without human input You need to import or load any data you are working with Load data and write output using R commands, not file.choose(), read.clipboard(), the buttons in RStudio, etc. All of the needed files live in your project folder Write relative paths, rather than absolute paths file.path(\"data\", \"cats-data_2020-03-04.xlsx\") Not: /Users/brenton/Research/cats_project/data/cats-data_2020-03-04.xlsx or C:\\\\Users\\\\brenton\\\\Research\\\\cats_project\\\\data\\\\cats-data_2020-03-04.xlsx orfile.path(\"C:\", \"Users\", \"brenton\", \"Research\", \"cats_project\", \"data\", \"cats-data_2020-03-04.xlsx\") Your script might run on any system Write the paths to files using file.path() or here::here(), rather than typing out the whole path This is because Windows and Mac/Linux have different syntaxes for file paths: Mac/Linux: path/to/folder Windows: path\\\\to\\\\folder This approach has several advantages: Frictionless running on different computers Less breakage (e.g., if you move a folder around, the relative paths will still work) Less surprising or weird behaviors due to session crud Easy to tweak data/code and update results "],["rstudio-projects-1.html", "Part 38 RStudio projects", " Part 38 RStudio projects RStudio projects can help with following these best practices. The best way to work with R is to have a separate Project for each project/paper you are working on. Switch RStudio to that Project whenever you are working on those files. You can you are working in a Project because its name will be in the RStudio title bar. One macOS, the Project name will also show on the Dock icon. "],["herehere.html", "Part 39 here::here()", " Part 39 here::here() The here package is the best way to refer to files. The major function is here::here(). Like file.path(), here::here() lets you specify a path to a file and then adds the system-appropriate separators (/ or \\\\). Where here::here() shines is that it figures out where the relative paths should start from. It looks around in the folders in your directory and finds the .Rproj file, then constructs the relative file paths from there. The .Rproj file is a sign post that says “START HERE”. For example, create a new folder “data” in your participation folder. Then, save gap_asia_2007 using the here::here() and write_csv() functions: write_csv(gap_asia_2007, here::here(&quot;participation&quot;, &quot;data&quot;, &quot;gap_asia_2007.csv&quot;) ) More details on here are available in this short article. "],["reading-data-from-disk.html", "Part 40 Reading data from disk", " Part 40 Reading data from disk The same csv file that we just saved to disk can be imported into R again by specifying the path where it exists: dat &lt;- read_csv(here::here(&quot;participation&quot;, &quot;data&quot;, &quot;gap_asia_2007.csv&quot;)) dat Notice that the output of the imported file is the same as the original tibble. read_csv() was intelligent enough to detect the types of the columns. This won’t always be true so it’s worth checking! In particular, be on the lookout for any columns it imports as col_character()! The read_csv() function has many additional options including the ability to specify column types (e.g., is “1990” a year or a number?), skip columns, skip rows, rename columns on import, trim whitespace, and more. To control the column types, use the cols() function: dat &lt;- read_csv( here::here(&quot;participation&quot;, &quot;data&quot;, &quot;gap_asia_2007.csv&quot;), col_types = cols( country = col_factor(), continent = col_factor(), year = col_date(format = &quot;%Y&quot;), .default = col_double() # all other columns as numeric (double) ) ) dat By default, it leaves all columns as col_guess(), but it’s better to be explicit. Another important option to set is the na argument, which specifies what values to treat as NA on import. By default, read_csv() treats blank cells (i.e., \"\") and cells with \"NA\" as missing. You might need to change this (e.g., if missing values are entered as -999). Note that readxl::read_excel() by default only has na = c(\"\") (no \"NA\")! dat &lt;- read_csv( here::here(&quot;participation&quot;, &quot;data&quot;, &quot;gap_asia_2007.csv&quot;), col_types = cols( country = col_factor(), continent = col_factor(), year = col_date(format = &quot;%Y&quot;), .default = col_double() # all other columns as numeric (double) ), na = c(&quot;&quot;, &quot;NA&quot;, -99, &quot;No response&quot;) ) dat "],["import-a-file-from-the-webcloud.html", "Part 41 Import a file from the web/cloud 41.1 Import a CSV file from the internet 41.2 Import an Excel file (.xls or .xlsx) from the internet", " Part 41 Import a file from the web/cloud 41.1 Import a CSV file from the internet To import a CSV file directly from the web, assign the URL to a variable url &lt;- &quot;http://gattonweb.uky.edu/sheather/book/docs/datasets/magazines.csv&quot; and then apply read_csv file to the url. dat &lt;- read_csv(url) You can do this in one step if you like: read_csv(&quot;http://gattonweb.uky.edu/sheather/book/docs/datasets/magazines.csv&quot;) 41.2 Import an Excel file (.xls or .xlsx) from the internet First, we’ll need the package to load in Excel files: library(readxl) Datafiles from this tutorial were obtained from: https://beanumber.github.io/sds192/lab-import.html#data_from_an_excel_file Unlike with a CSV file, to import an .xls or .xlsx file from the internet, you first need to download it locally. Note: The folder you want to save the file to has to exist!. If it doesn’t, you will get an error. You can create the folder path in one of three ways: Create them directly in Finder/Windows Explorer Use the buttons in the Files tab in RStudio Use the dir.create() function: if ( !dir.exists( here::here(&quot;participation&quot;, &quot;data&quot;) ) ) { dir.create( here::here(&quot;participation&quot;, &quot;data&quot;), recursive = TRUE ) } Next, you download the file. To download it, create a new object called xls_url and then use download.file() to download it to a specified path. xls_url &lt;- &quot;http://gattonweb.uky.edu/sheather/book/docs/datasets/GreatestGivers.xls&quot; download.file( xls_url, here::here(&quot;participation&quot;, &quot;data&quot;, &quot;some_file.xls&quot;), mode = &quot;wb&quot; ) NOTE: Don’t assign the result of download.file(). NOTE: The mode = \"wb\" argument at the end is really important if you are on Windows. If you omit, you will probably get a message about downloading a corrupt file. More details about this behavior can be found here. Naming a file “some_file” is extremely bad practice (hard to keep track of the files). You should always give it a more descriptive name. It’s also a good idea to avoid spaces in filenames. You should come up with a system for naming your files and use it consistently. My file names look like this: progdata_example-dataset_2021-03-02_bmw.csv Often, it’s a good idea to name the file similarly (or the same as) the original file (sometimes that might not be a good idea if the original name is non-descriptive). There’s handy trick to extract the filename from the URL: file_name &lt;- basename(xls_url) download.file( xls_url, here::here(&quot;participation&quot;, &quot;data&quot;, file_name), mode = &quot;wb&quot; ) Now we can import the file: dat &lt;- read_excel( here::here(&quot;participation&quot;, &quot;data&quot;, file_name) ) "],["read-in-a-sample-spss-file..html", "Part 42 Read in a sample SPSS file.", " Part 42 Read in a sample SPSS file. Let’s load a sample SPSS file and work with it. Download the file from here and save it in your data folder in your class GitHub repo. These data are a random subset of the data used in this paper. This was a study looking at personality traits that distinguish C-level executives from lower-level managers among men and women. The subset of data here consists of 200 cases, with variables indicating: The language of assessment (English, Dutch, or French) Gender C-level or not Extraversion level, as well as 4 facet traits (Leading, Communion, Persuasive, Motivating) Let’s load in the data using the haven package. (clevel &lt;- haven::read_spss( here::here(&quot;participation&quot;, &quot;data&quot;, &quot;clevel.sav&quot;) ) ) Notice that this tibble looks a little different for the language and gender variables than normal. It has labels for the numeric values. This format is what SPSS uses, but it’s not standard for R. Let’s convert those variables, and isClevel as well, to factors: clevel_cleaned &lt;- clevel |&gt; mutate(language = as_factor(language), gender = as_factor(gender), isClevel = factor(isClevel, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;)) ) print(clevel_cleaned) Notice how the variables are now factors with labels as the entries, instead of the original code numbers. "],["saving-files.html", "Part 43 Saving files 43.1 Saving data frames 43.2 Saving plots", " Part 43 Saving files 43.1 Saving data frames Let’s save this file as a CSV file so that it’s a smaller file and easier to import again in the future. write_csv( clevel_cleaned, here::here(&quot;participation&quot;, &quot;data&quot;, &quot;clevel_cleaned.csv&quot;) ) 43.2 Saving plots Now let’s make a plot. clevel_plot &lt;- clevel_cleaned |&gt; mutate(isClevel = recode(isClevel, No = &quot;Below C-level&quot;, Yes = &quot;C-level&quot;), gender = recode(gender, Female = &quot;Women&quot;, Male = &quot;Men&quot;)) |&gt; ggplot(aes(paste(isClevel, gender, sep = &quot;\\n&quot;), Extraversion, color = gender)) + geom_boxplot() + geom_jitter(height = .2) + scale_color_manual(values = c(&quot;#1b9e77&quot;, &quot;#7570b3&quot;)) + ggtitle(&quot;Extraversion Stan Scores&quot;) + scale_y_continuous(breaks = 1:9) + ggthemes::theme_fivethirtyeight() clevel_plot Let’s save the plot in several formats. This is useful if we want to use the plot outside of Markdown. Save plots using the ggsave() funnction. ggsave() has many options. See the help function ?ggsave for full details. The main arguments are: filename plot width and height (inches by default) dpi (dots per inch; for bitmap formats). ggsave() will try to guess what format you want based on the file name. If you want, you can specify a specific format or R graphics device to save with using the device argument. dir.create(here::here(&quot;participation&quot;, &quot;output&quot;, &quot;figures&quot;), recursive = TRUE) ggsave(here::here(&quot;participation&quot;, &quot;output&quot;, &quot;figures&quot;, &quot;clevel_extraversion.svg&quot;), clevel_plot, height = 6, width = 6 ) You can save to several formats. Generally, work with a vector format like .svg, .eps, or .pdf. Vector graphics represent the image as a series of data points and equations. This means that they can be made smaller or larger or zoomed in on without damaging the image quality. If you can’t use a vector format for some reason, you can also export to a bitmap format. Bitmap graphs represent the image as colored dots or pixels. This means that the image quality will suffer if you make the image larger or zoom in on it (making it smaller can also sometimes compromise quality). With bitmap images, you need to be concerned with resolution (how many pixels/dots per inch when printed). Always use at least 300 DPI resolution. There are several popular bitmap image formats. .tiff/.tif Highest quality, but also the largest. Use it for print graphics, but you should probably avoid it for images to be hosted on the web. .png A bit smaller, and it should be your go to for charts, figures, line drawings, etc. More complex images (e.g., photos) can get pretty big with .png, though. .jpg/.jpeg Probably the most popular bitmap format. Works well for photographs hosted on the web, but its compression often makes line drawing and charts look terrible. .jpg/.jpeg also degrades in quality each time you edit/save it, so don’t use it for images you intend to edit. .gif Generally avoid .gif unless you are making an animation or you need very small file size for a simple image. .gif supports very few colors, so always check your image quality after making a .gif. Let’s save to some other formats: ggsave(here::here(&quot;participation&quot;, &quot;output&quot;, &quot;figures&quot;, &quot;clevel_extraversion.pdf&quot;), clevel_plot, height = 6, width = 4) ggsave(here::here(&quot;participation&quot;, &quot;output&quot;, &quot;figures&quot;, &quot;clevel_extraversion.tiff&quot;), clevel_plot) ggsave(here::here(&quot;participation&quot;, &quot;output&quot;, &quot;figures&quot;, &quot;clevel_extraversion.png&quot;), clevel_plot) "],["organizing-your-project-folders.html", "Part 44 Organizing Your Project Folders", " Part 44 Organizing Your Project Folders Follow a consistent folder structure for all of your projects. This will make it easier for you to say organized and make your code, data, and projects easy to share. In your project’s root folder, you should have a README.md file and a .Rproj project. Then, you should have folders that separate different types of files: data: Stores all of your data files for a project Have subfolders for different dates, waves, groups, etc. as needed If you plan on saving cleaned data, having separate data_raw and data folders is a good idea. output: Stores any output your scripts generate Depending on how many figures and other output files you will create, you might want to split/subfolder this into figures, reports, etc. You might add additional folders, such as: tests: A folder that includes tests to check that your scripts or results are accurate. Check out the testthat package. templates: A folder to hold template files (e.g., RMarkdown templates, Word templates, CSS files for HTML output, TeX templates for PDF output) admin: For administrative documents (e.g., IRB approval, grant information) doc: For documentation (e.g., variable codebooks, style guides) R or src or scripts: Folders to store functions and scripts that you call from your markdown (e.g., a data import and cleaning script) Use folders or subfolders for each language if you are programming in multiple languages: R, python, sql, C, etc. Can put them all in a generic scripts or src folder if not too many markdown: If you have multiple RMarkdown documents for your project, consider putting them into a subfolder "],["bonus-activity-rio.html", "Part 45 Bonus Activity: rio", " Part 45 Bonus Activity: rio Do the clevel activity again, but this time use the rio::import() and rio::export() functions instead of the read_*() and write_*() functions. In rio::import(), be sure to specify: rio::import(..., setclass = \"tibble\"). "],["entering-data-manually.html", "Part 46 Entering Data Manually 46.1 Making data.frames or tibbles 46.2 datapasta", " Part 46 Entering Data Manually 46.1 Making data.frames or tibbles In base R, we can make data frames using the data.frame() function. The tidyverse version is tibble::tibble(). tibble() is stricter by not allowing recycling unless the vector is of length 1. Good: dat &lt;- tibble( x = 1:6, y = min(x) ) Bad: dat &lt;- tibble( x = 1:6, y = 1:2 ) Manual construction of tibbles is convenient with tibble::tribble(): dat &lt;- tribble( ~Day, ~Breakfast, 1, &quot;Apple&quot;, 2, &quot;Yogurt&quot;, 3, &quot;Yogurt&quot; ) 46.2 datapasta The datapasta package helps to reproducibly copy-paste data from spreadsheets into R. datapasta uses tribble(). install.packages(&quot;datapasta&quot;) After you install the package, you can set keyboard shortcuts in RStudio for its functions: Click Tools → Modify Keyboard Shortcuts… Search for ‘paste’. Set keyboard shortcuts for “Paste as tribble”, “Paste as vector”, and other functions if you like. Now, try out pasting a vector and a tibble. "],["lab-07-make-a-portfolio-piece.html", "Part 47 Lab 07: Make a portfolio piece 47.1 Step 1: Make a new GitHub repo 47.2 Step 2: Set up your folder structure 47.3 Step 3: Find, download, and document a dataset 47.4 Step 4: Practice input, process, output 47.5 Step 5: Finish your data exploration report", " Part 47 Lab 07: Make a portfolio piece For this week’s activities, make a portfolio piece for class. Do the following: Make a new GitHub repo for your project and clone it to your computer. Make an appropriate file structure for your portfolio project, including, e.g., An RStudio project file. A data folder A figures folder Other folders as appropriate. Choose a dataset of interest to you 47.1 Step 1: Make a new GitHub repo Make a new GitHub repo for your project. It’s a good habit to make a new repo for each different project/paper you work on. Add a link to this new project repo to you main class repo README. Clone your new project repo down to your computer. 47.2 Step 2: Set up your folder structure In your new repo, prepare your folder structure. Folders and files you should probably include in this folder structure are: A README.md file describing the project and the contents of the subfolders An RStudio Project .Rproj file in the folder root A data folder that holds the relevant data files for the project Could also have separate data_raw and data folders One or more RMarkdown manuscripts An output folder that will hold your output files Depending on how many figures and other output files you may have, you might want to split/subfolder this into figures, reports, etc. Other subfolders as needed, e.g.: admin for adminstrative documents (e.g., IRB approval, grant information) doc for documentation (e.g., variable codebooks), R as a place to store functions and scripts that you call from your markdown (e.g., a data import and cleaning script) All of these folders should be described in your README.md file for this project (e.g., make a markdown table describing the folders). 47.3 Step 3: Find, download, and document a dataset In this step, you need to identify a dataset you want to work with for your final project. Some potential sources for datasets include: ICPSR Google Datasets Harvard Dataverse FiveThirtyEight’s GitHub repos Data from your own research work or experience The requirements for this dataset: It cannot be already available as an R package like gapminder, nycflights13, or one of the datasets in datasets It’s okay if there is any R package you use to download the data, such as rtweet, so long as it isn’t already a neatly packaged dataset for you. It should be a dataset that needs some cleaning, reshaping, wrangling before it’s ready for analysis. This is the case for almost any real-world data. Talk to me if you are having trouble finding an interesting or suitable dataset! Once you have identified a dataset, download it and place it in your data folder. Bonus: If this is data you are downloading from the internet, write and run a .R script to download the dataset Hint: Try to write this as a separate .R file from your homework .Rmd file and call it in your .Rmd using source(). Your data file should have a useful descriptive name (no data.xlsx!). It should include: A description of the file (e.g., “gapminder-data-all-countries”) A date in a sortable format (e.g., “2020-03-04”) No spaces The file extension (e.g., “.csv” or “.xlsx”) Document your data. Describe the key variables, their types (e.g., character, integrer, numeric), their possible values (e.g., for a personality item, maybe integers from 1-5), etc. Describe how missing data is indicated (blank cells, “NA”, “-999”, etc.). This documentation could be in the main project README.md file or in a README.md or other .md file in the data folder. 47.4 Step 4: Practice input, process, output In your .Rmd file for this homework assignment (call it hw04.Rmd), practice reading your data file into R, processing the imported data, and outputting files. Import your file using R functions (not the buttons in RStudio). You will want to output two things: A data file, such as cleaned dataset and/or a summarized dataset/table A plot, in several formats, including (1) a bitmap format, (2) a vector format, and (3) PDF Save your data file and plots to your output folder (or other more specific folders if that is your structure). Be sure to use descriptive file names and document what these files are with a .md file. Save your data file using write_csv() or a similar function. Save your images using ggsave(). Do not use the buttons in RStudio. 47.5 Step 5: Finish your data exploration report Finish writing your RMarkdown document to describe the data, conduct your analyses, and describe your interpretations or conclusions. library(tidyverse) library(tidytext) library(textdata) knitr::opts_chunk$set(echo = TRUE, include = TRUE, fig.width = 5, fig.height = 4, fig.align = &quot;center&quot;) "],["text-data.html", "Part 48 Text Data 48.1 Wrangling 48.2 Natural Language Processing (NLP) - Terminology 48.3 The mighty unnest_tokens() function 48.4 Tidy data text principles 48.5 Clean your text! Grammar vs. Computers 48.6 Capitalization 48.7 Stemming 48.8 Regular Expressions (Regex) for URLS 48.9 Counting words 48.10 Can’t stop (words) won’t stop (words) 48.11 Lexicon-based approach", " Part 48 Text Data 48.1 Wrangling We’ve been working primarily with numerical-based data. However, we might also be interested in text-based data. This field of study is called Natural Language Processing (NLP). Keeping in scope with the class being “How do we wrangle/program with our data?” we will go over how to wrangle and shape text-based data as well as some metrics to measure important words. The primary package we will be using to wrangling is the tidytext package. Another useful package for text (or string) manipulation is the stringr package. 48.2 Natural Language Processing (NLP) - Terminology In your journey of NLP, you might stumble across different terms used and it’s important to get familiar with them so we can start formulating our ideas and expressing our questions with the terminology. Corpus - A broad collection of documents Plural: Corpora Document - A collection of terms Terms - Words Tokens - The unit of the analysis (words, characters, sentence, paragraph, n-grams, etc.) Changes meaning based off what we are wanting to analyze We tokenize our data; the process of splitting text into tokens Read more about other key terms here. 48.3 The mighty unnest_tokens() function Lets look at an example of some text: text &lt;- c(&quot;Today and only today is a lovely day to learn about text data and NLP&quot;, &quot;I hope that one day I will be able to apply all of what I learn because learning if cool and fun&quot;, &quot;If I were to express my feelings in two words: excitement and anxious&quot;) text Now let’s put it into a data frame structure: text_df &lt;- tibble( line = 1:3, text = text ) text_df 48.4 Tidy data text principles The “tidy data” principles apply to text data as well! We want our data to have one-token-per-document-per-row. In our case, tokens are individual words. The main three arguments of unnest_tokens() look like this: unnest_tokens(data_frame, new_name_of_column, column_name_in_data_frame) or data_frame |&gt; unnest_tokens(new_name_of_column, column_name_in_data_frame) Let’s take a look at what that looks like: unnest_df &lt;- text_df |&gt; unnest_tokens(word, text) unnest_df I’m sure you’ve noted a few things about our text from using unnest_tokens() Our words are split into a word-per-row We have a way of identifying from which document (line) our words come from Our words are…lower case? 48.5 Clean your text! Grammar vs. Computers In English (and broadly all language), we have grammar. Computers don’t. This has big implications. We need to make sure that we capture our words (or tokens) as we understand them. Computers don’t know that “Capital Word” is the same thing as “capital word”. They also don’t know that “running” is the same idea as “run”. I’m sure you already have thought of practical solutions to some of these problems and guess what? We do exactly that. 48.6 Capitalization This one’s pretty easy: we make all our text the same format. Usually lowercase is the format of choice. I like lowercase because then I’m not being yelled at. unnest_tokens() automatically makes words lowercase. We can change that by specifying to_lower = FALSE. (In the case we are working with URLs.) 48.7 Stemming For words that have the same “stem” (or root) we can remove the end. Example: These words: Consult Consultant Consultants Consulting Consultative Become: Consult We can specify this in unnest_tokens() with token = argument. Look at ?unnext_tokens() to see the different types of tokens the function offers. These grammar vs. computer problems don’t stop here (nor do their solutions). 48.8 Regular Expressions (Regex) for URLS Depending on the text data we are working with, we might either be interested (or not interested) in URLs. Using regular expressions, we can try and detect URLs within our text. Regular expressions are a whole other beast so to keep in scope with the class, here is one example of a regular expression for URLs with a little explanation. (http|https)([^\\\\s]*) The (http|https) looks for “http” or “http” at the start of a string. The ([^\\\\s]*) means “keep matching until a white space” Using stringr we can select different str_* functions to aid in our goal. Here are a list of texts that have a URL in different places within the string. url_front &lt;- c(&quot;https://regex101.com/ is an awesome website to check out different regular expressions&quot;) url_middle &lt;- c(&quot;One of my friends suggested I look at https://www.youtube.com/watch?v=dQw4w9WgXcQ for a good tutorial on regular expressions&quot;) url_back &lt;- c(&quot;Here is a good resource on regex: https://stackoverflow.com/questions/tagged/regex&quot;) urls &lt;- c(url_front, url_middle, url_back) We can create functions to help us do it! remove_url &lt;- function(text) { # Regex url_regex_expression &lt;- &quot;(http|https)([^\\\\s]*)&quot; # Removes URL text &lt;- stringr::str_remove( string = text, pattern = url_regex_expression ) return(text) } extract_url &lt;- function(text) { # Regex url_regex_expression &lt;- &quot;(http|https)([^\\\\s]*)&quot; # Extracts URL text &lt;- stringr::str_extract( string = text, pattern = url_regex_expression ) return(text) } Putting it all together can call the function either on each individual string… # Individual URL matching remove_url(url_middle) extract_url(url_middle) …or within a list like so: # Group URL matching remove_url(urls) extract_url(urls) 48.9 Counting words Now that we have our tidy text data frame, we can start digging in. Let’s add a count of each word: unnest_df |&gt; count(word, sort = TRUE) But lets work within our sentences (or documents) count_df &lt;- unnest_df |&gt; group_by(line) |&gt; count(word, sort = TRUE) count_df Lets look at a (not) very interesting graph of what we have: library(ggplot2) count_df |&gt; ggplot() + aes(y = n, x = forcats::fct_reorder(word, n, max)) + # sort our bars geom_col() + coord_flip() + # flip the coords (easier to digest graph) facet_wrap(~ line, scales = &quot;free_y&quot;) + # let our y axis be free labs(y = &quot;word&quot;, x = &quot;count&quot;) From the graph above, we can see that we have a lot of words that don’t really carry a lot of meaning. 48.10 Can’t stop (words) won’t stop (words) Not all words are created equal - some words are more important than others. Words like “the”, “a”, “it” don’t have any inherent meaning. Pronouns also may not be meaningful “he”, “their”, “hers”. These are called stop words So what do we do? Get rid of ’em. How? We get a stop words data set and anti_join() them. There exists many stop word data sets. These are sometimes called “Lexicons”. You can think of a lexicon as a set of words with a common idea. 48.11 Lexicon-based approach Some common (and native) stop word data sets (or Lexicons) are: Onix SMART snowball These are all within the tidytext package’s stop_words data set. This is a glimpse of what it looks like: tidytext::stop_words |&gt; head() Now let’s remove our stop words. unnest_df &lt;- count_df |&gt; anti_join(stop_words, by = &quot;word&quot;) Here we can see our words with substance. unnest_df |&gt; ggplot() + aes(x = word, y = n) + geom_col() + facet_wrap(~ line, scale = &quot;free_y&quot;) + coord_flip() "],["case-study-martin-luther-king-jr.s-speech.html", "Part 49 Case Study: Martin Luther King Jr.’s Speech 49.1 Wrangle our text 49.2 Visualizing the words", " Part 49 Case Study: Martin Luther King Jr.’s Speech We are going to look at Martin Luther King Jr.’s “I Have a Dream” speech found here. Lets read it in. Since it’s in a .txt file we can use read.table() and specify some things: speech_txt &lt;- read.table( here::here(&quot;data&quot;, &quot;martin_luther_king_speech.txt&quot;), quote = &quot;&quot;, header = FALSE, sep = &quot;\\n&quot;, stringsAsFactors = FALSE ) |&gt; as_tibble() |&gt; rename(line = V1) 49.1 Wrangle our text 49.1.1 unnest_tokens() Wielding the power of tidytext: speech_unnest &lt;- speech_txt |&gt; unnest_tokens(word, line) speech_unnest 49.1.2 Remove stop words speech_unnest &lt;- speech_unnest |&gt; anti_join(stop_words, by = &quot;word&quot;) speech_unnest 49.1.3 Count the words speech_unnest &lt;- speech_unnest |&gt; count(word, sort = TRUE) speech_unnest 49.2 Visualizing the words speech_unnest |&gt; filter(n &gt;= 4) |&gt; # look at words that only occurred 4 or more times ggplot() + aes(y = n, x = forcats::fct_reorder(word, n, max)) + # sort our bars geom_col() + coord_flip() + # flip the coords (easier to digest graph) labs(y = &quot;word&quot;, x = &quot;count&quot;) "],["mining.html", "Part 50 Mining 50.1 Sentiment Analysis 50.2 Metrics of Text", " Part 50 Mining Now that we have gone over how to wrangle/shape our data, it’s time to talk about some useful quantitative measure of text that help us get a better understanding of whats going on within our text. 50.1 Sentiment Analysis Feeling is a big part of communication. Some words have different emotions/feelings behind them. That may be a interest of ours when looking at text. One simple approach we can take is simply having a list of a bunch of words labeled with different emotions. We treat our text as a bag-of-words and just match words to a dataset with those word and sentiment. This is a lexicon-based approach (similar to stop words lexicon based approach). There exists different lexicons with different ways of expressing sentiment. We are going to look at some lexicons found in the textdata package that work nice within tidytext. textdata lexicons: Bing: Expresses words in a binary positive or negative fashion Positive or negative AFINN: Expresses words with a value (-5 to 5) according the the word’s valence -5 to 5 NRC: Expresses words with the following sentiments: Negative Positive Anger Anticipation Disgust Fear Job Sadness Surprise Trust 50.1.1 Looking at our Lexicons Using the tidytext::get_sentiments() function we can take a peek at the words within. We’ll stick to manually reading it in as a .csv to practice our skills. 50.1.1.1 Bing - Positive vs. Negative bing &lt;- read.csv(here::here(&quot;data&quot;,&quot;bing_dictionary.csv&quot;)) bing |&gt; dplyr::count(sentiment) Here we can see how many words are positive and negative within our lexicon 50.1.1.2 AFINN - Words on a -5 to 5 scale afinn &lt;- read.csv(here::here(&quot;data&quot;,&quot;afinn_dictionary.csv&quot;)) afinn |&gt; mutate(value = as_factor(value)) |&gt; # make our value a factor ggplot() + aes(x = value) + geom_histogram(stat = &quot;count&quot;) # tell ggplot to count() From the above plot we can get a picture of the count of each value. In other words, we see that we have a lot of words with a -2 rating. If we wanted specific counts we can count() the value ourselves afinn |&gt; mutate(value = as_factor(value)) |&gt; count(value) 50.1.1.3 NRC - Anger, Anticipation, … nrc &lt;- read.csv(here::here(&quot;data&quot;,&quot;nrc_dictionary.csv&quot;)) This lexicon has multiple representations of a word with different sentiments The word “abandon” has “fear”,“negative”,“sadness” attached to it. Let’s get a feel (ba-dum-tsh) for our lexicon: We can see our unique words count()ing them: nrc |&gt; count(word) |&gt; # automatically collapse the same word nrow() Here we can see how many different sentiments are attached to each word. nrc |&gt; count(sentiment) |&gt; ggplot() + aes(x = sentiment, y = n) + geom_col() 50.1.2 Using inner_join() with our bing lexicon Let’s revisit our unnest_df from earlier unnest_df |&gt; head() Using inner_join() we can match words from our lexicon to our data. Note: Remember to have the same column name as both your data and lexicon for the words. unnest_df |&gt; inner_join(bing, by = &quot;word&quot;) From the above output we can see that line 2 had 2 positive words while line 3 had a mix of 1 positive word and 1 negative word. 50.1.3 Important things with sentiment analysis Now there are a couple things to consider when deciding on which lexicon to use other than just the depth/type of sentiment we want to look at. Here are a some questions to keep in mind: What was the lexicon developed for? Reviews? Tweets? How was the lexicon developed? Through researchers? Crowd sourced? How old is the lexicon? Words change meaning over time Culture of the words used? Another thing to consider is that lexicon sentiment analysis is a bag-of-word type of analysis. Context isn’t considered when looking at words. If you’re interested in looking at different techniques look into n-grams 50.2 Metrics of Text Let’s dig into some quantitative measures of text data. Two main metrics we will consider in this chapter are: TF-IDF Weighted log-odds ratio. 50.2.1 Term-Document Inverse Document Frequency (TF-IDF) TF-IDF stands for Term-Document Inverse Document Frequency. It’s main goal is to measure how “important” a word is in our corpus (Silge &amp; Robinson, 2017). TF-IDF is useful when we want to understand the terms in the corpus as a whole. Let’s break down TF-IDF into it’s two parts: Term-frequency: How many times a term/word occurs? Inverse-Document Frequency: The inverse of the number of times a word shows up in a collection of documents. \\[ idf(term) = ln(\\frac{n_{documents}}{n_{documents\\ containing\\ terms}}) \\] This is based off of the idea that the more a term shows up, the less important that term is. 50.2.1.1 Zipf’s Law TF-IDF is inspired by George Zipf’s law. Zipf’s law states the following: the frequency that a word appears is inversely proportional to its rank. This makes sense if we think about how language is. We would expect words that don’t show up often to carry more weight and be more important than words that show up all the time. 50.2.1.2 Example Let’s calculate the TF-IDF with bind_tf_idf() We need our data set to have one-row per document-term: tidy_df &lt;- unnest_df |&gt; count(line, word, sort = TRUE) tidy_df |&gt; head() Now we can bind_tf_idf() tidy_tf_idf_df &lt;- tidy_df |&gt; bind_tf_idf(word, line, n) tidy_tf_idf_df We can see the top TF-IDF words are: “words”, “feelings”, “express”, “excitement”, and “anxious” while our lowest are “learn” and “day.” 50.2.1.3 Visualize TF-IDF tidy_tf_idf_df |&gt; ggplot() + aes(x = fct_reorder(word, tf_idf), # order `word` by `tf_idf` y = tf_idf, fill = as_factor(line)) + # color :-) geom_col() + coord_flip() + facet_wrap(~ line, scales = &quot;free_y&quot;) + labs(x = &quot;&quot;, fill = &quot;Line #&quot;) Above we get a nice graph of our words and their TF-IDF. 50.2.2 Weighted Log-odds Sometimes we are interested in comparing words between different groups within a corpus. Here are some examples: Corpus of scientific articles from different fields (e.g., economics, medicine, technology): What words are you most likely to see given the field? Corpus of two twitter users: What words are you more likely to see from each user? Corpus of a collection of products and their respective reviews: What words are you more likely to see from each product/rating? Corpus of different baby’s names from different time periods (1960s, 1970s, …) What names were most prevalent in each year compared to other years? To do this we can use the weighted log-odds ratio. Let’s break down what “weighted log-odds ratio” means. 50.2.2.1 Odds Ratio Odds ratio, simply, is the ratio of something happening divided by that thing not happening. \\[ Odds\\ Ratio = \\frac{x\\ happening}{x\\ not\\ happening} \\] This is different from probability. Probability can be thought of the ratio of x happening with all possible happenings. 50.2.2.2 Log-odds ratio The log-odds ratio is just the odds ratio with a log transformation (natural log). Reason being, it allows for some pretty nice behavior like: Symmetry around zero Transforms our range to be between \\([-\\infty, \\infty]\\) \\[ log(Odds\\ Ratio) = log(\\frac{x\\ happening}{x\\ not\\ happening}) \\] 50.2.2.3 Weighted log-odds ratio The bind_log_odds() function fits a posterior log odds ratio assuming a multinominal model with a Dirichlet prior. (See ?bind_log_odds for more info) To look into the actual method behind the function see Monroe, Colaresi &amp; Quinn 2007 50.2.3 Example Using bind_log_odds() we can specify different arguments to tweak our output. library(tidylo) # from earlier: # tidy_df &lt;- # unnest_df |&gt; # count(line, word, sort = TRUE) log_odds_df &lt;- tidy_df |&gt; bind_log_odds(line, word, n, uninformative = TRUE, # TRUE = Do not fit with a Dirichlet Prior unweighted = TRUE) # TRUE = Add a unweighted column 50.2.3.1 Visual We can now visualize the different weighted log odds for each line log_odds_df |&gt; ggplot() + aes(x = fct_reorder(word, log_odds_weighted), y = log_odds_weighted, fill = as_factor(line)) + geom_col() + coord_flip() + facet_wrap(~ line, scales = &quot;free_y&quot;) + labs(x = &quot;&quot;, fill = &quot;Line #&quot;) "],["case-study-2-reviews-on-product.html", "Part 51 Case Study 2: Reviews on Product 51.1 Contributions 51.2 References", " Part 51 Case Study 2: Reviews on Product Below we are going to look at different reviews on Amazon for candles. Let’s read in our dataset. # library(tidyverse) # library(tidytext) scented_reviews &lt;- readxl::read_excel(here::here(&quot;data&quot;, &quot;Scented_all.xlsx&quot;)) |&gt; mutate(review_id = row_number()) |&gt; janitor::clean_names() |&gt; # A function that changes the column names to snake_case/cleans them up a little mutate(rating = as_factor(rating)) 51.0.1 Clean Text First step is to unnest_tokens() our reviews. unnest_scent &lt;- scented_reviews |&gt; unnest_tokens(&quot;word&quot;, &quot;review&quot;, token = &quot;words&quot; ) 51.0.2 Remove Stop-words We then remove our normal stop words. unnest_scent &lt;- unnest_scent |&gt; anti_join(stop_words, by = &quot;word&quot;) 51.0.3 Removing Custom Stop-words Usually there are words that we know are more “noise” than “signal”. In order to determine if a word is “noise” we can implement different strategies. One strategy is to look through all unique words and skim to find one’s not offered in the stop words lexicon already. unnest_scent |&gt; distinct(word) |&gt; head() Another strategy is to look at the most frequent words and determine if the word is useful or not. unnest_scent |&gt; count(word) |&gt; arrange(desc(n)) |&gt; head() Now once we decide which to remove we can put it together in a tribble() custom_stopwords &lt;- tribble( ~word, ~lexicon, &quot;candle&quot;, &quot;custom&quot;, &quot;candles&quot;, &quot;custom&quot;, &quot;smell&quot;, &quot;custom&quot;, &quot;smells&quot;, &quot;custom&quot;, &quot;scent&quot;, &quot;custom&quot;, &quot;fragrance&quot;, &quot;custom&quot;, &quot;yankee&quot;, &quot;custom&quot;, &quot;love&quot;, &quot;custom&quot;, # This word shows up so much it acts like a stop word &quot;sooo&quot;, &quot;custom&quot;, # This is an example of using text from the internet &quot;soooo&quot;, &quot;custom&quot;, &quot;doesn&quot;, &quot;custom&quot;, &quot;don&quot;, &quot;custom&quot; ) We use the same method to remove our custom_stopwords as we do with regular ol’ stop_words unnest_scent &lt;- unnest_scent |&gt; anti_join(custom_stopwords, by = &quot;word&quot;) 51.0.3.1 Remove Numbers/Non-alphabet characters Depending on the type of data you’re working with there might be more than just characters you need to remove. Since we’re working with data from an online source, there are some weird characters that appear as well as digits used. Using the stringr package, we can clean it up. unnest_scent &lt;- unnest_scent |&gt; mutate(word = str_replace(word, pattern = &quot;[^\\x20-\\x7E](.*)&quot;, replacement = &quot;&quot;)) |&gt; # Remove weird characters filter( !str_detect(word, &quot;[0-9]+&quot;), # Remove digits word != &quot;&quot; ) 51.0.3.2 Removing words that only show up &lt; 10 times. A common way to reduce the amount of noise within a corpus is to focus on words that show up more than n number of times. unnest_scent &lt;- unnest_scent |&gt; add_count(word, name = &quot;n_total&quot;) |&gt; filter(n_total &gt;= 10) 51.0.4 Word Frequency 51.0.5 Sentiment analysis Let’s get a feel for our data (someone stop me, I’m on a roll). We are going to work with the BING and NRC lexicons. 51.0.5.1 BING Just a reminder, BING uses a positive/negative coding of each word. bing &lt;- read.csv(here::here(&quot;data&quot;,&quot;bing_dictionary.csv&quot;)) bing_scent &lt;- unnest_scent |&gt; inner_join(bing, by = &quot;word&quot;) 51.0.5.1.1 Overall bing_scent |&gt; count(sentiment) |&gt; ggplot() + aes( x = sentiment, y = n, fill = sentiment ) + geom_col() + labs( title = &quot;Overall BING Sentiment of Customer Reviews&quot;, y = &quot;Number of Words&quot; ) According to the graph above, overall we have a lot more positive words than negative ones within our reviews. 51.0.5.1.2 By Rating Now it might be useful to know how the number of positive and negative words change within each rating given. bing_scent |&gt; group_by(rating) |&gt; count(sentiment) |&gt; ggplot() + aes(x = sentiment, y = n, fill = sentiment) + geom_col() + facet_wrap(~rating, scales = &quot;free&quot;) + labs( title = &quot;BING Sentiment of Customer Reviews by Rating&quot;, y = &quot;Number of Words&quot; ) Intuitively, we expect for an inverse relationship between the rating and number of positive/negative words. To put another way, we expect people who rate the candle lower to express/use more negative sentiment/words 51.0.5.2 NRC Reminder: NRC codes words with different emotions. nrc &lt;- read.csv(here::here(&quot;data&quot;,&quot;nrc_dictionary.csv&quot;)) nrc_scent &lt;- unnest_scent |&gt; inner_join(nrc, by = &quot;word&quot;) 51.0.5.2.1 Overall Let’s take a peek at the over amount of sentiment expressed within our reviews. nrc_scent |&gt; count(sentiment) |&gt; ggplot() + aes(x = fct_reorder(sentiment, n), y = n, fill = sentiment) + geom_col() + labs( title = &quot;Overall NCR Sentiment of Customer Reviews&quot;, y = &quot;Number of Words&quot;, x = &quot;Sentiment&quot; ) Similar to our BING analysis, we have a higher use of positive-type words than negative. 51.0.5.2.2 By Rating Again, its useful to take it within the context of what the review had in rating. nrc_scent |&gt; group_by(rating) |&gt; count(sentiment) |&gt; ggplot() + aes(x = fct_reorder(sentiment, n), y = n, fill = sentiment) + geom_col() + facet_wrap(~rating, scales = &quot;free&quot;) + labs( title = &quot;NCR Sentiment of Customer Reviews by Rating&quot;, y = &quot;Number of Words&quot;, x = &quot;Sentiment&quot; ) + coord_flip() Directing our attention to ratings 1 and 5, we see the same relationship with the BING analysis. 51.0.6 Weighted log-odds Let’s look at what words are used between ratings using the weighted log-odds. First we use the helpful bind_log_odds() function from the tidylo package. library(tidylo) scent_log_odds &lt;- unnest_scent |&gt; group_by(rating) |&gt; add_count(word, name = &quot;n_rating&quot;) |&gt; # count each word within the ratings distinct(word, .keep_all = TRUE) |&gt; # we only need the word (`word`) and how many times it shows up (`n_rating`) bind_log_odds(rating, word, n_rating) Now that we have our metrics lets look at important words for each rating. Let’s focus on the top 15 words used to reduce clutter. scent_log_odds |&gt; group_by(rating) |&gt; top_n(15, log_odds_weighted) |&gt; ggplot() + aes(x = fct_reorder(word, log_odds_weighted), y = log_odds_weighted, fill = factor(rating)) + geom_col() + facet_wrap(~rating, scales = &quot;free_y&quot;) + coord_flip() + labs( x = &quot;&quot;, y = &quot;Weighted Log Odds&quot;, title = &quot;Top 15 words with highest weighted log odds&quot; ) + theme(legend.position = &quot;none&quot;) For our higher rated reviews, we see use of “yummy”, “excellent”, “affordable”. Now we also see “law”? Can you think of any reason why we would see that word? For lower rated reviews, we see “broken”, “unusable”, and “waste”. A interesting way to think about “importance” is to compare between a word within a group and the whole corpus. To do this we look at how much the word happens overall and the weighted log odds. scent_log_odds |&gt; top_n(10, n_rating) |&gt; ggplot() + aes(x = n_total, y = log_odds_weighted, label = word, color = factor(rating)) + geom_point() + ggrepel::geom_text_repel(max.overlaps = 50) + labs( title = &quot;How important is a word within a rating versus within the whole corpus?&quot;, subtitle = &quot;Top 10 words per Customer Rating&quot;, x = &quot;Count of occurances within corpus&quot;, y = &quot;Weighted Log odds&quot;, color = &quot;Rating&quot; ) + geom_hline(yintercept = 0, lty = 2, alpha = 0.2, size = 1.2) Lets digest what the above graph is saying. Starting with the word “broken” we see that within a rating of 1, “broken” is extremely important. However as we look at the second point for “broken” we see that it is a lot less important for rating 2, similarly with rating 3. We can also compare different words with similar importance. For example, the word disappointed within a rating of 2 is around the same importance as wonderful is within a rating of 5. Now because there are 5 ratings on this graph, it can be less work if we just look at ratings of 1 and 5. scent_log_odds |&gt; filter(rating %in% c(1, 5)) |&gt; top_n(20, n_rating) |&gt; ggplot() + aes(x = n_total, y = log_odds_weighted, label = word, color = rating) + geom_point() + ggrepel::geom_text_repel(max.overlaps = 40) + labs( title = &quot;How important is a word between a rating versus within the whole corpus?&quot;, subtitle = &quot;Reviews with a Rating of 1 or 5; Top 20&quot;, x = &quot;Count of occurances within corpus&quot;, y = &quot;Log odds&quot; ) + geom_hline(yintercept = 0, lty = 2, alpha = 0.2, size = 1.2) Above we get a clearer picture of the importance of words between ratings of 1 and 5. 51.1 Contributions Julia Silge deserves a huge amount of credit for not only a great outline of text analysis within R but also for her inspiration. Thank you to the following for your support and guidance: Alex Denison, Loni Hagen, Mary Falling. 51.2 References Wickham, H., &amp; Grolemund, G. (2016). R for data science: Import, tidy, transform, visualize, and model data (First edition). O’Reilly. Silge, J., &amp; Robinson, D. (2017). Text mining with R: A tidy approach (First edition). O’Reilly. "],["all-the-plot-things.html", "Part 52 All the plot things", " Part 52 All the plot things So far in class, we’ve made a bunch of ggplots, but we haven’t paid much attention to making them beautiful. For the next few sessions, we’ll explore how we can customize the look and feel of our plots to make them (1) aesthetically appealing and engaging, (2) informative, and (3) accessible. library(ggplot2) library(dplyr) library(patchwork) "],["basic-elements-of-ggplot.html", "Part 53 Basic elements of ggplot", " Part 53 Basic elements of ggplot Data Aesthetics (mapping) Geoms Facets Stats "],["changing-the-stat.html", "Part 54 Changing the stat", " Part 54 Changing the stat In the process of connecting data -&gt; mapping, there is some statistical computation. In many cases, this computation is identity(). For example, with geom_point(), it plots the points directly based on x and y values. Other times, there is more of a computation. stat_count() stat_bin() stat_density() geom_*() functions usually come with a reasonable default stat_*() and subsequent mapping. For example, geom_histogram() does stat_bin() to bin the continuous values, then aes() maps y to the count in each bin. But sometimes you might want to change this mapping after the stat. Do that with after_stat() inside aes(). bw &lt;- .3 h1 &lt;- ggplot(mpg) + aes(x = displ) + geom_histogram(binwidth = bw, fill = &quot;skyblue&quot;) ## histogram with density h2 &lt;- ggplot(mpg) + aes(x = displ) + geom_histogram(aes(y = after_stat(density)), binwidth = bw, fill = &quot;skyblue&quot;) ## histogram with proportion h3 &lt;- ggplot(mpg) + aes(x = displ) + geom_histogram(aes(y = after_stat(count / sum(count))), binwidth = bw, fill = &quot;skyblue&quot;) h1 + h2 + h3 ## histogram with density curve ggplot(mpg) + aes(displ) + geom_histogram(aes(y = after_stat(count)), binwidth = bw, fill = &quot;skyblue&quot;) + geom_density(aes(y = after_stat(count) * bw)) + theme_minimal() "],["customizing-plots.html", "Part 55 Customizing plots!", " Part 55 Customizing plots! You can customize all sorts of stuff in your ggplot plots! Scales x and y Colors Shapes Line types Alpha Guides Legend (legend, bins, colorbar, colorsteps) Axis None Text Axis labels Geom labels Annotations Titles and captions Themes (design features) Background Grid lines Text styling Alignment "],["scales.html", "Part 56 Scales", " Part 56 Scales A ggplot is constructed with a flow like this: data -&gt; stat -&gt; scale -&gt; mapping -&gt; render The “scale” step concerns how the computed data/stat corresponds to numbers/values on the plot (e.g., “20 mpg” corresponds to a point “3 units up on the y axis”; “4 cylinder” corresponds to “blue”). You control how values are scaled using the scale_*() functions. Almost every aesthetic you can map to data (e.g., x, y, color, fill, alpha, linetype, shape, size, radius) has corresponding scale functions. These functions all have the following form: scale_[aesthetic]_[type] The scale_*() functions include many arguments to control various features of the scale, depending on the aesthetic. "],["position-scales.html", "Part 57 Position scales 57.1 Arguments", " Part 57 Position scales The scales for x and y control positioning, limits, breaks, labels, titles, etc. of the axes. scale_x_continuous() scale_x_discrete() scale_x_binned() 57.1 Arguments name() The name of the scale. Used for the axis/legend title. Give multiple scales the same name to combine them. scale_x_continuous(name = \"MPG\") breaks Where to break the scale (e.g., where to bin or where to place labeled tick marks) To override the default computed breaks, pass a vector of values (There are more advanced options as well) scale_x_continuous(name = \"MPG\", breaks = c(0, 10, 20, 30)) labels Override the existing axis labels Usually given as a function that transforms the text e.g., Numbers and Dates scales::label_percent scales::label_dollar scales::label_date scales::label_comma scales::label_pvalue Text stringr::str_to_upper stringr::str_to_sentence scales::label_parse / scales::label_math limits The minimum and maximum values for the scale e.g., scale_x_continuous(limits = c(40, 100)) oob What to do with values outside the limits Defaults Continuous: scales::oob_censor Binned: scales::oob_squish scales::oob_keep: Like zooming, don’t remove or change values. ggplot(mtcars) + aes( x = hp, y = mpg, color = factor(cyl), shape = hp &gt; 200 ) + geom_point(size = 3) + scale_x_continuous( limits = c(0, 200), oob = scales::oob_squish ) trans A transformation function to apply to each value of the scales Must be a quoted function name e.g., “sqrt” or “rev” or “log10” scale_x_continuous(trans = \"sqrt\") scale_x_sqrt(), scale_x_reverse(), and scale_x_log10() are shortcuts position “top”, “bottom”, “left”, or “right” Can also be set in guide_axis() guide A function controlling formatting of the guide I usually set in guides() instead na.translate TRUE or FALSE, for discrete scales, plot NA? scale_x_discrete(na.translate = FALSE) na.value What value to replace NA with. scale_x_discrete(na.translate = TRUE, na.value = \"(Missing)\") "],["color-scales.html", "Part 58 Color scales 58.1 Arguments", " Part 58 Color scales There are a variety of ways to specify colors in R, as well as several nice built-in color palletes. Some resources on color tools in R: R color cheatsheet colorspace package R cookbook colors Another nice ggplot color guide Colormind coolors Color Designer Reminder: color: The outline color for lines/points/regions fill: The fill color for regions and point-shapes with a filling (shapes 21-25) The color scale functions you should use in ggplot all have the following form: scale_[aesthetic]_[type + pallete] For example: Custom Gradients, Color Wheels, and Palettes - Continuous: scale_color_gradient() - Continuous: scale_color_gradient2() - Continuous: scale_color_gradientn() Binned: scale_color_steps() Binned: scale_color_steps2() Binned: scale_color_stepsn() Discrete: scale_color_hue() Discrete: scale_color_manual() Viridis - Continuous: scale_color_viridis_c() - Binned: scale_color_viridis_b() - Discrete: scale_color_viridis_d() Color Brewer - Discrete: scale_color_brewer() - Continuous: scale_color_distiller() - Binned: scale_color_fermenter() Greyscale - scale_color_grey() ggthemes - ggthemes::scale_color_excel() - ggthemes::economist() - ggthemes::scale_color_fivethirtyeight() - ggthemes::scale_color_colorblind() Other packages - wesanderson - ggsci 58.1 Arguments All of the arguments that can be used with position scales can also be used with color scales. name breaks labels limits oob trans position guide na.translate na.value Color-specific arguments Color scale functions also have arguments to control how the palettes are constructed. The exact arguments vary by scale function. Let’s look at a few. scale_color_viridis_*() begin and end Values between 0 and 1 saying where on the scale to start and stop. direction Which end to start from? 1 or -1 option Which of the viridis family palettes to use? “A”, “B”, “C”, “D”, “E” aesthetics Which aesthetics to apply the palette to? Useful to do aesthetics = c(\"color\", \"fill\") scale_color_brewer() direction Which end to start from? 1 or -1 type Which kind of color scale to use? “seq” (sequential), “div” (diverging), “qual” (qualitative) palette Which specific palette to use? Can be a number or one of the named palettes (see ?scale_color_brewer()) aesthetics scale_color_manual() For specifying your own discrete palettes. values A character vector of colors names or hex codes to use. If the vector has names, it will match data values to names. Must be at least as long as the number of unique values. aesthetics p &lt;- ggplot(mtcars) + aes(mpg, wt) + geom_point( aes(colour = factor(cyl)), size = 8 ) p + scale_colour_manual(values = c(&quot;4&quot; = &quot;red&quot;, &quot;6&quot; = &quot;blue&quot;, &quot;8&quot; = &quot;darkgreen&quot;)) pal &lt;- palette.colors(&quot;Okabe-Ito&quot;, n = 10) p + scale_colour_manual(values = pal) pal &lt;- palette.colors(&quot;R4&quot;, n = 4) p + scale_colour_manual(values = pal) scale_color_gradient() For specifying your own gradient. scale_colour_gradient( ..., low = &quot;#132B43&quot;, high = &quot;#56B1F7&quot;, na.value = &quot;grey50&quot;, aesthetics = &quot;colour&quot; ) scale_colour_gradient2( ..., low = muted(&quot;red&quot;), mid = &quot;white&quot;, high = muted(&quot;blue&quot;), midpoint = 0, na.value = &quot;grey50&quot;, aesthetics = &quot;colour&quot; ) scale_colour_gradientn( ..., colours, values = NULL, na.value = &quot;grey50&quot;, aesthetics = &quot;colour&quot;, colors ) scale_color_identity() A special scale. It doesn’t map colors to a specific value, but uses the variable values directly. Useful if you want to store the color codes directly in the data. "],["shape-scales.html", "Part 59 Shape scales", " Part 59 Shape scales library(ggplot2) library(dplyr) library(patchwork) In addition to distinguishing groups or values by color, you can also distinguish groups by shape. You can see a list of R’s built-in plot shapes here. You can use scale_shape_*() with discrete and binned variables, but not continuous variables. R has a set of built-in plot shapes: 0–14: Empty shapes — outline color, no fill 15–20: Solid shapes — solid color, no separate fill - Default ggplot shape is 19 21–25: Filled shapes — outline color, separate fill ggplot(mtcars) + aes(x = hp, y = mpg, shape = factor(cyl)) + geom_point(size = 4, color = &quot;blue&quot;, fill = &quot;yellow&quot;) + scale_shape_manual(values = c(1, 19, 25)) + theme_minimal() You can also give a character vector to plot those characters. Each element must be a single character long. ggplot(mtcars) + aes(x = hp, y = mpg, shape = factor(cyl)) + geom_point(size = 4, color = &quot;blue&quot;, fill = &quot;yellow&quot;) + scale_shape_manual(values = c(&quot;4&quot;, &quot;6&quot;, &quot;8&quot;)) + theme_minimal() mtcars |&gt; tibble::rownames_to_column(var = &quot;model&quot;) |&gt; ggplot() + aes(x = hp, y = mpg, shape = factor(cyl), label = model) + geom_text(size = 4, color = &quot;blue&quot;, fill = &quot;yellow&quot;) + theme_minimal() ggplot(mtcars) + aes(x = hp, y = mpg, shape = factor(cyl)) + geom_point(size = 4, color = &quot;blue&quot;, fill = &quot;yellow&quot;) + scale_shape_manual(values = c(&quot;😸&quot;, &quot;😾&quot;, &quot;🙀&quot;)) + theme_minimal() scale_*_identity() is a special scale that uses the value of the variable directly on the plot. With shape, you can use it to plot the values if they are single character. (Use one of the text geoms if you want multi-character shapes.) ggplot(mtcars) + aes(x = hp, y = mpg, shape = factor(cyl)) + geom_point(size = 4, color = &quot;blue&quot;, fill = &quot;yellow&quot;) + scale_shape_identity() + theme_minimal() You can also use it with color or fill to store the color codes directly in the data. tibble( scale = forcats::as_factor(c(&quot;ES&quot;, &quot;A&quot;, &quot;C&quot;, &quot;Ex&quot;, &quot;O&quot;)), score = c(5, 8, 4, 6, 3), .fill = c(&quot;#CD0BBC&quot;, &quot;#DF536B&quot;, &quot;#2297E6&quot;, &quot;#61D04F&quot;, &quot;#F5C710&quot;) ) %&gt;% ggplot() + aes(x = scale, y = score, fill = .fill) + geom_path(group = &quot;n&quot;) + geom_errorbar(aes(ymin = score - .5, ymax = score + .5), width = .05) + geom_point(shape = 21, size = 5) + scale_fill_identity() + ylim(0, 10) + theme_minimal() "],["linetype-scales.html", "Part 60 Linetype scales", " Part 60 Linetype scales Line type is like shape. You can specify line type for discrete and binned variables, but not continuous variables. You can see a list of R’s built-in line types here. You can refer to line types by number or name. R’s built-in line types: Number Name 0 ‘blank’ 1 ‘solid’ 2 ‘dashed’ 3 ‘dotted’ 4 ‘dotdash’ 5 ‘longdash’ 6 ‘twodash’ "],["alpha-and-size-scales.html", "Part 61 Alpha and size scales", " Part 61 Alpha and size scales scale_alpha_*() (transparency) and scale_size_*() work similarly to scale_color_*(). You may want to set the range argument to control the minimum and maximum alpha/size used. Alpha can range between 0 and 1. "],["guides.html", "Part 62 Guides", " Part 62 Guides Guides are the instructions for how to read the scales on your plot. The two major kinds of guides are axes and legends. You can customize the appearance of your guides using the guides() and guide_*() functions. p &lt;- data.frame( x = 1:5, y = 1:5, p = 1:5, q = 1:5, r = factor(1:5) ) %&gt;% ggplot() + aes(x, y, colour = p, size = q, shape = r) + geom_point() + scale_size_continuous(range = c(4, 8)) + theme_classic() p # Legends will be combined if they have the same type, values, and titles p + guides( colour = guide_legend(title = &quot;title&quot;), size = guide_legend(title = &quot;title&quot;), shape = guide_legend(title = &quot;title&quot;) ) # If all you want to change is the title of the guides, you can use labs() p + labs( color = &quot;title&quot;, size = &quot;title&quot;, shape = &quot;title&quot; ) "],["legend-guides.html", "Part 63 Legend guides 63.1 Legend arguments", " Part 63 Legend guides There are four styles of legend guides available: guide_legend() Basic legend guide. Use this one most of the time. guide_bins() Use if you’ve got binned scales. guide_colorbar() Nicely show a continuous color gradient. guide_colorsteps() Use with binned color/fill scales. Can also be useful if you have continous scales, but want to pick out useful reference levels. p1 &lt;- p + guides( colour = guide_legend(title = &quot;legend&quot;), size = &quot;none&quot;, shape = &quot;none&quot; ) p2 &lt;- p + guides( colour = guide_bins(title = &quot;bins&quot;), size = &quot;none&quot;, shape = &quot;none&quot; ) p3 &lt;- p + guides( colour = guide_colorbar(title = &quot;colorbar&quot;), size = &quot;none&quot;, shape = &quot;none&quot; ) p4 &lt;- p + guides( colour = guide_colorsteps(title = &quot;colorsteps&quot;), size = &quot;none&quot;, shape = &quot;none&quot; ) p1 + p2 + p3 + p4 63.1 Legend arguments title Set the title of the legend guide Also impacts legend merging direction \"vertical\" or \"horizontal\" nrow and ncol Control number of rows and columns of legend reverse TRUE or FALSE: Should the order of the legend values be reversed? order If multiple legend guides are shown separately, what order should they appear in? A number between 1 and 99 To place the legend somewhere other than the right side, add theme(legend.position = \"bottom\") to the plot: p + guides( colour = guide_legend(title = &quot;title&quot;), size = guide_legend(title = &quot;title&quot;), shape = guide_legend(title = &quot;title&quot;) ) + theme(legend.position = &quot;bottom&quot;) "],["axis-guides.html", "Part 64 Axis guides", " Part 64 Axis guides guide_axis() controls how the axes appear. The main use is to control label overlap and change the axis position. Some useful arguments are: title Set the title of the axis guide check.overlap Drop some labels if they overlap? Mostly usful if the values are ordered. angle Rotation in degrees of axis labels This is usually very hard to read and should be avoided. n.dodge Put labels on multiple rows/columns to avoid overlap? A number indicating how many rows/columns position Where to put the axis? \"top\", \"bottom\", \"left\", \"right\" "],["hide-guides.html", "Part 65 Hide guides", " Part 65 Hide guides You can hide a guide with guide_none(). p + guides( colour = guide_legend(title = &quot;Legend Title&quot;), size = guide_none(), shape = &quot;none&quot; ) + theme(legend.position = &quot;bottom&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
