# Data Simulation: Making Your Perfect Dataset

Today we will discuss various ways to simulate data. Generally, we either simulate data that follows some global distribution (e.g. normal), or some specific pattern of responses at the individual level (e.g. within-person normal).


## Worksheet TODO

You can find a worksheet template for today [here](https://raw.githubusercontent.com/USF-Psych-DataSci/Classroom/master/tutorials/s13_simulation-exercise.Rmd).

## Resources

### Coding Resources
- [An Intro to Simple Simulations](https://bookdown.org/rdpeng/rprogdatascience/simulation.html)
- [Simulating Different Distributions and Sequences](http://michaelminn.net/tutorials/r-simulating-data/)
- [Simulating Simple and Multivariate Data](https://it.unt.edu/simple-data-simulations)
- [https://aosmith.rbind.io/2018/08/29/getting-started-simulating-data/#discrete-counts-with-rpois](An Intro to Simulation Using Different Functions)
- [https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#using-the-aov-function-for-within-or-between-subjects-designs](Simulating Data for T-test and ANOVA Designs, Includes Power Analysis)
- [A Brief Introduction to the SimStudy Package](https://cran.r-project.org/web/packages/simstudy/vignettes/simstudy.html)
https://www.youtube.com/watch?v=gxAaO2rsdIs

### Theoretical Resources
For an overview of Directed Acyclic Graphs see
[This Paper](https://journals.sagepub.com/doi/full/10.1177/2515245917745629). These graphs are useful for visualizing any complex relationships you may want to simulate.

For an overview of how simulations can aid decision making in instrument development see [This Paper](https://journals.sagepub.com/doi/full/10.1177/0734282917718062).

For an overview of simulations in clinical psychology see [This Paper](https://journals.sagepub.com/doi/full/10.1177/0734282917690302). Although I think this paper is also useful for those outside of clinical to read.

For an overview of simulations in organizational and management research see [This paper](https://journals.aom.org/doi/10.5465/amr.2007.26586485)

```{r}
library(tidyverse)
```
## Simple Simulations

The first, and most important thing, to remember about simulations is that the process is random. You will never reproduce the same results twice unless you set a seed for your simulation. The `set.seed()` function allows you to specify a 4 digit number. This will fix how random numbers are generated so that every time you run a script the same results will be produced. This is true across any machine you run the analyses on, so others can replicate your results.

### Distributions in R

Base R provides several functions that allow you to sample data from various distributions. For example, the rnorm statement allows you to randomly sample values from a normal distribution with a specified mean and standard deviation.

The code below simulates data with a mean of 4, standard deviation of 1, and a sample size of 200.
```{r}
#This makes it so that all simulations will produce the same numbers
set.seed(1337)

dataNormal <- data.frame("X" = rnorm(200, mean = 10, sd = 1))

ggplot(dataNormal, aes(x=X)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

We can extend this to simulate data from multiple different groups and compare the resulting distributions. This is essentially how you would conduct a power analysis.
- What if you don't want a normal distribution?
- There are functions for several different distributions [built into R](https://www.stat.umn.edu/geyer/old/5101/rlook.html)

#### Go to part 1 of the exercise, `rnorm()`, to practice simulating data from two distributions.

### Simulating Distributions Based on Specific Probabilities

While the functions included in R provide an easy way to simulate lots of different distributions, we may want to be more specific about exactly what our distribution looks like. We can use the `sample()` function to sample from a range of numbers and specify a specific probability that each one is sampled.

We can replicate a normal sampling distribution above using specific scale points. For example, let's say we want to simulate a normal distribution of scores for a 5 item likert scale.
```{r}
set.seed(1337)
                            #The numbers we will sample from
sampleDiscreteNormalDistribution <- data.frame("X" = sample(c(1, 2, 3, 4, 5), #i.e. our scale points
#200 = the number of cases. Replace = T means we can sample the same number multiple times. prob = the probability of each point being sampled. (e.g. 1 has a 6% chance, 2 has a 24% chance)
                      200, replace=T, prob = c(6, 24, 40, 24, 6))) 
ggplot(sampleDiscreteNormalDistribution, aes(x=X)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")

```

How about a skewed distribution?
```{r}
set.seed(1337)

sampleSkewedDistribution <- data.frame("X" = sample(c(1, 2, 3, 4, 5), #scale points
                      200, replace=T, prob = c(4, 12, 20, 28, 38)))
ggplot(sampleSkewedDistribution, aes(x=X)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

If we wanted to, we could even match the normal distribution more precisely using the sample function. Here we use the sequence function to generate a range of numbers from -3.00 to 3.00, counting up by 0.01 for each new number. Thus, the start of the sequence would look like: -3.00, -2.99, -2.98, etc. We then use the same sequence with the dnorm function, which gives us the probability of a given number at a point along the normal curve. (Note: if you wanted you could change the mean and SD of the dnorm function to get different looking results).
```{r}
set.seed(1337)

sampleNormalDistribution <- data.frame("X" = sample(c(seq(-3.00, 3.00, by = 0.01)), #scale points
                      10000, replace=T, prob = c(dnorm(seq(-3.00, 3.00, by = 0.01), mean = 0, sd = 1))))
ggplot(sampleNormalDistribution, aes(x=X)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

#### Go to part 2 of the exercise, `sample()`, so practice using the sample function.

We can also use the `sample()` function to generate data for individual item responses. Below is a script that samples cases for 200 individuals responding to a 10 item measure. The numbers are sampled using the `sample()` function, and are filled by row into a matrix. This data follows a normal distribution globally. (Note: we now need to sample 2,000 cases because each 'person' has 10 responses. Thus, there are 200 * 10 total cases in our data).
```{r}
set.seed(1337)

sampleNormalItemDistribution <- sample(c(1, 2, 3, 4, 5), #scale points
                      2000, replace=T, prob = c(6, 24, 40, 24, 6)) 

matrixNormalItemDistribution <- matrix( 
  c(sampleNormalItemDistribution),         
  nrow=200,              
  ncol=10,              
  byrow = TRUE)
rm(sampleNormalItemDistribution)

dataNormalItemDistribution <- as.data.frame(matrixNormalItemDistribution)

dataNormalItemDistributionLong <- gather(dataNormalItemDistribution, key = "key", value = "value", V1:V10, factor_key = FALSE)

ggplot(dataNormalItemDistributionLong, aes(value)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")

```

This is great. But, this data follows the same global pattern for all individuals. That is, the distribution for each person will be nearly identical. We know this isn't the case for actual data. Some people will respond with low values on the scale and not high values. We can model this behavior within individual's using loops.

### Using Loops to Simulate Individual Behavior

First we need to talk about what loops are. Basically, it's a way to run the same function on something for multiple iterations.

For example, the below for loop will run 10 times. For each time it runs it will save the value of 'i' to the ith row in the first column of the 'dataForLoop' matrix. i = the loop you are currently on. So the values of i will range from 1 to 10. It will start at 1 and count up once for every loop.
```{r}
#Create a matrix we will save our values to
dataForLoop <- matrix(nrow=10,ncol=1)
#The loop length goes in parentheses ()
for(i in 1:10)
#The function the loop executes goes in curly brackets {}
{
  #we use brackets [] to call a specific row and column in our dataset
  #we specify the row we want first, then the column
  #we can open our data frame or matrix and count (starting from 1) to get the number of a row or column we want. 
  #we can also just hover the name of that row or column to see its number.
  dataForLoop[i,1] <- i
}

print(dataForLoop)
  
```

We can extend this to have a loop within another loop. This allows us to select both rows and columns at the same time. Below we will iterate through i 10 times and use that to symbolize rows. We will also iterate through j 5 times, which symbolizes columns. For each cell in our matrix will we print the current value of i plus the current value of j.
```{r}
dataDoubleLoop <-matrix(nrow=10,ncol=5)

for(i in 1:10)
{
  for (j in 1:5)
  {
   dataDoubleLoop[i,j] <- i + j
  }
}

print(dataDoubleLoop)

```

##TODO
What's actually going on here? Let's look at the GIF below.
```{r 1simpson, fig.cap="Example of how two loops can be used to write random numbers to each value of a data frame."}
knitr::include_graphics("/Loop-GIF.gif")
```

What we can do now is extend this functionality to sample values that follow a normal distribution WITHIN every individual. Each starting value is randomly selected from a uniform distribution, so the distribution of all scores should look roughly uniform, but the distribution of scores within every person should be normal.
```{r}
set.seed(1337)
#Creates our matrix
normalWPSample <-matrix(nrow=200,ncol=10)
#for loop to iterate over individuals
for(i in 1:200){
  #for loop to iterate over items within individuals
  for(j in 1:10){
    #first response randomly generated and set as the center point for all remaining responses
    if(j==1){ #remember j is our row, so this will sample a new center point whenever we are  on item 1 for a person.
      wInMean<-sample(1:5,1) #sets our mean. Could save in our matrix if we want to know it
          #We will sample a value that is the center point, or deviates from it by 1 or 2
      normalWPSample[i,j] <- sample(c(wInMean - 2, wInMean - 1, wInMean, 
                                      wInMean + 1, wInMean + 2), 
                                    #probabilities to sample these values
                                    1, replace=T, prob = c(6, 24, 40, 24, 6)) 
    }
    #all responses past first response. This function is the same as above, 
    #but we don't need to sample a value for the center point because we already have it
    else{
      normalWPSample[i,j] <- sample(c(wInMean - 2, wInMean - 1, wInMean, 
                                      wInMean + 1, wInMean + 2), 
                                    1, replace=T, prob = c(6, 24, 40, 24, 6)) 
    }
  }
}
dataNormalWPSample <- as.data.frame(normalWPSample)
dataNormalWPSampleLong <- gather(dataNormalWPSample, key = "key", value = "value", V1:V10, factor_key = FALSE)

ggplot(dataNormalWPSampleLong, aes(value)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

What's the issue here?

We're able to sample values from outside our scale values of 1-5 for people with low or high means. There are a few ways to fix this, but the simplest one is to just replace all values outside our scale with the closet scale point.

```{r}
#If there is a number greater than 5 replace it with 5
dataNormalWPSampleLong[dataNormalWPSampleLong>5]<-5
#If there is a number less than 1 replace it with 1
dataNormalWPSampleLong[dataNormalWPSampleLong<1]<-1

ggplot(dataNormalWPSampleLong, aes(value)) +
  geom_histogram(binwidth=0.5,color="black", fill="white")
```

We could also calculate a sum or average score for our scale.
```{r}

for (i in 1:200)
{
#We can use a loop to grab the mean for all 10 items and write it to a new column
dataNormalWPSample[i,11] <- mean(as.numeric(dataNormalWPSample[i,1:10]))
}

#we can use this to rename our 11th variable to something more meaningful
names(dataNormalWPSample)[11] <- "average"

ggplot(dataNormalWPSample, aes(average)) +
  geom_histogram(binwidth=0.1,color="black", fill="white")

psych::describe(dataNormalWPSample$average)
```

#### Go to part 3 of the exercise, `for()`.

### Simulating Actual Studies

#### Modeling training program effectiveness
Suppose there is an organizational researcher who wants to know the effect of two training programs on job performance. This researcher will test how these two programs (Training 1, and Training 2) perform against a control group. This researcher also believes that the trait conscientiousness will impact how effective these training methods are. Finally, the researcher has training administered by 3 different trainers and thinks that the specific relationship someone has with that trainer will impact the effectiveness of the training. Each person will receive their training from ONE trainer, but will receive EVERY training program.

We will first setup a data frame with all of the variables we will need for the study. The training group someone is in, their performance scores after training is received (3 scores for each person), their personal ID (1-300), the trainer they received training from, and whether they are high or low on conscientiousness.
```{r}
#Generates our data
dataTrainingMain <- data.frame(training = rep(c("Control", "Training 1", "Training 2")[1:3]),
                       perfScores = 0,
                       person = rep(1:300, each = 3),
                       Cgroup = rep(c("Low Conscientious","High Conscientious"), times=1, each = 450))

head(dataTrainingMain, n=5)
tail(dataTrainingMain, n=5)
```

We will first generate performance scores for each person based on the training they have received. All groups will have an SD of 1. The control group has a mean of 5, the group that receives the first training program has a mean 1 SD higher of 6, and the group that receives the second training program has a mean of 1SD higher than that of 7.
```{r}
set.seed(1337)
#training effects
for(i in 1:900){
    if(dataTrainingMain[i,1] == "Control"){
      dataTrainingMain[i,2] <- dataTrainingMain[i,2] + rnorm(1,mean = 5, sd=1)
    }
    else if(dataTrainingMain[i,1] == "Training 1"){
      dataTrainingMain[i,2] <- dataTrainingMain[i,2] + rnorm(1,mean = 6, sd=1)
    }
    else if(dataTrainingMain[i,1] == "Training 2"){
      dataTrainingMain[i,2] <- dataTrainingMain[i,2] + rnorm(1,mean = 7, sd=1)
    }
}
```

Let's take a look at our results.
```{r}
#Color blind friendly pallete
cbPalette <- c("#999999", "#0072B2", "#D55E00", "#F0E442", "#000000", "#CC79A7", "#FF00FF")

ggplot(dataTrainingMain, aes(x=training, y=perfScores)) + 
  geom_boxplot(aes(group=training, color=training)) +
  scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette)

```

Next we want to make training method 1 less effective for people with high conscientious and more effective for people with low conscientious. We also want to do the opposite for people who are low on conscientious.
```{r}
#conscientiousness effects
for(i in 1:900){
  #We check if someone recieved training method 1 and if they belong to the high conscientiousness group. Column 1 contains their training value, and column 5 contains their conscientiousness group. If both are true we will subtract 1 from their score, which is contained in column 2.
  if(dataTrainingMain[i,1] == "Training 1" && dataTrainingMain[i,4] == "High Conscientious"){
    dataTrainingMain[i,2] <- dataTrainingMain[i,2] - 1
  }
  #Now we check if someone received training method 2 and if they belong to the high conscientiousness group. If both are true we will add 1 to their score.
  #NOTE: We need to call their current performance value and then add 1 to it. If we simply did <- +1, or <- -1 instead that would set their performance score to 1 or -1. 
  else if(dataTrainingMain[i,1] == "Training 2" && dataTrainingMain[i,4] == "High Conscientious"){
    dataTrainingMain[i,2] <- dataTrainingMain[i,2] + 1
  }
  else if(dataTrainingMain[i,1] == "Training 1" && dataTrainingMain[i,4] == "Low Conscientious"){
    dataTrainingMain[i,2] <- dataTrainingMain[i,2] + 1
  }
  else if(dataTrainingMain[i,1] == "Training 2" && dataTrainingMain[i,4] == "Low Conscientious"){
    dataTrainingMain[i,2] <- dataTrainingMain[i,2] - 1
  }
}
```

Let's see what things look like with these added effects.
```{r}
ggplot(dataTrainingMain, aes(x=training, y=perfScores)) + 
  geom_boxplot(aes(group=training, color=training)) +
  scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette)

ggplot(dataTrainingMain, aes(x=training, y=perfScores)) + 
  stat_summary(aes(group=Cgroup, color=Cgroup), fun.y=mean, geom = "line") +
  labs(x = "Training", y = "Performance", color = "Conscientiousness Level") +
  scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette)
```

We can also take a look at these results using ANOVA. We see that there is a significant main effect of training, and that training also interacts with someones conscientiousness.
```{r}
fit1 <- lm(perfScores ~ training*Cgroup, data=dataTrainingMain)
summary(fit1)

```

#### Modeling an educational intervention

We are working for a small high school that has lower than average student performance. The school wants to develop an intervention that will increase the number of students who get a C+ or higher. Their goal is to find an intervention that triples the amount of kids who receive a C+ or higher. How big of an effect size does this intervention need to have?

We are told that the current average performance of students is a C-, 70%. We will assume that grades follow a normal distribution with a SD that is approximately half a letter grade. Thus, our initial sample will have a mean of 70 and SD of 4.

Given that we know the distribution of our data we can calculate the percentage of kids who are currently receiving a C+ or better. Since our SD is 4 all we have to do is figure out what percent of cases fall above 1 SD of a normal curve.
```{r}
#First we simulate some data following our paramters
dataQuantileTest <- rnorm(50000, mean = 70, sd = 4)

#Then we find the number of values greater than or equal to 75 and devide it by our total number of values
sum(dataQuantileTest >= 75) / length(dataQuantileTest)
```

About 10% of our students are currently getting a C+ or better, or 20 * .10 = 2 students. So, after our intervention we should have at least 2 * 3 = 6 students who receive a C+ or better.

So, our first question is, "How big of an effect does our training need to have in order for 6 more students to get a C+ or better?" Let's simulate some data to see.
```{r}
set.seed(1337)
#Creates an empty data frame and renames the columns to be simulation1-20
matrixIET <- matrix(nrow = 20, ncol = 20)
dataInterventionEffectTest <- as.data.frame(matrixIET)
names(dataInterventionEffectTest) <- paste("Simulation", seq(1:20), sep = "")

#simulates our data
for (i in 1:20)
{
#We can use a loop to generate means that are 1 point higher every iteration and write it to the ith COLUMN.
#Notice that we are using i to represent columns here instead of rows because we want each sample to belong to a seperate column.
#Leaving the rows spot blank tells r we want all rows to be filled. In this case we have 20 cases, so they will each fill a seperate row.
dataInterventionEffectTest[,i] <- rnorm(20, mean = 70 + i, sd = 4)
}

```

We will then see how many scores are a C or above.
```{r}
set.seed(1337)
matrixAboveC <- matrix(nrow = 1, ncol = 20)

for (i in 1:20)
{
#We can then see how many people get grades of 75% or above.
matrixAboveC[,i] <- sum(dataInterventionEffectTest[,i] >= 75)
}

print(matrixAboveC)
```

Most of the later cases are quite overpowered for what we want. It would be great if we could get all 20 students to have a C or better, but our goal is to find the smallest intervention that will have at least 6 students with a C or better. The first 7 cases look promising. Let's see what their distributions look like.
```{r}
dataInterventionEffectTestLong <- gather(dataInterventionEffectTest, key = "key", value = "value", Simulation1:Simulation7, factor_key = FALSE)

ggplot(data=dataInterventionEffectTestLong, aes(x=value, group=key, fill=key)) +
    geom_density(alpha=.2) + 
    scale_fill_manual(values=cbPalette) + scale_colour_manual(values=cbPalette) + 
    geom_vline(xintercept = 75, linetype = "dashed", colour = "black", size= 1)
```

Based on the above results it doesn't seem clear which intervention is the best. We also have to consider that these results might be due to our small sample size of 20. What would happen if we did a hundred simulations? How many times would at least 6 of our students get a C or higher? Let's simulate 100 cases with a mean of 74 and sd of 4.
```{r}
matrixBlank <- matrix(nrow = 20, ncol = 100)
dataMean74Effect <- as.data.frame(matrixBlank)
names(dataMean74Effect) <- paste("Simulation", seq(1:100), sep = "")

#simulates our data
for (i in 1:100)
  {
  #Notice we add i to our seed so it is different every loop
  set.seed(1000 + i)
  dataMean74Effect[,i] <- rnorm(20, mean = 74, sd = 4)
  }
```

Now we'll write a loop to check if the sum of cases greater than 75 is greater than 6.
```{r}
matrixPercentAboveC <- matrix(nrow = 100, ncol = 1)

for (i in 1:100)
{
  if(6 <= sum(dataMean74Effect[,i] >= 75))
  {
     matrixPercentAboveC[i,1] <- 1
  }
  else{
    matrixPercentAboveC[i,1] <- 0
  }
}
sum(matrixPercentAboveC)
```

So, only 88% of the time do our samples contain at least 6 kids who received a C or better. Based on a traditional alpha value of .05 we would expect that ~95 cases will have 6 or more kids who get a C or better if our intervention is truly effective. Since this only happened 88% of the time let's see if simulating data with a mean of 75 produces the desired results.

```{r}
matrixBlank <- matrix(nrow = 20, ncol = 100)
dataMean75Effect <- as.data.frame(matrixBlank)
names(dataMean74Effect) <- paste("Simulation", seq(1:100), sep = "")

#simulates our data
for (i in 1:100)
  {
  set.seed(1000 + i)
  dataMean75Effect[,i] <- rnorm(20, mean = 75, sd = 4)
}

#Calculates number of cases that are 75 or greater.
matrixPercentAboveC <- matrix(nrow = 100, ncol = 1)

for (i in 1:100)
{
  if(6 <= sum(dataMean75Effect[,i] >= 75))
  {
     matrixPercentAboveC[i,1] <- 1
  }
  else{
    matrixPercentAboveC[i,1] <- 0
  }
}
sum(matrixPercentAboveC)
```

Now 100% of our simulated data sets have at least 6 kids with a C or better. So, we would conclude that we need an intervention that raises our mean by at least 5 points. If we wanted to put this in terms of effect size we would say we need an intervention that has at least a d of 1.25.

We can also alter the paramters of our simulation to test different thresholds of scores. For example, what if we want an intervention that will ensure no students are failing the class, where failing means a D- or below.
```{r}
matrixBlank <- matrix(nrow = 20, ncol = 100)
dataMean74Effect <- as.data.frame(matrixBlank)
names(dataMean77Effect) <- paste("Simulation", seq(1:100), sep = "")

#simulates our data
for (i in 1:100)
  {
  set.seed(1000 + i)
  dataMean74Effect[,i] <- rnorm(20, mean = 74, sd = 4)
}

#Calculates number of cases that are 62 or less.
matrixPercentBelowD <- matrix(nrow = 100, ncol = 1)

for (i in 1:100)
{
  if(1 <= sum(dataMean74Effect[,i] <= 63))
  {
     matrixPercentBelowD[i,1] <- 1
  }
  else{
    matrixPercentBelowD[i,1] <- 0
  }
}
sum(matrixPercentBelowD)
```

It turns out that an intervention of d = 1, or a mean shift of 4 points, produces an outcome where 95% of the time all students are above a D-. Note that in this case we want an outcome where are sum is 5 or lower since we are coding times where at least one student got a D- or lower as 1.

Last, if we wanted to run a power analysis of our first analysis we could. The below power analysis tests whether the simulated rnorm samples with a mean of 75 and sd of 4 differs from 70. [Chapter 5 of this book from the Crump Lab](https://crumplab.github.io/programmingforpsych/simulating-and-analyzing-data-in-r.html#power-analysis-and-sample-size-planning-by-monte-carlo-simulation) contains examples of lots of other power analyses for more complex designs.
```{r}
sims_to_run <- 1000
#Vector we use to save a p-value from each run
save_p <- vector(length=sims_to_run)

for(i in 1:sims_to_run){
set.seed(1000 + i)
sim_data <- rnorm(20,mean = 75,sd = 4)
t.out <-t.test(sim_data,mu=70)
#Saves the p-value from our t-test output to our save_p vector
save_p[i] <- t.out$p.value
}
#Converts the vector to a data frame for ggplot graphing
dataP <- as.data.frame(save_p)

ggplot(dataP, aes(x=save_p)) +
  geom_histogram(color="black", fill="white")
```
We can see all our p-values are well below the .05 point.

#### Go to part 4 of the exercise, creating your own study.

### Final Thoughts

We have gone over some basics of how to simulate data so far, but there's a lot we have not discussed. One of the main differences between this tutorial and the actual use of simulations is that we did a lot more leg work than was needed and wrote code that was longer than it needed to be. For example, in practice we would use the pwr package to run a power analysis instead of writing our own script. We would probably use the `replicate()` function in place of some of our `for()` loops. We would probably use the simstudy package to help us more easily run our simulations, or make use of other existing packages. We may also write our own functions to make it faster to run certain common processes. We would also only need to use the `set.seed()` function once at the start of our script instead of running it for every single simulation.

The main reason we didn't use these easier methods here is to give you a better sense of how all these processes work. You might use faster methods to simulate data in the future, but should now have a rough idea of how those processes work and could hopefully replicate those processes yourself if you needed to. Everything we've covered should also extend to more complex designs than the simple t-test's or ANOVA's we've covered here.

Another common simulation method we did not cover here is simulations over time. For example, simulations are currently being used to [model the spread of CoronaVirus](https://www.youtube.com/watch?v=gxAaO2rsdIs).

Something you'll hear the guy in the video say a lot though is that simulations are 'toy models'. This is an idea that has been around for a while where people essentially say that simulations aren't actually useful for anything other than playing around with hypothetical examples that will never happen. From this viewpoint, simulations are about as useful for making research decisions as a Lego model would be for making decisions about designing a skyscraper. Basically, they can be fun toys, but don't have any place in the real world. I think this viewpoint is a big reason why simulations are not more common in research today. I also think this viewpoint is, bluntly, kind of ignorant. Simulations can be a great way to move beyond a purely conceptual model and determine what is and is not possible. Recently, power analysis has become a very popular way to use data simulation as part of the research process. It saves people lots of time and effort by determining what our probability is of observing an effect in our experiment if that effect actually exists. I think we can also use other types of data simulation as part of the research process as well. While it is important to keep in mind that simulations do not reflect real world data and cannot stand in for that data, that doesn't mean they cannot help us make decisions and save us resources. I think [this paper](https://journals.aom.org/doi/10.5465/amr.2007.26586485) provides a good introduction to thinking about how and when to incorporate data simulation into your research and where it sits in the research process in general.